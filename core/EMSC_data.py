"""
EMSCÊï∞ÊçÆÂä†ËΩΩÊ®°Âùó
ÂåÖÂê´ÊâÄÊúâÊï∞ÊçÆÂä†ËΩΩÂíåÂ§ÑÁêÜÁõ∏ÂÖ≥ÁöÑÁ±ªÂíåÂáΩÊï∞
"""

import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.utils import Sequence
import threading
from queue import Queue
from datetime import datetime
import joblib

class EMSCDataGenerator(Sequence):
    """
    Ëá™ÂÆö‰πâÊï∞ÊçÆÁîüÊàêÂô®ÔºåÁî®‰∫éÈ´òÊïàÂä†ËΩΩÂ§ßÂûãÊï∞ÊçÆÈõÜ
    ÈíàÂØπ15000Êù°Êï∞ÊçÆÁöÑÊï∞ÊçÆÈõÜ‰ºòÂåñÂèÇÊï∞
    """
    def __init__(self, X_paths, Y_paths, init_states, batch_size=8, shuffle=True,
                 num_workers=4, prefetch_factor=2):
        self.X_paths = X_paths
        self.Y_paths = Y_paths
        self.init_states = init_states
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.indexes = np.arange(len(X_paths))
        self.on_epoch_end()
        
        # ‰ºòÂåñÁºìÂ≠òÂèÇÊï∞
        total_samples = len(X_paths)
        self.cache_size = min(int(total_samples * 0.05), 1000)
        self.preload_queue_size = min(self.cache_size * prefetch_factor, 2000)
        
        # ÂàõÂª∫Êï∞ÊçÆÁºìÂ≠ò
        self.cache = {}
        self.cache_lock = threading.Lock()
        
        # ÂàõÂª∫È¢ÑÂä†ËΩΩÁ∫øÁ®ãÊ±†
        self.preload_queue = Queue(maxsize=self.preload_queue_size)
        self.stop_preload = threading.Event()
        self.num_preload_threads = min(num_workers, 8)  # ÈôêÂà∂ÊúÄÂ§ßÁ∫øÁ®ãÊï∞
        self.preload_threads = []
        for _ in range(self.num_preload_threads):
            thread = threading.Thread(target=self._preload_data)
            thread.daemon = True
            thread.start()
            self.preload_threads.append(thread)
        
        print(f"Êï∞ÊçÆÁîüÊàêÂô®ÂàùÂßãÂåñÂÆåÊàê:")
        print(f"- ÊÄªÊ†∑Êú¨Êï∞: {total_samples}")
        print(f"- ÁºìÂ≠òÂ§ßÂ∞è: {self.cache_size}")
        print(f"- È¢ÑÂä†ËΩΩÈòüÂàóÂ§ßÂ∞è: {self.preload_queue_size}")
        print(f"- È¢ÑÂä†ËΩΩÁ∫øÁ®ãÊï∞: {self.num_preload_threads}")
        print(f"- È¢ÑÂèñÂõ†Â≠ê: {prefetch_factor}")
    
    def __len__(self):
        """ËøîÂõûÊâπÊ¨°Êï∞Èáè"""
        return int(np.ceil(len(self.X_paths) / self.batch_size))
    
    def on_epoch_end(self):
        """ÊØè‰∏™epochÁªìÊùüÊó∂Ë∞ÉÁî®"""
        if self.shuffle:
            np.random.shuffle(self.indexes)
    
    def _preload_data(self):
        """È¢ÑÂä†ËΩΩÊï∞ÊçÆÁöÑÁ∫øÁ®ãÂáΩÊï∞"""
        while not self.stop_preload.is_set():
            try:
                idx = self.preload_queue.get(timeout=1)
                if idx not in self.cache:
                    with self.cache_lock:
                        if len(self.cache) >= self.cache_size:
                            num_to_remove = int(self.cache_size * 0.2)
                            for _ in range(num_to_remove):
                                if self.cache:
                                    oldest_key = next(iter(self.cache))
                                    del self.cache[oldest_key]
                        self.cache[idx] = {
                            'X': np.array(self.X_paths[idx], dtype=np.float32),
                            'Y': np.array(self.Y_paths[idx], dtype=np.float32)
                        }
                self.preload_queue.task_done()
            except:
                continue
    
    def __getitem__(self, idx):
        """Ëé∑Âèñ‰∏Ä‰∏™ÊâπÊ¨°ÁöÑÊï∞ÊçÆ"""
        batch_indexes = self.indexes[idx * self.batch_size:(idx + 1) * self.batch_size]
        
        batch_X = []
        batch_Y = []
        batch_init_states = []
        
        next_batch_start = ((idx + 1) * self.batch_size) % len(self.X_paths)
        next_batch_indices = range(next_batch_start, 
                                 min(next_batch_start + self.batch_size, len(self.X_paths)))
        
        for next_idx in next_batch_indices:
            if next_idx not in self.cache:
                try:
                    self.preload_queue.put(next_idx, block=False)
                except:
                    pass
        
        for i in batch_indexes:
            with self.cache_lock:
                if i in self.cache:
                    data = self.cache[i]
                    X = data['X']
                    Y = data['Y']
                else:
                    X = np.array(self.X_paths[i], dtype=np.float32)
                    Y = np.array(self.Y_paths[i], dtype=np.float32)
                    if len(self.cache) < self.cache_size:
                        self.cache[i] = {'X': X, 'Y': Y}
            
            batch_X.append(X)
            batch_Y.append(Y)
            batch_init_states.append(self.init_states[i])
        
        batch_X = np.array(batch_X)
        batch_Y = np.array(batch_Y)
        batch_init_states = np.array(batch_init_states)
        
        return {
            'delta_input': batch_X,
            'init_state': batch_init_states
        }, batch_Y
    
    def __del__(self):
        """Ê∏ÖÁêÜËµÑÊ∫ê"""
        self.stop_preload.set()
        for thread in self.preload_threads:
            thread.join(timeout=1)

def create_tf_dataset(X_paths, Y_paths, init_states, batch_size=8, shuffle=True,
                     num_parallel_calls=tf.data.AUTOTUNE):
    """
    ÂàõÂª∫TensorFlowÊï∞ÊçÆÈõÜÔºåÈíàÂØπ15000Êù°Êï∞ÊçÆ‰ºòÂåñÂèÇÊï∞
    """
    total_samples = len(X_paths)
    shuffle_buffer_size = min(total_samples, 5000)
    
    dataset = tf.data.Dataset.from_tensor_slices((
        {
            'delta_input': X_paths,
            'init_state': init_states
        },
        Y_paths
    ))
    
    dataset = dataset.cache()
    
    if shuffle:
        dataset = dataset.shuffle(
            buffer_size=shuffle_buffer_size,
            reshuffle_each_iteration=True
        )
    
    dataset = dataset.batch(batch_size)
    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)
    
    # Âπ∂Ë°åÂ§ÑÁêÜ‰ºòÂåñ
    dataset = dataset.map(
        lambda x, y: (x, y),
        num_parallel_calls=num_parallel_calls
    )
    
    return dataset

def save_dataset_to_npz(X_paths, Y_paths, save_path='./msc_models/dataset.npz'):
    """
    Â∞ÜÈ¢ÑÂ§ÑÁêÜÂêéÁöÑÊï∞ÊçÆÈõÜ‰øùÂ≠ò‰∏∫npzÊ†ºÂºè
    """
    try:
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        X_array = np.array(X_paths, dtype=object)
        Y_array = np.array(Y_paths, dtype=object)
        
        np.savez_compressed(
            save_path,
            X_paths=X_array,
            Y_paths=Y_array,
            save_time=datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        )
        print(f"Êï∞ÊçÆÈõÜÂ∑≤‰øùÂ≠òËá≥: {save_path}")
        return save_path
    except Exception as e:
        print(f"‰øùÂ≠òÊï∞ÊçÆÈõÜÊó∂Âá∫Èîô: {e}")
        return None

def load_dataset_from_npz(npz_path='./msc_models/dataset.npz'):
    """
    ‰ªénpzÊñá‰ª∂Âä†ËΩΩÊï∞ÊçÆÈõÜ
    """
    try:
        if not os.path.exists(npz_path):
            print(f"Êï∞ÊçÆÈõÜÊñá‰ª∂‰∏çÂ≠òÂú®: {npz_path}")
            return None, None
        
        data = np.load(npz_path, allow_pickle=True)
        X_paths = data['X_paths'].tolist()
        Y_paths = data['Y_paths'].tolist()
        
        print(f"‰ªé {npz_path} Âä†ËΩΩÊï∞ÊçÆÈõÜ")
        print(f"‰øùÂ≠òÊó∂Èó¥: {data['save_time']}")
        print(f"Â∫èÂàóÊï∞Èáè: {len(X_paths)}")
        
        lengths = [len(x) for x in X_paths]
        print(f"Â∫èÂàóÈïøÂ∫¶ÁªüËÆ°:")
        print(f"ÊúÄÁü≠: {min(lengths)}")
        print(f"ÊúÄÈïø: {max(lengths)}")
        print(f"Âπ≥Âùá: {np.mean(lengths):.2f}")
        print(f"‰∏≠‰ΩçÊï∞: {np.median(lengths)}")
        
        return X_paths, Y_paths
    except Exception as e:
        print(f"Âä†ËΩΩÊï∞ÊçÆÈõÜÊó∂Âá∫Èîô: {e}")
        return None, None

def load_dataset_smart(dataset_path, use_tfrecord=True):
    """
    Êô∫ËÉΩÂä†ËΩΩÊï∞ÊçÆÈõÜÔºö‰ºòÂÖà‰ΩøÁî®TFRecordÔºåÂ¶ÇÊûúÊ≤°ÊúâÂàô‰ΩøÁî®NPZ
    
    Args:
        dataset_path: Êï∞ÊçÆÈõÜË∑ØÂæÑÔºàÂèØ‰ª•ÊòØ.npzÊàñ.tfrecordÔºâ
        use_tfrecord: ÊòØÂê¶‰ºòÂÖà‰ΩøÁî®TFRecordÊ†ºÂºè
    
    Returns:
        tuple: (X_paths, Y_paths) Êàñ tf.data.DatasetÔºàÂ¶ÇÊûú‰ΩøÁî®TFRecordÔºâ
    """
    import os
    from pathlib import Path
    
    # Ëé∑ÂèñÂü∫Á°ÄË∑ØÂæÑÔºàÂéªÈô§Êâ©Â±ïÂêçÔºâ
    base_path = str(Path(dataset_path).with_suffix(''))
    npz_path = base_path + '.npz'
    tfrecord_path = base_path + '.tfrecord'
    
    print(f"üîç Êô∫ËÉΩÊï∞ÊçÆÈõÜÂä†ËΩΩ:")
    print(f"   Âü∫Á°ÄË∑ØÂæÑ: {base_path}")
    print(f"   NPZË∑ØÂæÑ: {npz_path}")
    print(f"   TFRecordË∑ØÂæÑ: {tfrecord_path}")
    
    if use_tfrecord and os.path.exists(tfrecord_path):
        # ‰ºòÂÖà‰ΩøÁî®TFRecord
        try:
            from EMSC_Net.utils.EMSC_dataset_converter import load_tfrecord_dataset, check_dataset_exists
            
            if check_dataset_exists(tfrecord_path):
                print(f"‚úÖ ‰ΩøÁî®TFRecordÊï∞ÊçÆÈõÜ: {tfrecord_path}")
                return 'tfrecord', tfrecord_path
            else:
                print(f"‚ö†Ô∏è TFRecordÊñá‰ª∂Â≠òÂú®‰ΩÜ‰∏çÂÆåÊï¥ÔºåÂõûÈÄÄÂà∞NPZ")
        except Exception as e:
            print(f"‚ö†Ô∏è TFRecordÂä†ËΩΩÂ§±Ë¥•: {e}ÔºåÂõûÈÄÄÂà∞NPZ")
    
    # ‰ΩøÁî®NPZÊñá‰ª∂
    if os.path.exists(npz_path):
        print(f"‚úÖ ‰ΩøÁî®NPZÊï∞ÊçÆÈõÜ: {npz_path}")
        X_paths, Y_paths = load_dataset_from_npz(npz_path)
        if X_paths is not None and Y_paths is not None:
            return 'npz', (X_paths, Y_paths)
        else:
            print(f"‚ùå NPZÊï∞ÊçÆÈõÜÂä†ËΩΩÂ§±Ë¥•")
            return None, None
    
    print(f"‚ùå Êú™ÊâæÂà∞ÂèØÁî®ÁöÑÊï∞ÊçÆÈõÜÊñá‰ª∂")
    print(f"   Â∞ùËØïÊü•ÊâæÁöÑÊñá‰ª∂:")
    print(f"   - NPZ: {npz_path} (Â≠òÂú®: {os.path.exists(npz_path)})")
    print(f"   - TFRecord: {tfrecord_path} (Â≠òÂú®: {os.path.exists(tfrecord_path)})")
    return None, None