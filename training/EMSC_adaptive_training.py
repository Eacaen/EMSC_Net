"""
EMSCè‡ªé€‚åº”è®­ç»ƒè„šæœ¬
ä¸“é—¨ç”¨äºè§£å†³è®­ç»ƒåœæ»é—®é¢˜çš„é«˜çº§è®­ç»ƒç­–ç•¥
"""

import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.optimizers import Adam
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.EMSC_model import build_msc_model
from core.EMSC_losses import EMSCLoss
from training.EMSC_callbacks import MSCProgressCallback, create_early_stopping_callback


class AdaptiveLossWeight:
    """è‡ªé€‚åº”æŸå¤±æƒé‡è°ƒæ•´å™¨"""
    
    def __init__(self, initial_mse_weight=1.0, initial_reg_weight=0.01):
        self.mse_weight = initial_mse_weight
        self.reg_weight = initial_reg_weight
        self.loss_history = []
        self.patience_count = 0
        
    def update_weights(self, epoch, current_loss, val_loss=None):
        """æ ¹æ®è®­ç»ƒçŠ¶å†µåŠ¨æ€è°ƒæ•´æŸå¤±æƒé‡"""
        self.loss_history.append(current_loss)
        
        # å¦‚æœæŸå¤±åœæ»ï¼Œè°ƒæ•´æƒé‡
        if len(self.loss_history) >= 10:
            recent_losses = self.loss_history[-10:]
            loss_variance = np.var(recent_losses)
            loss_improvement = (recent_losses[0] - recent_losses[-1]) / recent_losses[0]
            
            # å¦‚æœæŸå¤±æ”¹å–„å¾ˆå°ä¸”æ–¹å·®å¾ˆå°ï¼ˆåœæ»ï¼‰
            if loss_improvement < 0.01 and loss_variance < 1e-6:
                self.patience_count += 1
                if self.patience_count >= 5:
                    # é™ä½æ­£åˆ™åŒ–æƒé‡ï¼Œå¢å¼ºæ‹Ÿåˆèƒ½åŠ›
                    self.reg_weight *= 0.5
                    self.mse_weight *= 1.2
                    self.patience_count = 0
                    print(f"Epoch {epoch}: æ£€æµ‹åˆ°åœæ»ï¼Œè°ƒæ•´æƒé‡ - MSEæƒé‡: {self.mse_weight:.4f}, æ­£åˆ™åŒ–æƒé‡: {self.reg_weight:.4f}")
            else:
                self.patience_count = 0
                
        return self.mse_weight, self.reg_weight


class CyclicalLearningRate:
    """å¾ªç¯å­¦ä¹ ç‡è°ƒåº¦å™¨"""
    
    def __init__(self, base_lr=1e-5, max_lr=1e-2, step_size=100):
        self.base_lr = base_lr
        self.max_lr = max_lr
        self.step_size = step_size
        
    def get_lr(self, epoch):
        """è®¡ç®—å½“å‰epochçš„å­¦ä¹ ç‡"""
        cycle = np.floor(1 + epoch / (2 * self.step_size))
        x = np.abs(epoch / self.step_size - 2 * cycle + 1)
        lr = self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x))
        return lr


class WarmRestartCallback(tf.keras.callbacks.Callback):
    """çƒ­é‡å¯å›è°ƒ - æ£€æµ‹åœæ»æ—¶é‡å¯è®­ç»ƒ"""
    
    def __init__(self, patience=20, restart_lr_factor=0.5, min_delta=1e-6):
        super().__init__()
        self.patience = patience
        self.restart_lr_factor = restart_lr_factor
        self.min_delta = min_delta
        self.best_loss = float('inf')
        self.wait = 0
        self.restart_count = 0
        
    def on_epoch_end(self, epoch, logs=None):
        current_loss = logs.get('val_loss') or logs.get('loss')
        
        if current_loss < self.best_loss - self.min_delta:
            self.best_loss = current_loss
            self.wait = 0
        else:
            self.wait += 1
            
        if self.wait >= self.patience:
            # æ‰§è¡Œçƒ­é‡å¯
            self.restart_count += 1
            new_lr = self.model.optimizer.learning_rate.numpy() * self.restart_lr_factor
            self.model.optimizer.learning_rate.assign(max(new_lr, 1e-7))
            
            # æ·»åŠ ä¸€äº›å™ªå£°åˆ°æ¨¡å‹æƒé‡
            for layer in self.model.layers:
                if hasattr(layer, 'kernel'):
                    noise = tf.random.normal(layer.kernel.shape, stddev=0.01)
                    layer.kernel.assign_add(noise)
                    
            self.wait = 0
            print(f"Epoch {epoch + 1}: æ‰§è¡Œçƒ­é‡å¯ #{self.restart_count}, æ–°å­¦ä¹ ç‡: {new_lr:.2e}")


def create_adaptive_training_config():
    """åˆ›å»ºè‡ªé€‚åº”è®­ç»ƒé…ç½®"""
    return {
        'use_cyclical_lr': True,
        'use_warm_restart': True,
        'use_adaptive_loss_weight': True,
        'use_gradient_scaling': True,
        'use_noise_injection': True,
        'early_stopping_patience': 50,  # å¢åŠ æ—©åœè€å¿ƒ
        'lr_reduction_patience': 8      # æ›´å¿«çš„å­¦ä¹ ç‡è°ƒæ•´
    }


def noise_injection_callback():
    """å™ªå£°æ³¨å…¥å›è°ƒ - é˜²æ­¢è¿‡æ‹Ÿåˆå’Œå±€éƒ¨æœ€ä¼˜"""
    class NoiseInjection(tf.keras.callbacks.Callback):
        def __init__(self, noise_std=0.001, injection_frequency=10):
            super().__init__()
            self.noise_std = noise_std
            self.injection_frequency = injection_frequency
            
        def on_epoch_end(self, epoch, logs=None):
            if epoch % self.injection_frequency == 0 and epoch > 0:
                # åªå¯¹éƒ¨åˆ†å±‚æ³¨å…¥å™ªå£°
                for layer in self.model.layers[-2:]:  # åªå¯¹æœ€åä¸¤å±‚æ³¨å…¥å™ªå£°
                    if hasattr(layer, 'kernel'):
                        noise = tf.random.normal(layer.kernel.shape, stddev=self.noise_std)
                        layer.kernel.assign_add(noise)
                print(f"Epoch {epoch + 1}: æ³¨å…¥å™ªå£°åˆ°æ¨¡å‹æƒé‡")
                        
    return NoiseInjection()


def adaptive_train_model(model, train_dataset, val_dataset, epochs=2000, 
                        initial_lr=1e-3, state_dim=8, save_path='./models/'):
    """
    ä½¿ç”¨è‡ªé€‚åº”ç­–ç•¥è®­ç»ƒEMSCæ¨¡å‹
    
    Args:
        model: EMSCæ¨¡å‹
        train_dataset: è®­ç»ƒæ•°æ®é›†
        val_dataset: éªŒè¯æ•°æ®é›†
        epochs: è®­ç»ƒè½®æ•°
        initial_lr: åˆå§‹å­¦ä¹ ç‡
        state_dim: çŠ¶æ€ç»´åº¦
        save_path: æ¨¡å‹ä¿å­˜è·¯å¾„
    """
    
    print("ğŸš€ å¯åŠ¨è‡ªé€‚åº”è®­ç»ƒæ¨¡å¼...")
    
    # åˆ›å»ºè‡ªé€‚åº”ç»„ä»¶
    adaptive_loss = AdaptiveLossWeight()
    cyclical_lr = CyclicalLearningRate(base_lr=initial_lr/10, max_lr=initial_lr*2)
    
    # åˆ›å»ºä¼˜åŒ–å™¨ - ä½¿ç”¨æ›´æ¿€è¿›çš„è®¾ç½®
    optimizer = Adam(
        learning_rate=initial_lr,
        clipnorm=0.5,    # æ›´ä¸¥æ ¼çš„æ¢¯åº¦è£å‰ª
        clipvalue=0.1,   # æ›´ä¸¥æ ¼çš„å€¼è£å‰ª
        amsgrad=True     # ä½¿ç”¨AMSGradå˜ä½“
    )
    
    # åˆ›å»ºæŸå¤±å‡½æ•°
    custom_loss = EMSCLoss(state_dim=state_dim)
    
    # ç¼–è¯‘æ¨¡å‹
    model.compile(
        optimizer=optimizer,
        loss=custom_loss,
        jit_compile=False
    )
    
    # åˆ›å»ºå›è°ƒåˆ—è¡¨
    callbacks = []
    
    # 1. è¿›åº¦ä¿å­˜å›è°ƒ
    progress_callback = MSCProgressCallback(
        save_path=save_path,
        model_name='adaptive_msc_model',
        best_model_name='best_adaptive_msc_model',
        save_frequency=5  # æ›´é¢‘ç¹ä¿å­˜
    )
    callbacks.append(progress_callback)
    
    # 2. å¾ªç¯å­¦ä¹ ç‡å›è°ƒ
    class CyclicalLRCallback(tf.keras.callbacks.Callback):
        def on_epoch_begin(self, epoch, logs=None):
            lr = cyclical_lr.get_lr(epoch)
            self.model.optimizer.learning_rate.assign(lr)
            if epoch % 50 == 0:
                print(f"Epoch {epoch}: å¾ªç¯å­¦ä¹ ç‡ = {lr:.2e}")
    
    callbacks.append(CyclicalLRCallback())
    
    # 3. çƒ­é‡å¯å›è°ƒ
    callbacks.append(WarmRestartCallback(patience=15, restart_lr_factor=0.3))
    
    # 4. å™ªå£°æ³¨å…¥å›è°ƒ
    callbacks.append(noise_injection_callback())
    
    # 5. æ”¹è¿›çš„æ—©åœå›è°ƒ
    early_stopping = tf.keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=80,        # æ›´å¤§çš„è€å¿ƒ
        min_delta=1e-7,     # æ›´å°çš„æœ€å°æ”¹å–„
        restore_best_weights=True,
        verbose=1
    )
    callbacks.append(early_stopping)
    
    # 6. å­¦ä¹ ç‡ç›‘æ§
    lr_monitor = tf.keras.callbacks.LearningRateScheduler(
        lambda epoch: cyclical_lr.get_lr(epoch), verbose=0
    )
    callbacks.append(lr_monitor)
    
    print("ğŸ“Š è‡ªé€‚åº”è®­ç»ƒé…ç½®:")
    print(f"- åˆå§‹å­¦ä¹ ç‡: {initial_lr}")
    print(f"- å¾ªç¯å­¦ä¹ ç‡èŒƒå›´: {cyclical_lr.base_lr:.2e} - {cyclical_lr.max_lr:.2e}")
    print(f"- æ¢¯åº¦è£å‰ª: clipnorm=0.5, clipvalue=0.1")
    print(f"- ä¼˜åŒ–å™¨: Adam with AMSGrad")
    print(f"- çƒ­é‡å¯: 15 epoch patience")
    print(f"- å™ªå£°æ³¨å…¥: æ¯10 epoch")
    print(f"- æ—©åœ: 80 epoch patience")
    
    # å¼€å§‹è®­ç»ƒ
    history = model.fit(
        train_dataset,
        validation_data=val_dataset,
        epochs=epochs,
        callbacks=callbacks,
        verbose=1
    )
    
    print("âœ… è‡ªé€‚åº”è®­ç»ƒå®Œæˆ!")
    
    return model, history


if __name__ == "__main__":
    print("EMSCè‡ªé€‚åº”è®­ç»ƒæ¨¡å—")
    print("è¯·ä»ä¸»è®­ç»ƒè„šæœ¬è°ƒç”¨ adaptive_train_model å‡½æ•°") 