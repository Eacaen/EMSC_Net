"""
EMSCæ¨¡å‹ä¸»è®­ç»ƒè„šæœ¬
ä½¿ç”¨æ¨¡å—åŒ–ç»“æ„ç»„ç»‡è®­ç»ƒæµç¨‹ï¼Œæ”¯æŒå¤šCPUè®­ç»ƒ
"""

import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import mixed_precision

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from EMSC_model import build_msc_model
from EMSC_data import EMSCDataGenerator, create_tf_dataset, load_dataset_from_npz
from EMSC_callbacks import MSCProgressCallback, create_early_stopping_callback, create_learning_rate_scheduler
from EMSC_cpu_monitor import create_cpu_monitor_callback
from EMSC_dynamic_batch import DynamicBatchTrainer, create_dynamic_batch_callback
from EMSC_cpu_stress_test import comprehensive_performance_test
from EMSC_config import (create_training_config, save_training_config, 
                        parse_training_args, get_dataset_paths)
from EMSC_utils import (load_or_create_model_with_history, 
                       resume_training_from_checkpoint,
                       plot_final_training_summary,
                       print_training_summary)
from EMSC_losses import EMSCLoss

def check_environment():
    """æ£€æŸ¥å¹¶é…ç½®è®­ç»ƒç¯å¢ƒï¼Œä¼˜å…ˆä½¿ç”¨GPUï¼Œå›é€€åˆ°CPU"""
    print("æ£€æŸ¥è®­ç»ƒç¯å¢ƒ...")
    print(f"TensorFlowç‰ˆæœ¬: {tf.__version__}")
    print(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
    
    # æ£€æŸ¥GPUå¯ç”¨æ€§
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        print(f"å‘ç° {len(gpus)} ä¸ªGPUè®¾å¤‡:")
        for gpu in gpus:
            print(f"- {gpu}")
            # é…ç½®GPUå†…å­˜å¢é•¿
            try:
                tf.config.experimental.set_memory_growth(gpu, True)
                print(f"å·²ä¸º {gpu} å¯ç”¨å†…å­˜å¢é•¿")
            except RuntimeError as e:
                print(f"é…ç½®GPUå†…å­˜å¢é•¿æ—¶å‡ºé”™: {e}")
        
        # è®¾ç½®GPUä¸ºé»˜è®¤è®¾å¤‡
        try:
            tf.config.set_visible_devices(gpus[0], 'GPU')
            print(f"ä½¿ç”¨GPUè®¾å¤‡: {gpus[0]}")
            return None  # ä½¿ç”¨GPUæ—¶ä¸éœ€è¦è¿”å›workeræ•°
        except RuntimeError as e:
            print(f"è®¾ç½®GPUè®¾å¤‡æ—¶å‡ºé”™: {e}")
    
    # å¦‚æœæ²¡æœ‰GPUæˆ–GPUè®¾ç½®å¤±è´¥ï¼Œé…ç½®CPUç¯å¢ƒ
    print("æœªå‘ç°GPUè®¾å¤‡æˆ–GPUè®¾ç½®å¤±è´¥ï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ")
    
    # è·å–CPUæ ¸å¿ƒæ•°
    cpu_count = os.cpu_count()
    if cpu_count is None:
        cpu_count = 4  # é»˜è®¤å€¼
    
    # é’ˆå¯¹é˜¿é‡Œäº‘ç­‰äº‘ç¯å¢ƒçš„CPUä¼˜åŒ–é…ç½®
    # ä½¿ç”¨æ‰€æœ‰å¯ç”¨CPUæ ¸å¿ƒï¼Œä¸ä¿ç•™
    num_workers = cpu_count
    
    # è®¾ç½®TensorFlowçº¿ç¨‹é…ç½® - æ›´æ¿€è¿›çš„è®¾ç½®
    tf.config.threading.set_inter_op_parallelism_threads(num_workers)
    tf.config.threading.set_intra_op_parallelism_threads(num_workers)
    
    # è®¾ç½®OpenMPçº¿ç¨‹æ•°ï¼ˆç”¨äºNumPyã€MKLç­‰åº“ï¼‰
    os.environ['OMP_NUM_THREADS'] = str(num_workers)
    os.environ['MKL_NUM_THREADS'] = str(num_workers)
    os.environ['OPENBLAS_NUM_THREADS'] = str(num_workers)
    os.environ['NUMEXPR_NUM_THREADS'] = str(num_workers)
    
    # ä¼˜åŒ–TensorFlowçš„CPUæ€§èƒ½
    os.environ['TF_NUM_INTEROP_THREADS'] = str(num_workers)
    os.environ['TF_NUM_INTRAOP_THREADS'] = str(num_workers)
    
    # å¯ç”¨æ‰€æœ‰CPUä¼˜åŒ–
    os.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'  # å¯ç”¨OneDNNä¼˜åŒ–
    
    print(f"é˜¿é‡Œäº‘CPUç¯å¢ƒä¼˜åŒ–é…ç½®å®Œæˆ:")
    print(f"- æ€»CPUæ ¸å¿ƒæ•°: {cpu_count}")
    print(f"- è®­ç»ƒä½¿ç”¨çº¿ç¨‹æ•°: {num_workers}")
    print(f"- inter_op_parallelism_threads: {num_workers}")
    print(f"- intra_op_parallelism_threads: {num_workers}")
    print(f"- OMP_NUM_THREADS: {num_workers}")
    print(f"- å·²å¯ç”¨OneDNNä¼˜åŒ–")
    
    # æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ
    try:
        import psutil
        memory = psutil.virtual_memory()
        print(f"\nç³»ç»Ÿå†…å­˜ä¿¡æ¯:")
        print(f"æ€»å†…å­˜: {memory.total / (1024**3):.1f} GB")
        print(f"å¯ç”¨å†…å­˜: {memory.available / (1024**3):.1f} GB")
        print(f"å†…å­˜ä½¿ç”¨ç‡: {memory.percent}%")
        
        # æ˜¾ç¤ºCPUä¿¡æ¯
        print(f"\nCPUä¿¡æ¯:")
        print(f"ç‰©ç†CPUæ ¸å¿ƒæ•°: {psutil.cpu_count(logical=False)}")
        print(f"é€»è¾‘CPUæ ¸å¿ƒæ•°: {psutil.cpu_count(logical=True)}")
    except ImportError:
        print("æœªå®‰è£…psutilï¼Œè·³è¿‡ç³»ç»Ÿä¿¡æ¯æ£€æŸ¥")
    
    return num_workers

def get_optimal_batch_size(num_samples, num_workers):
    """
    è®¡ç®—æœ€ä¼˜æ‰¹å¤„ç†å¤§å° - é’ˆå¯¹é˜¿é‡Œäº‘CPUç¯å¢ƒä¼˜åŒ–
    
    Args:
        num_samples: è®­ç»ƒæ ·æœ¬æ•°é‡
        num_workers: å·¥ä½œçº¿ç¨‹æ•°ï¼ˆCPUæ¨¡å¼ï¼‰æˆ–Noneï¼ˆGPUæ¨¡å¼ï¼‰
    
    Returns:
        int: æœ€ä¼˜æ‰¹å¤„ç†å¤§å°
    """
    if num_workers is None:  # GPUæ¨¡å¼
        # GPUæ¨¡å¼ä¸‹ä½¿ç”¨è¾ƒå¤§çš„æ‰¹å¤„ç†å¤§å°
        return min(128, num_samples // 50)
    
    # CPUæ¨¡å¼ä¸‹çš„æ‰¹å¤„ç†å¤§å°è®¡ç®— - æ›´ç§¯æçš„é…ç½®
    # åŸºç¡€æ‰¹æ¬¡å¤§å° - ä¸ºCPUè®­ç»ƒå¢åŠ æ›´å¤§çš„åŸºæ•°
    base_batch = min(64, max(32, num_samples // 50))  # å¢åŠ åŸºç¡€æ‰¹æ¬¡å¤§å°
    
    # æ ¹æ®CPUçº¿ç¨‹æ•°è°ƒæ•´ - è®©æ¯ä¸ªçº¿ç¨‹å¤„ç†æ›´å¤šæ•°æ®
    # ä½¿ç”¨æ›´æ¿€è¿›çš„å€æ•°ï¼Œå……åˆ†åˆ©ç”¨å¤šæ ¸CPU
    if num_workers >= 16:  # é«˜æ ¸å¿ƒæ•°CPUï¼ˆé˜¿é‡Œäº‘é«˜é…ï¼‰
        multiplier = 2
    elif num_workers >= 8:  # ä¸­ç­‰æ ¸å¿ƒæ•°CPU
        multiplier = 3
    else:  # ä½æ ¸å¿ƒæ•°CPU
        multiplier = 4
    
    optimal_batch = base_batch * multiplier
    
    # ç¡®ä¿æ‰¹æ¬¡å¤§å°åˆç†
    optimal_batch = min(optimal_batch, num_samples)  # ä¸è¶…è¿‡æ ·æœ¬æ€»æ•°
    optimal_batch = max(16, optimal_batch)  # æœ€å°16
    
    # ç¡®ä¿æ˜¯8çš„å€æ•°ï¼ˆå¯¹å†…å­˜å¯¹é½å’Œå‘é‡åŒ–æœ‰åˆ©ï¼‰
    optimal_batch = (optimal_batch // 8) * 8
    
    print(f"CPUæ‰¹æ¬¡å¤§å°è®¡ç®—: åŸºç¡€={base_batch}, çº¿ç¨‹æ•°={num_workers}, å€æ•°={multiplier}, æœ€ç»ˆ={optimal_batch}")
    
    return optimal_batch

def main():
    # æ£€æŸ¥å¹¶é…ç½®ç¯å¢ƒ
    num_workers = check_environment()
    
    # å¼ºåˆ¶ç¦ç”¨æ··åˆç²¾åº¦ï¼Œç¡®ä¿CPUå’ŒGPUæ•°å€¼ä¸€è‡´æ€§
    tf.keras.mixed_precision.set_global_policy('float32')
    tf.keras.backend.set_floatx('float32')
    print("å¼ºåˆ¶ä½¿ç”¨float32ç²¾åº¦è®­ç»ƒï¼ˆç¦ç”¨æ··åˆç²¾åº¦ï¼‰")
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_training_args()
    
    # å¦‚æœç”¨æˆ·è¦æ±‚è¿è¡ŒCPUè¯Šæ–­ï¼Œå…ˆè¿è¡Œè¯Šæ–­ç„¶åé€€å‡º
    if args.diagnose_cpu:
        print("ğŸ” è¿è¡ŒCPUæ€§èƒ½è¯Šæ–­...")
        comprehensive_performance_test()
        return
    
    # è·å–æ•°æ®é›†è·¯å¾„
    paths = get_dataset_paths(args.dataset)
    dataset_dir = paths['dataset_dir']
    model_name = paths['model_name']
    best_model_name = paths['best_model_name']
    dataset_path = paths['dataset_path']
    
    # åˆ›å»ºå’Œä¿å­˜è®­ç»ƒé…ç½®
    training_config = create_training_config(
        state_dim=args.state_dim,
        input_dim=6,
        hidden_dim=args.hidden_dim,
        learning_rate=args.learning_rate,
        target_sequence_length=1000,
        epochs=args.epochs,
        batch_size=args.batch_size,
        save_frequency=args.save_frequency
    )
    save_training_config(training_config, dataset_dir)
    
    # åŠ è½½æ•°æ®é›†
    print(f"å°è¯•åŠ è½½æ•°æ®é›†: {dataset_path}")
    X_paths, Y_paths = load_dataset_from_npz(dataset_path)
    if X_paths is None or Y_paths is None:
        raise ValueError("æœªèƒ½æˆåŠŸåŠ è½½æ•°æ®é›†")
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    print("å‡†å¤‡è®­ç»ƒåºåˆ—...")
    init_states = np.zeros((len(X_paths), training_config['STATE_DIM']), dtype=np.float32)
    
    # éšæœºæ‰“ä¹±åºåˆ—
    print("éšæœºæ‰“ä¹±è®­ç»ƒåºåˆ—...")
    np.random.seed(training_config['random_seed'])
    indices = np.random.permutation(len(X_paths))
    X_paths = [X_paths[i] for i in indices]
    Y_paths = [Y_paths[i] for i in indices]
    init_states = init_states[indices]
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_size = int(training_config['train_test_split_ratio'] * len(X_paths))
    X_train = X_paths[:train_size]
    Y_train = Y_paths[:train_size]
    init_states_train = init_states[:train_size]
    
    X_val = X_paths[train_size:]
    Y_val = Y_paths[train_size:]
    init_states_val = init_states[train_size:]
    
    print(f"è®­ç»ƒé›†å¤§å°: {len(X_train)}")
    print(f"éªŒè¯é›†å¤§å°: {len(X_val)}")
    
    # ç¡®å®šæ‰¹å¤„ç†å¤§å°ï¼šä¼˜å…ˆä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„batch_sizeï¼Œå¦åˆ™è‡ªåŠ¨è®¡ç®—
    if args.batch_size is not None:
        batch_size = args.batch_size
        optimal_batch_size = get_optimal_batch_size(len(X_train), num_workers)
        print(f"ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„batch_size: {batch_size}")
        if batch_size != optimal_batch_size:
            print(f"æ³¨æ„ï¼šå»ºè®®çš„batch_sizeä¸º: {optimal_batch_size}")
    else:
        batch_size = get_optimal_batch_size(len(X_train), num_workers)
        print(f"æœªæŒ‡å®šbatch_sizeï¼Œä½¿ç”¨è‡ªåŠ¨è®¡ç®—å€¼: {batch_size}")
    
    # åˆ›å»ºTensorFlowæ•°æ®é›† - é’ˆå¯¹CPUä¼˜åŒ–æ•°æ®åŠ è½½å¹¶è¡Œåº¦
    print("åˆ›å»ºTensorFlowæ•°æ®é›†...")
    
    # ä¸ºCPUè®­ç»ƒä¼˜åŒ–æ•°æ®å¹¶è¡Œåº¦
    if num_workers is not None:  # CPUæ¨¡å¼
        data_parallel_calls = min(num_workers, 16)  # é™åˆ¶æœ€å¤§å¹¶è¡Œåº¦é¿å…è¿‡åº¦ç«äº‰
        prefetch_buffer = min(batch_size * 4, 64)  # é¢„å–ç¼“å†²åŒº
        print(f"CPUä¼˜åŒ–: æ•°æ®å¹¶è¡Œåº¦={data_parallel_calls}, é¢„å–ç¼“å†²={prefetch_buffer}")
    else:  # GPUæ¨¡å¼
        data_parallel_calls = tf.data.AUTOTUNE
        prefetch_buffer = tf.data.AUTOTUNE
    
    train_dataset = create_tf_dataset(
        X_train, Y_train, init_states_train,
        batch_size=batch_size,
        shuffle=True,
        num_parallel_calls=data_parallel_calls
    ).prefetch(prefetch_buffer)
    
    val_dataset = create_tf_dataset(
        X_val, Y_val, init_states_val,
        batch_size=batch_size,
        shuffle=False,
        num_parallel_calls=data_parallel_calls
    ).prefetch(prefetch_buffer)
    
    print(f"æ•°æ®åŠ è½½é…ç½®:")
    print(f"- æœ€ç»ˆä½¿ç”¨çš„æ‰¹å¤„ç†å¤§å°: {batch_size}")
    print(f"- æ•°æ®åŠ è½½çº¿ç¨‹æ•°: {num_workers if num_workers is not None else 'GPUæ¨¡å¼'}")
    
    # è®¡ç®—æœ€å¤§åºåˆ—é•¿åº¦
    max_seq_length = max(len(x) for x in X_paths)
    print(f"æ•°æ®é›†ä¸­æœ€å¤§åºåˆ—é•¿åº¦: {max_seq_length}")
    
    # åŠ è½½æˆ–åˆ›å»ºæ¨¡å‹
    epoch_offset = 0
    is_new_model = None
    if args.resume:
        print("å°è¯•æ¢å¤è®­ç»ƒ...")
        model, epoch_offset = resume_training_from_checkpoint(
            model_path=dataset_dir,
            model_name=model_name,
            best_model_name=best_model_name,
            resume_from_best=True,
            state_dim=args.state_dim,
            input_dim=6,
            output_dim=1,
            hidden_dim=args.hidden_dim,
            num_internal_layers=2,
            max_sequence_length=max_seq_length
        )
        
        if model is None:
            print("æ— æ³•æ¢å¤è®­ç»ƒï¼Œå°†åˆ›å»ºæ–°æ¨¡å‹")
            model, is_new_model = load_or_create_model_with_history(
                model_path=dataset_dir,
                model_name=model_name,
                best_model_name=best_model_name,
                state_dim=args.state_dim,
                input_dim=6,
                output_dim=1,
                hidden_dim=args.hidden_dim,
                num_internal_layers=2,
                max_sequence_length=max_seq_length
            )
    else:
        print("ä»å¤´å¼€å§‹è®­ç»ƒ...")
        model, is_new_model = load_or_create_model_with_history(
            model_path=dataset_dir,
            model_name=model_name,
            best_model_name=best_model_name,
            state_dim=args.state_dim,
            input_dim=6,
            output_dim=1,
            hidden_dim=args.hidden_dim,
            num_internal_layers=2,
            max_sequence_length=max_seq_length
        )
    
    # ç¼–è¯‘æ¨¡å‹
    optimizer = Adam(args.learning_rate)
    custom_loss = EMSCLoss(state_dim=args.state_dim)
    model.compile(
        optimizer=optimizer,
        loss=custom_loss,
        # æš‚æ—¶ç¦ç”¨JITç¼–è¯‘ä»¥é¿å…XLAè¦æ±‚å›ºå®štensorå¤§å°çš„é—®é¢˜
        jit_compile=False
    )
    
    if is_new_model:
        model.summary()
    
    # åˆ›å»ºå›è°ƒ
    progress_callback = MSCProgressCallback(
        save_path=dataset_dir,
        model_name=model_name,
        best_model_name=best_model_name,
        save_frequency=args.save_frequency
    )
    
    early_stopping = create_early_stopping_callback()
    
    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    lr_scheduler = create_learning_rate_scheduler(
        initial_learning_rate=args.learning_rate,
        decay_type='validation',  # ä½¿ç”¨åŸºäºéªŒè¯æŸå¤±çš„åŠ¨æ€è°ƒæ•´
        decay_steps=args.epochs,  # æ€»epochsæ•°
        decay_rate=0.9,          # æŒ‡æ•°è¡°å‡ç‡ï¼ˆå½“ä½¿ç”¨exponentialæ—¶ï¼‰
        min_learning_rate=1e-6,  # æœ€å°å­¦ä¹ ç‡
        patience=5,              # éªŒè¯æŸå¤±ä¸æ”¹å–„çš„å®¹å¿è½®æ•°
        factor=0.5,             # å­¦ä¹ ç‡è¡°å‡å› å­
        verbose=1               # æ‰“å°å­¦ä¹ ç‡å˜åŒ–
    )
    
    # åˆ›å»ºCPUç›‘æ§å›è°ƒï¼ˆä»…CPUè®­ç»ƒæ¨¡å¼ä¸”ç”¨æˆ·å¯ç”¨æ—¶ï¼Œä¸”ä¸ä¸åŠ¨æ€æ‰¹æ¬¡å†²çªï¼‰
    cpu_monitor = None
    if num_workers is not None and args.monitor_cpu and not args.dynamic_batch:
        cpu_monitor = create_cpu_monitor_callback(monitor_interval=30, verbose=True)
        print("å·²å¯ç”¨CPUä½¿ç”¨ç‡ç›‘æ§")
    elif num_workers is not None and args.monitor_cpu and args.dynamic_batch:
        print("æ³¨æ„ï¼šåŠ¨æ€æ‰¹æ¬¡è°ƒæ•´å·²åŒ…å«CPUç›‘æ§åŠŸèƒ½ï¼Œ--monitor_cpuå°†è¢«å¿½ç•¥")
    
    # å‡†å¤‡å›è°ƒåˆ—è¡¨
    callbacks = [progress_callback, early_stopping, lr_scheduler]
    if cpu_monitor is not None:
        callbacks.append(cpu_monitor)
    
    # è®­ç»ƒæ¨¡å‹
    remaining_epochs = args.epochs
    if remaining_epochs <= 0:
        print(f"æ¨¡å‹å·²ç»è®­ç»ƒäº† {epoch_offset} epochsï¼Œè¾¾åˆ°è®¾å®šçš„æ€»epochs {args.epochs}")
        print("å¦‚éœ€ç»§ç»­è®­ç»ƒï¼Œè¯·å¢åŠ æ€»epochsæ•°")
    else:
        print(f"\nå¼€å§‹è®­ç»ƒ MSC æ¨¡å‹...")
        print(f"å·²å®Œæˆepochs: {epoch_offset}")
        print(f"å‰©ä½™epochs: {remaining_epochs}")
        print(f"æ€»epochsç›®æ ‡: {args.epochs + epoch_offset}")
        print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"ä¿å­˜é¢‘ç‡: æ¯ {args.save_frequency} epochs")
        print(f"æ—©åœè®¾ç½®: patience={15}, min_delta={1e-4}")
        print(f"å­¦ä¹ ç‡è°ƒåº¦: åˆå§‹={args.learning_rate}, æœ€å°={1e-6}, åŠ¨æ€è°ƒæ•´")
        print(f"æ¨¡å‹ä¿å­˜è·¯å¾„: {dataset_dir}")
        print(f"è®­ç»ƒæ•°æ®å¤§å°: {len(X_train)}")
        print(f"éªŒè¯æ•°æ®å¤§å°: {len(X_val)}")
        print(f"è®­ç»ƒæ¨¡å¼: {'GPU' if num_workers is None else 'CPU (å¤šçº¿ç¨‹)'}")
        
        # ä½¿ç”¨æ€§èƒ½ä¼˜åŒ–çš„è®­ç»ƒé…ç½® - é’ˆå¯¹é˜¿é‡Œäº‘CPUä¼˜åŒ–
        if num_workers is not None and args.dynamic_batch:  # CPUæ¨¡å¼ + åŠ¨æ€æ‰¹æ¬¡
            print(f"ğŸš€ å¯ç”¨åŠ¨æ€æ‰¹æ¬¡å¤§å°è°ƒæ•´ (ç›®æ ‡CPUä½¿ç”¨ç‡: {args.target_cpu_usage}%)")
            
            # ä½¿ç”¨åŠ¨æ€æ‰¹æ¬¡è®­ç»ƒå™¨
            dynamic_trainer = DynamicBatchTrainer(
                model=model,
                train_data_info=(X_train, Y_train, init_states_train),
                val_data_info=(X_val, Y_val, init_states_val),
                initial_batch_size=batch_size
            )
            
            # æ·»åŠ åŠ¨æ€æ‰¹æ¬¡å›è°ƒï¼ˆæ›¿æ¢CPUç›‘æ§ï¼‰
            dynamic_callbacks = [progress_callback, early_stopping, lr_scheduler]
            dynamic_callbacks.append(create_dynamic_batch_callback(
                target_cpu_usage=args.target_cpu_usage,
                min_batch_size=16,
                max_batch_size=min(512, len(X_train)),
                verbose=True
            ))
            
            history = dynamic_trainer.fit(
                epochs=args.epochs,
                initial_epoch=epoch_offset,
                verbose=1,
                callbacks=dynamic_callbacks,
                use_multiprocessing=True,
                workers=min(num_workers, 32),
                max_queue_size=max(20, num_workers * 2)
            )
            
        elif num_workers is not None:  # CPUæ¨¡å¼ - ä¼ ç»Ÿè®­ç»ƒ
            # CPUè®­ç»ƒé…ç½® - æ›´æ¿€è¿›çš„å¤šè¿›ç¨‹è®¾ç½®
            max_queue = max(20, num_workers * 2)  # å¢åŠ é˜Ÿåˆ—å¤§å°
            cpu_workers = min(num_workers, 32)    # é™åˆ¶æœ€å¤§è¿›ç¨‹æ•°é¿å…è¿‡åº¦å¼€é”€
            print(f"CPUè®­ç»ƒé…ç½®: workers={cpu_workers}, max_queue_size={max_queue}")
            
            history = model.fit(
                 train_dataset,
                 validation_data=val_dataset,
                 epochs=args.epochs,
                 initial_epoch=epoch_offset,
                 verbose=1,
                 callbacks=callbacks,
                 use_multiprocessing=True,  # å¯ç”¨å¤šè¿›ç¨‹
                 workers=cpu_workers,
                 max_queue_size=max_queue
             )
        else:  # GPUæ¨¡å¼
            history = model.fit(
                train_dataset,
                validation_data=val_dataset,
                epochs=args.epochs,
                initial_epoch=epoch_offset,
                verbose=1,
                callbacks=callbacks,
                use_multiprocessing=False,
                workers=1,
                max_queue_size=10
            )
        
        # è®­ç»ƒå®Œæˆåæœ€ç»ˆä¿å­˜
        print("\nè®­ç»ƒå®Œæˆï¼Œæ‰§è¡Œæœ€ç»ˆä¿å­˜...")
        final_model_path = progress_callback._safe_save_model(model, is_best=False)
        
        # ä¿å­˜æœ€ç»ˆçš„è®­ç»ƒå†å²å’Œå›¾è¡¨
        progress_callback._save_training_history()
        progress_callback._plot_training_history()
        
        # ç»˜åˆ¶æœ€ç»ˆè®­ç»ƒæ€»ç»“
        plot_final_training_summary(
            history, epoch_offset, args.epochs,
            progress_callback, dataset_dir
        )
        
        # æ‰“å°è®­ç»ƒæ€»ç»“
        print_training_summary(
            progress_callback, dataset_dir,
            best_model_name, model_name
        )

if __name__ == '__main__':
    main()