"""
EMSCè®­ç»ƒç»“æœOSSä¸Šä¼ æ¨¡å—
åœ¨äº‘ç¯å¢ƒä¸‹å°†è®­ç»ƒç»“æœå‹ç¼©å¹¶ä¸Šä¼ åˆ°OSS
"""

import os
import json
import zipfile
import shutil
import time
from datetime import datetime
from pathlib import Path
import oss2
from oss2.exceptions import OssError
from .EMSC_oss_config import load_oss_config

class EMSCOSSUploader:
    """EMSCè®­ç»ƒç»“æœOSSä¸Šä¼ å™¨"""
    
    def __init__(self, oss_config_path=None):
        """
        åˆå§‹åŒ–OSSä¸Šä¼ å™¨
        
        Args:
            oss_config_path: OSSé…ç½®æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æŸ¥æ‰¾
        """
        self.oss_config_path = oss_config_path or self._find_oss_config()
        self.bucket = None
        self.config = None
        
        if self.oss_config_path and os.path.exists(self.oss_config_path):
            self._load_oss_config()
        else:
            print(f"âš ï¸  OSSé…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {self.oss_config_path}")
    
    def _find_oss_config(self):
        """è‡ªåŠ¨æŸ¥æ‰¾OSSé…ç½®æ–‡ä»¶"""
        possible_paths = [
            '/mnt/data/msc_models/dataset_EMSC_big/oss_config.json',
            './oss_config.json',
            '../oss_config.json',
            os.path.expanduser('~/oss_config.json')
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                return path
        
        return None
    
    def _load_oss_config(self):
        """åŠ è½½OSSé…ç½® - å¤ç”¨ç°æœ‰çš„OSSä¸‹è½½é…ç½®"""
        try:
            # ä½¿ç”¨ç°æœ‰çš„OSSé…ç½®åŠ è½½å‡½æ•°
            self.config = load_oss_config(self.oss_config_path)
            
            # åˆå§‹åŒ–OSSå®¢æˆ·ç«¯
            auth = oss2.Auth(
                self.config['access_key_id'], 
                self.config['access_key_secret']
            )
            
            self.bucket = oss2.Bucket(
                auth, 
                self.config['endpoint'], 
                self.config['bucket_name']
            )
            
            print(f"âœ… OSSé…ç½®åŠ è½½æˆåŠŸ (å¤ç”¨ç°æœ‰é…ç½®)")
            print(f"   Bucket: {self.config['bucket_name']}")
            print(f"   Endpoint: {self.config['endpoint']}")
            
        except Exception as e:
            print(f"âŒ OSSé…ç½®åŠ è½½å¤±è´¥: {e}")
            self.bucket = None
            self.config = None
    
    def create_training_archive(self, source_dir, output_filename=None, include_patterns=None, exclude_patterns=None):
        """
        åˆ›å»ºè®­ç»ƒç»“æœå‹ç¼©åŒ…
        
        Args:
            source_dir: æºç›®å½•è·¯å¾„
            output_filename: è¾“å‡ºæ–‡ä»¶åï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
            include_patterns: åŒ…å«çš„æ–‡ä»¶æ¨¡å¼åˆ—è¡¨
            exclude_patterns: æ’é™¤çš„æ–‡ä»¶æ¨¡å¼åˆ—è¡¨
        
        Returns:
            str: å‹ç¼©åŒ…è·¯å¾„
        """
        if not os.path.exists(source_dir):
            raise ValueError(f"æºç›®å½•ä¸å­˜åœ¨: {source_dir}")
        
        # è‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶å
        if output_filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            dir_name = os.path.basename(source_dir.rstrip('/'))
            output_filename = f"{dir_name}_{timestamp}.zip"
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(output_filename) or '.'
        os.makedirs(output_dir, exist_ok=True)
        
        # é»˜è®¤åŒ…å«å’Œæ’é™¤æ¨¡å¼
        if include_patterns is None:
            include_patterns = [
                '*.h5',           # æ¨¡å‹æ–‡ä»¶
                '*.keras',        # Kerasæ¨¡å‹æ–‡ä»¶
                '*.json',         # é…ç½®æ–‡ä»¶
                '*.png',          # å›¾è¡¨æ–‡ä»¶
                '*.jpg',          # å›¾è¡¨æ–‡ä»¶
                '*.txt',          # æ—¥å¿—æ–‡ä»¶
                '*.csv',          # è®­ç»ƒå†å²
                '*.md',           # æ–‡æ¡£æ–‡ä»¶
                'saved_model/*',  # SavedModelæ ¼å¼
                'best_*',         # æœ€ä½³æ¨¡å‹
                'training_*',     # è®­ç»ƒç›¸å…³æ–‡ä»¶
                'history_*',      # å†å²æ–‡ä»¶
                'checkpoint*'     # æ£€æŸ¥ç‚¹æ–‡ä»¶
            ]
        
        if exclude_patterns is None:
            exclude_patterns = [
                '*.pyc',          # Pythonç¼“å­˜æ–‡ä»¶
                '__pycache__/*',  # Pythonç¼“å­˜ç›®å½•
                '*.tmp',          # ä¸´æ—¶æ–‡ä»¶
                '*.log',          # å¤§å‹æ—¥å¿—æ–‡ä»¶
                '.DS_Store',      # macOSç³»ç»Ÿæ–‡ä»¶
                'Thumbs.db'       # Windowsç³»ç»Ÿæ–‡ä»¶
            ]
        
        print(f"ğŸ“¦ åˆ›å»ºè®­ç»ƒç»“æœå‹ç¼©åŒ…...")
        print(f"   æºç›®å½•: {source_dir}")
        print(f"   è¾“å‡ºæ–‡ä»¶: {output_filename}")
        
        total_files = 0
        total_size = 0
        
        try:
            with zipfile.ZipFile(output_filename, 'w', zipfile.ZIP_DEFLATED, compresslevel=6) as zipf:
                for root, dirs, files in os.walk(source_dir):
                    for file in files:
                        file_path = os.path.join(root, file)
                        relative_path = os.path.relpath(file_path, source_dir)
                        
                        # æ£€æŸ¥æ˜¯å¦åº”è¯¥åŒ…å«æ­¤æ–‡ä»¶
                        should_include = self._should_include_file(relative_path, include_patterns, exclude_patterns)
                        
                        if should_include:
                            try:
                                zipf.write(file_path, relative_path)
                                file_size = os.path.getsize(file_path)
                                total_files += 1
                                total_size += file_size
                                
                                if total_files % 10 == 0:
                                    print(f"   å·²å¤„ç† {total_files} ä¸ªæ–‡ä»¶...")
                                    
                            except Exception as e:
                                print(f"   âš ï¸  è·³è¿‡æ–‡ä»¶ {relative_path}: {e}")
            
            final_size = os.path.getsize(output_filename)
            compression_ratio = (1 - final_size / total_size) * 100 if total_size > 0 else 0
            
            print(f"âœ… å‹ç¼©åŒ…åˆ›å»ºå®Œæˆ:")
            print(f"   åŒ…å«æ–‡ä»¶: {total_files}")
            print(f"   åŸå§‹å¤§å°: {total_size / (1024*1024):.1f}MB")
            print(f"   å‹ç¼©å¤§å°: {final_size / (1024*1024):.1f}MB")
            print(f"   å‹ç¼©ç‡: {compression_ratio:.1f}%")
            
            return output_filename
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºå‹ç¼©åŒ…å¤±è´¥: {e}")
            if os.path.exists(output_filename):
                os.remove(output_filename)
            raise
    
    def _should_include_file(self, file_path, include_patterns, exclude_patterns):
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åº”è¯¥è¢«åŒ…å«"""
        import fnmatch
        
        # é¦–å…ˆæ£€æŸ¥æ’é™¤æ¨¡å¼
        for pattern in exclude_patterns:
            if fnmatch.fnmatch(file_path, pattern) or fnmatch.fnmatch(os.path.basename(file_path), pattern):
                return False
        
        # ç„¶åæ£€æŸ¥åŒ…å«æ¨¡å¼
        for pattern in include_patterns:
            if fnmatch.fnmatch(file_path, pattern) or fnmatch.fnmatch(os.path.basename(file_path), pattern):
                return True
        
        return False
    
    def upload_to_oss(self, local_file, oss_path=None, progress_callback=None):
        """
        ä¸Šä¼ æ–‡ä»¶åˆ°OSS
        
        Args:
            local_file: æœ¬åœ°æ–‡ä»¶è·¯å¾„
            oss_path: OSSè·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        
        Returns:
            str: OSSæ–‡ä»¶è·¯å¾„
        """
        if not self.bucket:
            raise ValueError("OSSæœªé…ç½®æˆ–é…ç½®æ— æ•ˆ")
        
        if not os.path.exists(local_file):
            raise ValueError(f"æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨: {local_file}")
        
        # è‡ªåŠ¨ç”ŸæˆOSSè·¯å¾„
        if oss_path is None:
            filename = os.path.basename(local_file)
            timestamp = datetime.now().strftime("%Y/%m/%d")
            oss_path = f"emsc_training_results/{timestamp}/{filename}"
        
        file_size = os.path.getsize(local_file)
        print(f"ğŸ“¤ ä¸Šä¼ æ–‡ä»¶åˆ°OSS...")
        print(f"   æœ¬åœ°æ–‡ä»¶: {local_file}")
        print(f"   OSSè·¯å¾„: {oss_path}")
        print(f"   æ–‡ä»¶å¤§å°: {file_size / (1024*1024):.1f}MB")
        
        try:
            start_time = time.time()
            
            # ä½¿ç”¨åˆ†ç‰‡ä¸Šä¼ å¤„ç†å¤§æ–‡ä»¶
            if file_size > 100 * 1024 * 1024:  # 100MBä»¥ä¸Šä½¿ç”¨åˆ†ç‰‡ä¸Šä¼ 
                print("   ä½¿ç”¨åˆ†ç‰‡ä¸Šä¼ ...")
                self._multipart_upload(local_file, oss_path, progress_callback)
            else:
                print("   ä½¿ç”¨æ™®é€šä¸Šä¼ ...")
                with open(local_file, 'rb') as fileobj:
                    self.bucket.put_object(oss_path, fileobj)
            
            upload_time = time.time() - start_time
            upload_speed = file_size / upload_time / (1024*1024)  # MB/s
            
            print(f"âœ… ä¸Šä¼ å®Œæˆ:")
            print(f"   è€—æ—¶: {upload_time:.1f}ç§’")
            print(f"   é€Ÿåº¦: {upload_speed:.1f}MB/s")
            print(f"   OSS URL: https://{self.config['bucket_name']}.{self.config['endpoint'].replace('https://', '')}/{oss_path}")
            
            return oss_path
            
        except OssError as e:
            print(f"âŒ OSSä¸Šä¼ å¤±è´¥: {e}")
            raise
        except Exception as e:
            print(f"âŒ ä¸Šä¼ å¤±è´¥: {e}")
            raise
    
    def _multipart_upload(self, local_file, oss_path, progress_callback=None):
        """åˆ†ç‰‡ä¸Šä¼ å¤§æ–‡ä»¶"""
        try:
            # åˆå§‹åŒ–åˆ†ç‰‡ä¸Šä¼ 
            upload_id = self.bucket.init_multipart_upload(oss_path).upload_id
            
            parts = []
            part_size = 10 * 1024 * 1024  # 10MB per part
            file_size = os.path.getsize(local_file)
            part_count = (file_size + part_size - 1) // part_size
            
            print(f"   åˆ†ç‰‡æ•°é‡: {part_count}")
            
            with open(local_file, 'rb') as fileobj:
                for part_number in range(1, part_count + 1):
                    data = fileobj.read(part_size)
                    if not data:
                        break
                    
                    result = self.bucket.upload_part(oss_path, upload_id, part_number, data)
                    parts.append(oss2.models.PartInfo(part_number, result.etag))
                    
                    if progress_callback:
                        progress = part_number / part_count * 100
                        progress_callback(progress)
                    
                    if part_number % 10 == 0:
                        print(f"   å·²ä¸Šä¼  {part_number}/{part_count} åˆ†ç‰‡...")
            
            # å®Œæˆåˆ†ç‰‡ä¸Šä¼ 
            self.bucket.complete_multipart_upload(oss_path, upload_id, parts)
            
        except Exception as e:
            # å¦‚æœå¤±è´¥ï¼Œå–æ¶ˆåˆ†ç‰‡ä¸Šä¼ 
            try:
                self.bucket.abort_multipart_upload(oss_path, upload_id)
            except:
                pass
            raise
    
    def upload_training_results(self, training_dir, cleanup_local=True):
        """
        å®Œæ•´çš„è®­ç»ƒç»“æœä¸Šä¼ æµç¨‹
        
        Args:
            training_dir: è®­ç»ƒç»“æœç›®å½•
            cleanup_local: æ˜¯å¦æ¸…ç†æœ¬åœ°å‹ç¼©åŒ…
        
        Returns:
            dict: ä¸Šä¼ ç»“æœä¿¡æ¯
        """
        if not self.bucket:
            print("âš ï¸  OSSæœªé…ç½®ï¼Œè·³è¿‡ä¸Šä¼ ")
            return None
        
        try:
            # 1. åˆ›å»ºå‹ç¼©åŒ…
            archive_path = self.create_training_archive(training_dir)
            
            # 2. ä¸Šä¼ åˆ°OSS
            oss_path = self.upload_to_oss(archive_path)
            
            # 3. æ¸…ç†æœ¬åœ°å‹ç¼©åŒ…ï¼ˆå¯é€‰ï¼‰
            if cleanup_local:
                os.remove(archive_path)
                print(f"ğŸ—‘ï¸  å·²æ¸…ç†æœ¬åœ°å‹ç¼©åŒ…: {archive_path}")
            
            # 4. è¿”å›ç»“æœä¿¡æ¯
            result = {
                'success': True,
                'training_dir': training_dir,
                'archive_path': archive_path if not cleanup_local else None,
                'oss_path': oss_path,
                'upload_time': datetime.now().isoformat(),
                'oss_url': f"https://{self.config['bucket_name']}.{self.config['endpoint'].replace('https://', '')}/{oss_path}"
            }
            
            print(f"ğŸ‰ è®­ç»ƒç»“æœä¸Šä¼ å®Œæˆ!")
            print(f"   OSSè·¯å¾„: {oss_path}")
            
            return result
            
        except Exception as e:
            print(f"âŒ è®­ç»ƒç»“æœä¸Šä¼ å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'training_dir': training_dir
            }

def check_oss_config_exists():
    """æ£€æŸ¥ç°æœ‰OSSé…ç½®æ–‡ä»¶"""
    possible_paths = [
        '/mnt/data/msc_models/dataset_EMSC_big/oss_config.json',
        './oss_config.json',
        '../oss_config.json',
        os.path.expanduser('~/oss_config.json')
    ]
    
    existing_configs = []
    for path in possible_paths:
        if os.path.exists(path):
            existing_configs.append(path)
    
    return existing_configs

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æ£€æŸ¥ç°æœ‰é…ç½®
    existing_configs = check_oss_config_exists()
    if existing_configs:
        print(f"å‘ç°ç°æœ‰OSSé…ç½®: {existing_configs[0]}")
        
        # åˆ›å»ºä¸Šä¼ å™¨
        uploader = EMSCOSSUploader(existing_configs[0])
        
        # ä¸Šä¼ è®­ç»ƒç»“æœ
        training_dir = "./network_6-32-32-8-1"
        result = uploader.upload_training_results(training_dir)
        
        if result and result['success']:
            print(f"âœ… ä¸Šä¼ æˆåŠŸ: {result['oss_url']}")
        else:
            print("âŒ ä¸Šä¼ å¤±è´¥")
    else:
        print("âŒ æœªæ‰¾åˆ°OSSé…ç½®æ–‡ä»¶ï¼Œè¯·å…ˆé…ç½®OSSä¸‹è½½åŠŸèƒ½") 