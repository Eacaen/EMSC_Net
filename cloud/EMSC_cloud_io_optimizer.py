"""
é˜¿é‡Œäº‘ç¯å¢ƒæ•°æ®I/Oä¼˜åŒ–æ¨¡å—
ä¸“é—¨è§£å†³äº‘ç¯å¢ƒCPUä½¿ç”¨ç‡ä½çš„é—®é¢˜
"""

import os
import time
import threading
import multiprocessing as mp
import numpy as np
import tensorflow as tf
from queue import Queue
from concurrent.futures import ThreadPoolExecutor, as_completed


class CloudIOOptimizer:
    """
    é˜¿é‡Œäº‘ç¯å¢ƒI/Oä¼˜åŒ–å™¨
    ä¸»è¦è§£å†³ï¼š
    1. äº‘ç›˜IOPSé™åˆ¶å¯¼è‡´çš„I/Oç“¶é¢ˆ
    2. æ•°æ®åŠ è½½ç®¡é“æ•ˆç‡ä½ä¸‹
    3. CPUç­‰å¾…I/Oå¯¼è‡´ä½¿ç”¨ç‡ä½
    """
    
    def __init__(self, 
                 io_buffer_size=64,      # I/Oç¼“å†²åŒºå¤§å° 
                 prefetch_factor=8,      # é¢„å–å› å­ï¼ˆäº‘ç¯å¢ƒéœ€è¦æ›´æ¿€è¿›ï¼‰
                 io_threads=16,          # I/Oçº¿ç¨‹æ•°ï¼ˆäº‘ç¯å¢ƒå¢åŠ ï¼‰
                 memory_cache_size=512,  # å†…å­˜ç¼“å­˜å¤§å°(MB)
                 use_ramdisk=False):     # æ˜¯å¦ä½¿ç”¨å†…å­˜ç›˜
        
        self.io_buffer_size = io_buffer_size
        self.prefetch_factor = prefetch_factor
        self.io_threads = io_threads
        self.memory_cache_size = memory_cache_size
        self.use_ramdisk = use_ramdisk
        
        # åˆ›å»ºI/Oçº¿ç¨‹æ± 
        self.io_executor = ThreadPoolExecutor(max_workers=io_threads)
        
        # å†…å­˜ç¼“å­˜
        self.memory_cache = {}
        self.cache_lock = threading.Lock()
        
        print(f"ğŸ”§ é˜¿é‡Œäº‘I/Oä¼˜åŒ–å™¨åˆå§‹åŒ–:")
        print(f"- I/Oç¼“å†²åŒº: {io_buffer_size}")
        print(f"- é¢„å–å› å­: {prefetch_factor}x")
        print(f"- I/Oçº¿ç¨‹æ•°: {io_threads}")
        print(f"- å†…å­˜ç¼“å­˜: {memory_cache_size}MB")
    
    def optimize_tensorflow_io(self):
        """ä¼˜åŒ–TensorFlowçš„I/Oé…ç½®"""
        
        # 1. è®¾ç½®TensorFlow I/Oä¼˜åŒ–
        os.environ['TF_DATA_EXPERIMENTAL_IO_MEMORY_BUFFER_SIZE'] = str(self.memory_cache_size * 1024 * 1024)
        os.environ['TF_DATA_EXPERIMENTAL_SLACK'] = 'True'
        
        # 2. äº‘ç¯å¢ƒä¸“ç”¨é…ç½®
        os.environ['TF_DATA_AUTOTUNE_MEMORY_BUDGET'] = str(self.memory_cache_size * 1024 * 1024)
        
        # 3. æ¿€è¿›çš„å¹¶è¡Œé…ç½®
        tf.config.threading.set_inter_op_parallelism_threads(self.io_threads)
        
        print(f"âœ… TensorFlow I/Oä¼˜åŒ–é…ç½®å®Œæˆ")
    
    def create_optimized_dataset(self, X_paths, Y_paths, init_states, batch_size):
        """
        åˆ›å»ºé’ˆå¯¹äº‘ç¯å¢ƒä¼˜åŒ–çš„æ•°æ®é›†
        """
        print(f"ğŸš€ åˆ›å»ºäº‘ç¯å¢ƒä¼˜åŒ–æ•°æ®é›†...")
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = tf.data.Dataset.from_tensor_slices((
            {'delta_input': X_paths, 'init_state': init_states},
            Y_paths
        ))
        
        # 1. æ¿€è¿›çš„ç¼“å­˜ç­–ç•¥
        dataset = dataset.cache()  # å…¨é‡ç¼“å­˜åˆ°å†…å­˜
        
        # 2. æ›´å¤§çš„shuffle bufferï¼ˆäº‘ç¯å¢ƒç½‘ç»œå»¶è¿Ÿé«˜ï¼‰
        dataset = dataset.shuffle(
            buffer_size=min(len(X_paths), 10000),  # æ›´å¤§çš„shuffle buffer
            reshuffle_each_iteration=True
        )
        
        # 3. æ‰¹å¤„ç†
        dataset = dataset.batch(batch_size, drop_remainder=False)
        
        # 4. è¶…æ¿€è¿›çš„é¢„å–ï¼ˆäº‘ç¯å¢ƒå…³é”®ä¼˜åŒ–ï¼‰
        dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)
        
        # 5. å¹¶è¡Œæ•°æ®å¤„ç†
        dataset = dataset.map(
            lambda x, y: (x, y),
            num_parallel_calls=tf.data.AUTOTUNE,
            deterministic=False  # å…è®¸éç¡®å®šæ€§ä»¥æé«˜æ€§èƒ½
        )
        
        # 6. å†æ¬¡é¢„å–ï¼ˆåŒé‡é¢„å–ç­–ç•¥ï¼‰
        dataset = dataset.prefetch(buffer_size=self.prefetch_factor)
        
        print(f"âœ… äº‘ç¯å¢ƒæ•°æ®é›†ä¼˜åŒ–å®Œæˆ")
        return dataset
    
    def diagnose_io_bottleneck(self, data_path):
        """
        è¯Šæ–­I/Oç“¶é¢ˆ
        """
        print(f"ğŸ” å¼€å§‹I/Oç“¶é¢ˆè¯Šæ–­...")
        
        # 1. ç£ç›˜è¯»å–æ€§èƒ½æµ‹è¯•
        start_time = time.time()
        test_size = 100 * 1024 * 1024  # 100MBæµ‹è¯•
        
        try:
            # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
            test_file = os.path.join(data_path, 'io_test.tmp')
            test_data = np.random.bytes(test_size)
            
            # å†™å…¥æµ‹è¯•
            write_start = time.time()
            with open(test_file, 'wb') as f:
                f.write(test_data)
            write_time = time.time() - write_start
            
            # è¯»å–æµ‹è¯•
            read_start = time.time()
            with open(test_file, 'rb') as f:
                _ = f.read()
            read_time = time.time() - read_start
            
            # æ¸…ç†æµ‹è¯•æ–‡ä»¶
            os.remove(test_file)
            
            write_speed = test_size / write_time / (1024*1024)  # MB/s
            read_speed = test_size / read_time / (1024*1024)   # MB/s
            
            print(f"ğŸ“Š ç£ç›˜æ€§èƒ½æµ‹è¯•ç»“æœ:")
            print(f"- å†™å…¥é€Ÿåº¦: {write_speed:.1f} MB/s")
            print(f"- è¯»å–é€Ÿåº¦: {read_speed:.1f} MB/s")
            
            # æ€§èƒ½å»ºè®®
            if read_speed < 50:
                print(f"âš ï¸  è¯»å–é€Ÿåº¦è¾ƒæ…¢ï¼Œå»ºè®®:")
                print(f"   - å‡çº§åˆ°SSDäº‘ç›˜")
                print(f"   - å¢åŠ IOPSé…ç½®")
                print(f"   - ä½¿ç”¨æœ¬åœ°SSD")
            
            if write_speed < 30:
                print(f"âš ï¸  å†™å…¥é€Ÿåº¦è¾ƒæ…¢ï¼Œå¯èƒ½å½±å“æ¨¡å‹ä¿å­˜")
            
        except Exception as e:
            print(f"âŒ I/Oæµ‹è¯•å¤±è´¥: {e}")
    
    def optimize_cloud_environment(self):
        """
        äº‘ç¯å¢ƒç»¼åˆä¼˜åŒ–
        """
        print(f"ğŸ”§ å¼€å§‹äº‘ç¯å¢ƒç»¼åˆä¼˜åŒ–...")
        
        # 1. ç³»ç»Ÿçº§ä¼˜åŒ–
        self._optimize_system_io()
        
        # 2. TensorFlowä¼˜åŒ–
        self.optimize_tensorflow_io()
        
        # 3. å†…å­˜ä¼˜åŒ–
        self._optimize_memory()
        
        print(f"âœ… äº‘ç¯å¢ƒä¼˜åŒ–å®Œæˆ")
    
    def _optimize_system_io(self):
        """ç³»ç»Ÿçº§I/Oä¼˜åŒ–"""
        try:
            # è®¾ç½®I/Oè°ƒåº¦å™¨ä¼˜åŒ–å‚æ•°
            io_optimizations = {
                'TF_DATA_EXPERIMENTAL_ENABLE_NUMA_AWARE_DATASETS': '1',
                'TF_DATA_EXPERIMENTAL_IO_THREAD_POOL_SIZE': str(self.io_threads),
                'TF_ENABLE_EAGER_CLIENT_STREAMING_ENQUEUE': '1',
            }
            
            for key, value in io_optimizations.items():
                os.environ[key] = value
                
            print(f"âœ… ç³»ç»ŸI/Oä¼˜åŒ–å®Œæˆ")
            
        except Exception as e:
            print(f"âš ï¸  ç³»ç»ŸI/Oä¼˜åŒ–éƒ¨åˆ†å¤±è´¥: {e}")
    
    def _optimize_memory(self):
        """å†…å­˜ä¼˜åŒ–"""
        try:
            # Pythonå†…å­˜ä¼˜åŒ–
            import gc
            gc.collect()
            
            # è®¾ç½®TensorFlowå†…å­˜å¢é•¿
            gpus = tf.config.list_physical_devices('GPU')
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            
            print(f"âœ… å†…å­˜ä¼˜åŒ–å®Œæˆ")
            
        except Exception as e:
            print(f"âš ï¸  å†…å­˜ä¼˜åŒ–éƒ¨åˆ†å¤±è´¥: {e}")


def create_cloud_optimized_training_config(num_samples, cpu_count):
    """
    ä¸ºé˜¿é‡Œäº‘ç¯å¢ƒåˆ›å»ºä¼˜åŒ–çš„è®­ç»ƒé…ç½®
    
    Args:
        num_samples: è®­ç»ƒæ ·æœ¬æ•°
        cpu_count: CPUæ ¸å¿ƒæ•°
    
    Returns:
        dict: ä¼˜åŒ–é…ç½®
    """
    
    # äº‘ç¯å¢ƒç‰¹æ®Šé…ç½®
    config = {
        # æ‰¹æ¬¡å¤§å°ï¼šäº‘ç¯å¢ƒä¼˜å…ˆè€ƒè™‘I/Oæ•ˆç‡
        'batch_size': max(64, min(256, num_samples // 20)),
        
        # I/Oé…ç½®ï¼šæ¿€è¿›çš„å¹¶è¡Œå’Œç¼“å­˜
        'io_threads': min(cpu_count * 2, 32),
        'prefetch_factor': 16,  # äº‘ç¯å¢ƒæ›´æ¿€è¿›çš„é¢„å–
        'cache_size': 1024,     # 1GBå†…å­˜ç¼“å­˜
        
        # æ•°æ®åŠ è½½é…ç½®
        'num_parallel_calls': tf.data.AUTOTUNE,
        'shuffle_buffer': min(num_samples, 8192),
        
        # è®­ç»ƒé…ç½®
        'workers': min(cpu_count, 16),  # é™åˆ¶è¿›ç¨‹æ•°
        'max_queue_size': cpu_count * 4,  # å¤§é˜Ÿåˆ—
        'use_multiprocessing': True,
    }
    
    print(f"ğŸŒ¥ï¸  é˜¿é‡Œäº‘è®­ç»ƒé…ç½®:")
    for key, value in config.items():
        print(f"- {key}: {value}")
    
    return config


def monitor_cloud_performance():
    """
    ç›‘æ§äº‘ç¯å¢ƒæ€§èƒ½æŒ‡æ ‡
    """
    try:
        import psutil
        
        print(f"ğŸ“Š äº‘ç¯å¢ƒæ€§èƒ½ç›‘æ§:")
        
        # CPUä¿¡æ¯
        cpu_percent = psutil.cpu_percent(interval=1, percpu=True)
        avg_cpu = sum(cpu_percent) / len(cpu_percent)
        print(f"- å¹³å‡CPUä½¿ç”¨ç‡: {avg_cpu:.1f}%")
        print(f"- CPUæ ¸å¿ƒä½¿ç”¨ç‡åˆ†å¸ƒ: {[f'{x:.1f}%' for x in cpu_percent]}")
        
        # å†…å­˜ä¿¡æ¯
        memory = psutil.virtual_memory()
        print(f"- å†…å­˜ä½¿ç”¨ç‡: {memory.percent:.1f}%")
        print(f"- å¯ç”¨å†…å­˜: {memory.available / (1024**3):.1f}GB")
        
        # ç£ç›˜I/O
        disk_io = psutil.disk_io_counters()
        print(f"- ç£ç›˜è¯»å–: {disk_io.read_bytes / (1024**3):.2f}GB")
        print(f"- ç£ç›˜å†™å…¥: {disk_io.write_bytes / (1024**3):.2f}GB")
        
        # ç½‘ç»œI/O
        net_io = psutil.net_io_counters()
        print(f"- ç½‘ç»œæ¥æ”¶: {net_io.bytes_recv / (1024**3):.2f}GB")
        print(f"- ç½‘ç»œå‘é€: {net_io.bytes_sent / (1024**3):.2f}GB")
        
        # è¯Šæ–­å»ºè®®
        if avg_cpu < 30:
            print(f"ğŸ’¡ CPUä½¿ç”¨ç‡åä½ï¼Œå¯èƒ½åŸå› :")
            print(f"   - I/Oç“¶é¢ˆï¼ˆç£ç›˜/ç½‘ç»œï¼‰")
            print(f"   - æ•°æ®åŠ è½½ç®¡é“æ•ˆç‡ä½")
            print(f"   - æ‰¹æ¬¡å¤§å°è¿‡å°")
            print(f"   - çº¿ç¨‹é…ç½®ä¸å½“")
        
        if memory.percent > 85:
            print(f"âš ï¸  å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå¯èƒ½å½±å“æ€§èƒ½")
        
    except ImportError:
        print(f"âŒ éœ€è¦å®‰è£…psutilè¿›è¡Œæ€§èƒ½ç›‘æ§")


if __name__ == "__main__":
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = CloudIOOptimizer()
    
    # è¿è¡Œä¼˜åŒ–
    optimizer.optimize_cloud_environment()
    
    # æ€§èƒ½ç›‘æ§
    monitor_cloud_performance()