import os
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, MaxAbsScaler
import joblib
from datetime import datetime
import glob
import matplotlib.pyplot as plt
from scipy.interpolate import interp1d
import matplotlib as mpl
import tensorflow as tf
import json
from tqdm import tqdm
import time

# è®¾ç½®ä¸­æ–‡å­—ä½“
def set_chinese_font():
    """è®¾ç½®ä¸­æ–‡å­—ä½“"""
    # å°è¯•è®¾ç½®ä¸åŒçš„ä¸­æ–‡å­—ä½“
    chinese_fonts = [
        'SimHei',  # é»‘ä½“
        'Microsoft YaHei',  # å¾®è½¯é›…é»‘
        'PingFang SC',  # è‹¹æ–¹
        'STHeiti',  # åæ–‡é»‘ä½“
        'Arial Unicode MS'  # Arial Unicode
    ]
    
    # æ£€æŸ¥ç³»ç»Ÿå­—ä½“
    system_fonts = set([f.name for f in mpl.font_manager.fontManager.ttflist])
    
    # é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„ä¸­æ–‡å­—ä½“
    for font in chinese_fonts:
        if font in system_fonts:
            plt.rcParams['font.family'] = font
            plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
            print(f"ä½¿ç”¨ä¸­æ–‡å­—ä½“: {font}")
            return True
    
    print("è­¦å‘Š: æœªæ‰¾åˆ°åˆé€‚çš„ä¸­æ–‡å­—ä½“ï¼Œå›¾è¡¨ä¸­æ–‡å¯èƒ½æ— æ³•æ­£ç¡®æ˜¾ç¤º")
    return False

class EMSCDatasetGenerator:
    """
    EMSCæ•°æ®é›†ç”Ÿæˆå™¨ç±»ï¼Œç”¨äºå¤„ç†å’Œç”Ÿæˆè®­ç»ƒæ•°æ®
    æ”¯æŒNPZå’ŒTFRecordä¸¤ç§æ ¼å¼
    """
    def __init__(self, target_sequence_length=1000, window_size=None, stride=None, max_subsequences=200,
                 normalize=True, scaler_type='minmax', output_format='npz'):
        """
        åˆå§‹åŒ–æ•°æ®é›†ç”Ÿæˆå™¨
        
        å‚æ•°:
        target_sequence_length: ç›®æ ‡åºåˆ—é•¿åº¦
        window_size: æ»‘åŠ¨çª—å£å¤§å°ï¼Œé»˜è®¤ä¸ºtarget_sequence_length
        stride: æ»‘åŠ¨çª—å£æ­¥é•¿ï¼Œé»˜è®¤ä¸ºtarget_sequence_length//10
        max_subsequences: æ¯ä¸ªåºåˆ—æœ€å¤šç”Ÿæˆçš„å­åºåˆ—æ•°
        normalize: æ˜¯å¦å¯¹æ•°æ®è¿›è¡Œå½’ä¸€åŒ–ï¼Œé»˜è®¤True
        scaler_type: å½’ä¸€åŒ–æ–¹æ³•ç±»å‹ï¼Œå¯é€‰å€¼ï¼š
                    'minmax' - MinMaxScaler (é»˜è®¤)
                    'standard' - StandardScaler (Z-scoreæ ‡å‡†åŒ–)
                    'robust' - RobustScaler (åŸºäºä¸­ä½æ•°å’Œå››åˆ†ä½æ•°çš„æ ‡å‡†åŒ–)
                    'maxabs' - MaxAbsScaler (åŸºäºæœ€å¤§ç»å¯¹å€¼çš„æ ‡å‡†åŒ–)
        output_format: è¾“å‡ºæ ¼å¼ï¼Œå¯é€‰å€¼ï¼š'npz', 'tfrecord', 'both'
        """
        self.target_sequence_length = target_sequence_length
        self.window_size = window_size if window_size is not None else target_sequence_length
        self.stride = stride if stride is not None else target_sequence_length // 10
        self.max_subsequences = max_subsequences
        
        # å½’ä¸€åŒ–é€‰é¡¹
        self.normalize = normalize
        self.scaler_type = scaler_type.lower()
        
        # è¾“å‡ºæ ¼å¼é€‰é¡¹
        self.output_format = output_format.lower()
        valid_formats = ['npz', 'tfrecord', 'both']
        if self.output_format not in valid_formats:
            raise ValueError(f"output_formatå¿…é¡»æ˜¯ä»¥ä¸‹ä¹‹ä¸€: {valid_formats}")
        
        # éªŒè¯scaler_type
        valid_scalers = ['minmax', 'standard', 'robust', 'maxabs']
        if self.scaler_type not in valid_scalers:
            raise ValueError(f"scaler_typeå¿…é¡»æ˜¯ä»¥ä¸‹ä¹‹ä¸€: {valid_scalers}")
        
        # åˆå§‹åŒ–æ ‡å‡†åŒ–å™¨
        if self.normalize:
            self.x_scaler = self._create_scaler()
            self.y_scaler = self._create_scaler()
        else:
            self.x_scaler = None
            self.y_scaler = None
        
        # åˆå§‹åŒ–æ•°æ®å­˜å‚¨
        self.X_paths = []
        self.Y_paths = []
        
        # æ•°æ®é›†åŸºç¡€è·¯å¾„
        self.base_dir = "/Users/tianyunhu/Documents/temp/code/Test_app/EMSC_Model/msc_models"
        self.dataset_name = None  # å°†åœ¨prepare_and_save_datasetä¸­è®¾ç½®
        
        # æ•°æ®å­˜å‚¨
        self.sequence_lengths = []
        self.temperature_stats = {}
        self.strain_rate_stats = {}  # æ–°å¢ï¼šåº”å˜ç‡ç»Ÿè®¡
        
        # å®šä¹‰åˆ—åæ˜ å°„
        self.column_mapping = {
            'time': 'time',
            'true_strain': 'true_strain',
            'true_stress': 'true_stress',
            'temperature': 'temperature',
            'delta_strain': 'delta_strain',  # åŸ Î”Îµ
            'delta_time': 'delta_time',      # åŸ Î”t
            'delta_temperature': 'delta_temperature',  # åŸ Î”T

            'init_strain': 'init_strain',    # åˆå§‹åº”å˜
            'init_time': 'init_time',        # åˆå§‹æ—¶é—´
            'init_temp': 'init_temp'         # åˆå§‹æ¸©åº¦
        }
        
    def _create_scaler(self):
        """
        æ ¹æ®scaler_typeåˆ›å»ºç›¸åº”çš„æ ‡å‡†åŒ–å™¨
        
        è¿”å›:
        scaler: æ ‡å‡†åŒ–å™¨å®ä¾‹
        """
        if self.scaler_type == 'minmax':
            return MinMaxScaler()
        elif self.scaler_type == 'standard':
            return StandardScaler()
        elif self.scaler_type == 'robust':
            return RobustScaler()
        elif self.scaler_type == 'maxabs':
            return MaxAbsScaler()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„scaler_type: {self.scaler_type}")
    
    def get_scaler_info(self):
        """
        è·å–å½“å‰æ ‡å‡†åŒ–å™¨ä¿¡æ¯
        
        è¿”å›:
        dict: åŒ…å«æ ‡å‡†åŒ–å™¨ä¿¡æ¯çš„å­—å…¸
        """
        return {
            'normalize': self.normalize,
            'scaler_type': self.scaler_type if self.normalize else None,
            'scaler_name': self.x_scaler.__class__.__name__ if self.normalize else None
        }

    def get_dataset_paths(self, dataset_name):
        """
        è·å–æ•°æ®é›†ç›¸å…³çš„æ‰€æœ‰è·¯å¾„
        
        å‚æ•°:
        dataset_name: æ•°æ®é›†åç§°
        
        è¿”å›:
        dict: åŒ…å«æ‰€æœ‰ç›¸å…³è·¯å¾„çš„å­—å…¸
        """
        # ä½¿ç”¨æ•°æ®é›†åç§°ä½œä¸ºé¡¶å±‚ç›®å½•
        dataset_dir = os.path.join(self.base_dir, dataset_name)
        
        return {
            'dataset_dir': dataset_dir,                    # æ•°æ®é›†ç›®å½•
            'dataset_file': os.path.join(dataset_dir, f'{dataset_name}.npz'),  # NPZæ•°æ®é›†æ–‡ä»¶
            'tfrecord_file': os.path.join(dataset_dir, f'{dataset_name}.tfrecord'),  # TFRecordæ•°æ®é›†æ–‡ä»¶
            'tfrecord_info': os.path.join(dataset_dir, f'{dataset_name}.tfrecord.info.json'),  # TFRecordä¿¡æ¯æ–‡ä»¶
            'scaler_dir': os.path.join(dataset_dir, 'scalers'),  # æ ‡å‡†åŒ–å™¨ç›®å½•
            'x_scaler_file': os.path.join(dataset_dir, 'scalers', 'x_scaler.save'),  # Xæ ‡å‡†åŒ–å™¨æ–‡ä»¶
            'y_scaler_file': os.path.join(dataset_dir, 'scalers', 'y_scaler.save'),  # Yæ ‡å‡†åŒ–å™¨æ–‡ä»¶
            'stats_plot': os.path.join(dataset_dir, f'{dataset_name}_statistics.png')  # ç»Ÿè®¡å›¾è¡¨
        }

    def extract_strain_rate_from_filename(self, filename):
        """ä»æ–‡ä»¶åä¸­æå–åº”å˜ç‡ä¿¡æ¯"""
        try:
            strain_rate = os.path.splitext(filename)[0].split('_')[2]
            return float(strain_rate)
        except (IndexError, ValueError) as e:
            print(f"è­¦å‘Š: æ— æ³•ä»æ–‡ä»¶å {filename} æå–åº”å˜ç‡ä¿¡æ¯: {e}")
            return None
                
    def load_and_preprocess_data(self, file_list):
        """
        åŠ è½½å’Œé¢„å¤„ç†æ•°æ®æ–‡ä»¶
        
        å‚æ•°:
        file_list: æ•°æ®æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        
        è¿”å›:
        X_paths: è¾“å…¥åºåˆ—åˆ—è¡¨
        Y_paths: ç›®æ ‡åºåˆ—åˆ—è¡¨
        """
        self.X_paths = []
        self.Y_paths = []
        self.sequence_lengths = []
        self.temperature_stats = {}
        self.strain_rate_stats = {}  # é‡ç½®åº”å˜ç‡ç»Ÿè®¡
        
        for file_idx, file in enumerate(file_list):
            print(f"æ–‡ä»¶ {file_idx+1}/{len(file_list)}: {file} å¼€å§‹å¤„ç†")
            try:
                df = pd.read_excel(file)
                df = df.rename(columns=lambda x: x.strip())
                
                # éªŒè¯å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
                required_columns = {'time', 'true_strain', 'true_stress', 'temperature'}
                if not required_columns.issubset({col.lower() for col in df.columns}):
                    print(f"æ–‡ä»¶ {file} ç¼ºå°‘å¿…è¦åˆ—ï¼Œå·²è·³è¿‡")
                    continue

                # æå–æ•°æ®
                time = df[self.column_mapping['time']]
                true_strain = df[self.column_mapping['true_strain']]
                true_stress = df[self.column_mapping['true_stress']]
                temperature = df[self.column_mapping['temperature']]

                # å¤„ç†å‹ç¼©æ•°æ®
                if 'com' in file:
                    print(f"æ–‡ä»¶ '{file}' æ£€æµ‹åˆ°å‹ç¼©æ•°æ®ï¼Œå·²å°†åº”åŠ›å’Œåº”å˜å–å")
                    true_strain = -true_strain
                    true_stress = -true_stress

                # æå–æ¸©åº¦å’Œåº”å˜ç‡ä¿¡æ¯
                strain_rate = self.extract_strain_rate_from_filename(file)
                
                if temperature is not None:
                    if temperature[0] not in self.temperature_stats:
                        self.temperature_stats[temperature[0]] = 0
                    self.temperature_stats[temperature[0]] += 1
                
                if strain_rate is not None:
                    if strain_rate not in self.strain_rate_stats:
                        self.strain_rate_stats[strain_rate] = 0
                    self.strain_rate_stats[strain_rate] += 1

                # è®¡ç®—å¢é‡
                df[self.column_mapping['delta_strain']] = true_strain.diff().fillna(0)
                df[self.column_mapping['delta_time']] = time.diff().fillna(1e-5)
                df[self.column_mapping['delta_temperature']] = temperature.diff().fillna(0)
                
                # è·å–åˆå§‹å€¼ï¼ˆåªå–ç¬¬ä¸€ä¸ªå€¼ï¼‰
                init_strain = true_strain.iloc[0]
                init_time = time.iloc[0]
                init_temp = temperature.iloc[0]
                
                # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡
                # å¢é‡ç‰¹å¾
                delta_features = df[[
                    self.column_mapping['delta_strain'],
                    self.column_mapping['delta_time'],
                    self.column_mapping['delta_temperature']
                ]].values
                
                # å°†åˆå§‹å€¼ä½œä¸ºé¢å¤–ç‰¹å¾æ·»åŠ åˆ°æ¯ä¸ªæ—¶é—´æ­¥
                init_features = np.array([init_strain, init_time, init_temp])
                x_full = np.column_stack([delta_features, np.tile(init_features, (len(delta_features), 1))])
                y_full = true_stress.values.reshape(-1, 1)
                
                full_len = len(x_full)
                self.sequence_lengths.append(full_len)
                
                # å¤„ç†é•¿åºåˆ—
                if full_len > self.window_size:
                    num_subsequences = 0
                    for i in range(0, full_len - self.window_size + 1, self.stride):
                        if num_subsequences >= self.max_subsequences:
                            break
                        
                        x_sub = x_full[i : i + self.window_size]
                        y_sub = y_full[i : i + self.window_size]
                        
                        self.X_paths.append(x_sub)
                        self.Y_paths.append(y_sub)
                        num_subsequences += 1
                    print(f"æ–‡ä»¶ {file} (é•¿åº¦ {full_len}) åˆ†å‰²ä¸º {num_subsequences} ä¸ªå­åºåˆ—")
                else:
                    # å¤„ç†çŸ­åºåˆ—
                    if full_len < self.target_sequence_length:
                        x_full, y_full = self.augment_short_sequence(x_full, y_full)
                    self.X_paths.append(x_full)
                    self.Y_paths.append(y_full)
                    
            except Exception as e:
                print(f"å¤„ç†æ–‡ä»¶ {file} æ—¶å‡ºé”™: {e}")
                continue
        
        self._print_dataset_statistics()
        return self.X_paths, self.Y_paths
    
    def augment_short_sequence(self, x, y, target_length=None):
        """
        å¯¹çŸ­åºåˆ—è¿›è¡Œæ•°æ®å¢å¼º
        
        å‚æ•°:
        x: è¾“å…¥åºåˆ—
        y: ç›®æ ‡åºåˆ—
        target_length: ç›®æ ‡é•¿åº¦ï¼Œé»˜è®¤ä½¿ç”¨self.target_sequence_length
        
        è¿”å›:
        x_aug: å¢å¼ºåçš„è¾“å…¥åºåˆ—
        y_aug: å¢å¼ºåçš„ç›®æ ‡åºåˆ—
        """
        target_length = target_length or self.target_sequence_length
        if len(x) >= target_length:
            return x, y
        
        # ä½¿ç”¨æ’å€¼æ–¹æ³•ç”Ÿæˆæ›´å¤šæ•°æ®ç‚¹
        t = np.linspace(0, 1, len(x))
        t_new = np.linspace(0, 1, target_length)
        
        x_aug = np.array([interp1d(t, x[:, i])(t_new) for i in range(x.shape[1])]).T
        y_aug = interp1d(t, y.flatten())(t_new).reshape(-1, 1)
        
        return x_aug, y_aug
    
    def is_normalized(self, data):
        """
        æ£€æŸ¥æ•°æ®æ˜¯å¦å·²ç»æ ‡å‡†åŒ–
        
        å‚æ•°:
        data: è¾“å…¥æ•°æ®ï¼ˆå¯ä»¥æ˜¯åˆ—è¡¨æˆ–numpyæ•°ç»„ï¼‰
        
        è¿”å›:
        bool: æ•°æ®æ˜¯å¦å·²æ ‡å‡†åŒ–
        """
        if not self.normalize:
            return True  # å¦‚æœä¸éœ€è¦å½’ä¸€åŒ–ï¼Œåˆ™è®¤ä¸ºå·²ç»"æ ‡å‡†åŒ–"
            
        if isinstance(data, list):
            data = np.concatenate(data)
        
        data_min = np.min(data, axis=0)
        data_max = np.max(data, axis=0)
        data_range = np.ptp(data, axis=0)
        mean = np.mean(data, axis=0)
        std = np.std(data, axis=0)
        
        if self.scaler_type == 'minmax':
            # æ£€æŸ¥æ•°æ®æ˜¯å¦åœ¨[0,1]èŒƒå›´å†…
            is_normalized = (np.all(data_min >= -1e-6) and 
                           np.all(data_max <= 1 + 1e-6) and 
                           np.all(data_range >= 0.1))
        elif self.scaler_type == 'standard':
            # æ£€æŸ¥æ˜¯å¦æ¥è¿‘0å‡å€¼1æ ‡å‡†å·®
            is_normalized = (np.all(np.abs(mean) < 0.1) and 
                           np.all(np.abs(std - 1) < 0.1))
        elif self.scaler_type == 'robust':
            # å¯¹äºRobustScalerï¼Œæ£€æŸ¥ä¸­ä½æ•°æ˜¯å¦æ¥è¿‘0
            median = np.median(data, axis=0)
            is_normalized = np.all(np.abs(median) < 0.1)
        elif self.scaler_type == 'maxabs':
            # æ£€æŸ¥æœ€å¤§ç»å¯¹å€¼æ˜¯å¦æ¥è¿‘1
            max_abs = np.max(np.abs(data), axis=0)
            is_normalized = np.all(max_abs <= 1 + 1e-6) and np.all(max_abs >= 0.1)
        else:
            # å¦‚æœæ˜¯å…¶ä»–ç±»å‹ï¼Œå°è¯•é€šç”¨æ£€æŸ¥
            is_minmax = (np.all(data_min >= -1e-6) and 
                        np.all(data_max <= 1 + 1e-6) and 
                        np.all(data_range >= 0.1))
            is_standard = (np.all(np.abs(mean) < 0.1) and 
                          np.all(np.abs(std - 1) < 0.1))
            is_normalized = is_minmax or is_standard
        
        return is_normalized
    
    def prepare_and_save_dataset(self, dataset_name, force_normalize=False):
        """
        å‡†å¤‡è®­ç»ƒåºåˆ—å¹¶ä¿å­˜æ•°æ®é›†ï¼ŒåŒ…æ‹¬æ ‡å‡†åŒ–å’Œæ©ç ç”Ÿæˆ
        
        å‚æ•°:
        dataset_name: æ•°æ®é›†åç§°
        force_normalize: æ˜¯å¦å¼ºåˆ¶é‡æ–°æ ‡å‡†åŒ–æ•°æ®ï¼ˆå³ä½¿æ•°æ®å·²ç»æ ‡å‡†åŒ–ï¼‰
        
        è¿”å›:
        tuple: (X_seq, Y_seq, masks) æ ‡å‡†åŒ–åçš„åºåˆ—å’Œæ©ç 
        """
        try:
            # è®¾ç½®æ•°æ®é›†åç§°å¹¶è·å–è·¯å¾„
            self.dataset_name = dataset_name
            paths = self.get_dataset_paths(dataset_name)
            
            # ç¡®ä¿æ•°æ®é›†ç›®å½•å­˜åœ¨
            os.makedirs(paths['dataset_dir'], exist_ok=True)
            os.makedirs(paths['scaler_dir'], exist_ok=True)
            
            # å¤„ç†å½’ä¸€åŒ–
            if not self.normalize:
                print("å½’ä¸€åŒ–å·²ç¦ç”¨ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®...")
                x_scaled = self.X_paths
                y_scaled = self.Y_paths
                print("Xæ•°æ®èŒƒå›´:", np.min(self.X_paths[0]), "åˆ°", np.max(self.X_paths[0]))
                print("Yæ•°æ®èŒƒå›´:", np.min(self.Y_paths[0]), "åˆ°", np.max(self.Y_paths[0]))
            else:
                # æ£€æŸ¥æ•°æ®æ˜¯å¦å·²ç»æ ‡å‡†åŒ–
                x_is_normalized = self.is_normalized(self.X_paths)
                y_is_normalized = self.is_normalized(self.Y_paths)
                
                if x_is_normalized and y_is_normalized and not force_normalize:
                    print(f"æ•°æ®å·²ç»ä½¿ç”¨{self.scaler_type}æ–¹æ³•æ ‡å‡†åŒ–ï¼Œç›´æ¥ä½¿ç”¨...")
                    print("Xæ•°æ®èŒƒå›´:", np.min(self.X_paths[0]), "åˆ°", np.max(self.X_paths[0]))
                    print("Yæ•°æ®èŒƒå›´:", np.min(self.Y_paths[0]), "åˆ°", np.max(self.Y_paths[0]))
                    x_scaled = self.X_paths
                    y_scaled = self.Y_paths
                else:
                    print(f"æ•°æ®æœªæ ‡å‡†åŒ–æˆ–éœ€è¦é‡æ–°æ ‡å‡†åŒ–ï¼Œå¼€å§‹ä½¿ç”¨{self.scaler_type}æ–¹æ³•æ ‡å‡†åŒ–...")
                    print("åŸå§‹æ•°æ®èŒƒå›´:")
                    print("Xæ•°æ®èŒƒå›´:", np.min(self.X_paths[0]), "åˆ°", np.max(self.X_paths[0]))
                    print("Yæ•°æ®èŒƒå›´:", np.min(self.Y_paths[0]), "åˆ°", np.max(self.Y_paths[0]))
                    
                    # é¦–å…ˆå°†æ‰€æœ‰æ•°æ®åˆå¹¶ä»¥è¿›è¡Œæ ‡å‡†åŒ–å™¨æ‹Ÿåˆ
                    all_x = np.vstack(self.X_paths)
                    all_y = np.vstack(self.Y_paths)
                    
                    # æ‹Ÿåˆæ ‡å‡†åŒ–å™¨
                    print(f"æ‹Ÿåˆ{self.x_scaler.__class__.__name__}æ ‡å‡†åŒ–å™¨...")
                    self.x_scaler.fit(all_x)
                    self.y_scaler.fit(all_y)
                    print("æ ‡å‡†åŒ–å™¨æ‹Ÿåˆå®Œæˆ")
                    
                    # æ ‡å‡†åŒ–æ•°æ®
                    print("æ ‡å‡†åŒ–æ•°æ®...")
                    x_scaled = [self.x_scaler.transform(x) for x in self.X_paths]
                    y_scaled = [self.y_scaler.transform(y) for y in self.Y_paths]
                    print("æ•°æ®æ ‡å‡†åŒ–å®Œæˆ")
                    print("æ ‡å‡†åŒ–åæ•°æ®èŒƒå›´:")
                    print("Xæ•°æ®èŒƒå›´:", np.min(x_scaled[0]), "åˆ°", np.max(x_scaled[0]))
                    print("Yæ•°æ®èŒƒå›´:", np.min(y_scaled[0]), "åˆ°", np.max(y_scaled[0]))
            
            # å‡†å¤‡åºåˆ—å’Œæ©ç 
            print("å‡†å¤‡åºåˆ—å’Œæ©ç ...")
            X_seq = []
            Y_seq = []
            masks = []
            
            for x, y in zip(x_scaled, y_scaled):
                seq_len = len(x)
                
                # ç”Ÿæˆæ©ç ï¼ˆ1è¡¨ç¤ºæœ‰æ•ˆæ•°æ®ï¼Œ0è¡¨ç¤ºå¡«å……ï¼‰
                mask = np.ones(min(seq_len, self.window_size), dtype=np.float32)
                
                # å¡«å……æˆ–æˆªæ–­åºåˆ—
                x_padded = np.pad(x[:self.window_size], 
                                ((0, max(0, self.window_size - seq_len)), (0, 0)), 
                                mode='constant', constant_values=0)
                y_padded = np.pad(y[:self.window_size], 
                                ((0, max(0, self.window_size - seq_len)), (0, 0)), 
                                mode='constant', constant_values=0)
                mask_padded = np.pad(mask, 
                                   (0, max(0, self.window_size - len(mask))), 
                                   mode='constant', constant_values=0)
                
                X_seq.append(x_padded)
                Y_seq.append(y_padded)
                masks.append(mask_padded)
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            X_seq = np.array(X_seq, dtype=np.float32)
            Y_seq = np.array(Y_seq, dtype=np.float32)
            masks = np.array(masks, dtype=np.float32)
            
            print("åºåˆ—å’Œæ©ç å‡†å¤‡å®Œæˆ")
            
            # ä¿å­˜æ•°æ®é›†
            print(f"ä¿å­˜æ•°æ®é›† (æ ¼å¼: {self.output_format})...")
            
            # ä¿å­˜NPZæ ¼å¼
            if self.output_format in ['npz', 'both']:
                print("ä¿å­˜NPZæ ¼å¼...")
                save_data = {
                    'X_paths': np.array(x_scaled, dtype=object),
                    'Y_paths': np.array(y_scaled, dtype=object),
                    'normalize': self.normalize,
                    'scaler_type': self.scaler_type if self.normalize else None,
                    'output_format': self.output_format,
                    'save_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }
                np.savez_compressed(paths['dataset_file'], **save_data)
                print(f"NPZæ•°æ®é›†å·²ä¿å­˜: {paths['dataset_file']}")
            
            # ä¿å­˜TFRecordæ ¼å¼
            if self.output_format in ['tfrecord', 'both']:
                print("ä¿å­˜TFRecordæ ¼å¼...")
                self._save_tfrecord_dataset(x_scaled, y_scaled, paths)
                print(f"TFRecordæ•°æ®é›†å·²ä¿å­˜: {paths['tfrecord_file']}")
            
            # ä¿å­˜æ ‡å‡†åŒ–å™¨ï¼ˆå¦‚æœå¯ç”¨äº†å½’ä¸€åŒ–ï¼‰
            if self.normalize and self.x_scaler is not None and self.y_scaler is not None:
                print("ä¿å­˜æ ‡å‡†åŒ–å™¨...")
                joblib.dump(self.x_scaler, paths['x_scaler_file'])
                joblib.dump(self.y_scaler, paths['y_scaler_file'])
                print(f"æ•°æ®é›†å’Œ{self.scaler_type}æ ‡å‡†åŒ–å™¨å·²ä¿å­˜è‡³: {paths['dataset_dir']}")
            else:
                print(f"æ•°æ®é›†å·²ä¿å­˜è‡³: {paths['dataset_dir']} (æœªä½¿ç”¨å½’ä¸€åŒ–)")
            
            return X_seq, Y_seq, masks
            
        except Exception as e:
            print(f"å‡†å¤‡å’Œä¿å­˜æ•°æ®é›†æ—¶å‡ºé”™: {e}")
            return None, None, None

    def load_dataset(self, dataset_name):
        """
        åŠ è½½æ•°æ®é›†å’Œæ ‡å‡†åŒ–å™¨
        
        å‚æ•°:
        dataset_name: æ•°æ®é›†åç§°
        
        è¿”å›:
        bool: æ˜¯å¦æˆåŠŸåŠ è½½
        """
        try:
            # è·å–è·¯å¾„
            paths = self.get_dataset_paths(dataset_name)
            
            # æ£€æŸ¥æ•°æ®é›†æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(paths['dataset_file']):
                print(f"æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {paths['dataset_file']}")
                return False
            
            # åŠ è½½æ•°æ®é›†
            print(f"åŠ è½½æ•°æ®é›†: {paths['dataset_file']}")
            data = np.load(paths['dataset_file'], allow_pickle=True)
            self.X_paths = data['X_paths'].tolist()
            self.Y_paths = data['Y_paths'].tolist()
            
            # åŠ è½½å½’ä¸€åŒ–è®¾ç½®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if 'normalize' in data:
                saved_normalize = bool(data['normalize'])
                saved_scaler_type = str(data['scaler_type']) if data['scaler_type'] is not None else None
                
                print(f"æ•°æ®é›†å½’ä¸€åŒ–è®¾ç½®: normalize={saved_normalize}, scaler_type={saved_scaler_type}")
                
                # æ£€æŸ¥å½“å‰è®¾ç½®æ˜¯å¦ä¸ä¿å­˜çš„è®¾ç½®ä¸€è‡´
                if self.normalize != saved_normalize:
                    print(f"è­¦å‘Š: å½“å‰å½’ä¸€åŒ–è®¾ç½®({self.normalize})ä¸æ•°æ®é›†è®¾ç½®({saved_normalize})ä¸ä¸€è‡´")
                if self.normalize and saved_scaler_type and self.scaler_type != saved_scaler_type:
                    print(f"è­¦å‘Š: å½“å‰å½’ä¸€åŒ–æ–¹æ³•({self.scaler_type})ä¸æ•°æ®é›†æ–¹æ³•({saved_scaler_type})ä¸ä¸€è‡´")
            else:
                print("æ•°æ®é›†æœªåŒ…å«å½’ä¸€åŒ–è®¾ç½®ä¿¡æ¯ï¼Œä½¿ç”¨å½“å‰è®¾ç½®")
            
            # åŠ è½½æ ‡å‡†åŒ–å™¨ï¼ˆå¦‚æœå¯ç”¨äº†å½’ä¸€åŒ–ä¸”æ–‡ä»¶å­˜åœ¨ï¼‰
            if self.normalize:
                if os.path.exists(paths['x_scaler_file']) and os.path.exists(paths['y_scaler_file']):
                    print("åŠ è½½æ ‡å‡†åŒ–å™¨...")
                    self.x_scaler = joblib.load(paths['x_scaler_file'])
                    self.y_scaler = joblib.load(paths['y_scaler_file'])
                    print(f"æ ‡å‡†åŒ–å™¨åŠ è½½å®Œæˆ: {self.x_scaler.__class__.__name__}")
                else:
                    print("è­¦å‘Š: å¯ç”¨äº†å½’ä¸€åŒ–ä½†æœªæ‰¾åˆ°æ ‡å‡†åŒ–å™¨æ–‡ä»¶")
                    # é‡æ–°åˆ›å»ºæ ‡å‡†åŒ–å™¨
                    self.x_scaler = self._create_scaler()
                    self.y_scaler = self._create_scaler()
            else:
                print("å½’ä¸€åŒ–å·²ç¦ç”¨ï¼Œè·³è¿‡æ ‡å‡†åŒ–å™¨åŠ è½½")
            
            self.dataset_name = dataset_name
            return True
            
        except Exception as e:
            print(f"åŠ è½½æ•°æ®é›†æ—¶å‡ºé”™: {e}")
            return False

    def _print_dataset_statistics(self):
        """æ‰“å°æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        if not self.sequence_lengths:
            self.sequence_lengths = [len(x) for x in self.X_paths]
        
        print("\næ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯:")
        print("="*50)
        print(f"åºåˆ—æ•°é‡: {len(self.X_paths)}")
        print(f"å½’ä¸€åŒ–è®¾ç½®: {'å¯ç”¨' if self.normalize else 'ç¦ç”¨'}")
        if self.normalize:
            print(f"å½’ä¸€åŒ–æ–¹æ³•: {self.scaler_type} ({self.x_scaler.__class__.__name__ if self.x_scaler else 'None'})")
        
        # æ‰“å°X_pathså’ŒY_pathsçš„ç»´åº¦ä¿¡æ¯
        if self.X_paths:
            print("\nX_pathså‰5ä¸ªåºåˆ—çš„å‰5ä¸ªå…ƒç´ :")
            for i, x_seq in enumerate(self.X_paths[:5]):
                print(f"åºåˆ— {i+1}:")
                print(x_seq[:5])
            
            print("\nY_pathså‰5ä¸ªåºåˆ—çš„å‰5ä¸ªå…ƒç´ :")
            for i, y_seq in enumerate(self.Y_paths[:5]):
                print(f"åºåˆ— {i+1}:")
                print(y_seq[:5])
            x_shapes = [x.shape for x in self.X_paths]
            y_shapes = [y.shape for y in self.Y_paths]
            
            print("\nX_pathsç»´åº¦ç»Ÿè®¡:")
            print(f"æ ·æœ¬æ•°é‡: {len(x_shapes)}")
            print(f"å½¢çŠ¶åˆ†å¸ƒ:")
            shape_counts = {}
            for shape in x_shapes:
                shape_str = str(shape)
                if shape_str not in shape_counts:
                    shape_counts[shape_str] = 0
                shape_counts[shape_str] += 1
            for shape_str, count in sorted(shape_counts.items()):
                print(f"  {shape_str}: {count}ä¸ªæ ·æœ¬")
            
            print("\nY_pathsç»´åº¦ç»Ÿè®¡:")
            print(f"æ ·æœ¬æ•°é‡: {len(y_shapes)}")
            print(f"å½¢çŠ¶åˆ†å¸ƒ:")
            shape_counts = {}
            for shape in y_shapes:
                shape_str = str(shape)
                if shape_str not in shape_counts:
                    shape_counts[shape_str] = 0
                shape_counts[shape_str] += 1
            for shape_str, count in sorted(shape_counts.items()):
                print(f"  {shape_str}: {count}ä¸ªæ ·æœ¬")
            
            # æ‰“å°ç‰¹å¾ç»´åº¦ä¿¡æ¯
            if x_shapes:
                print(f"\nç‰¹å¾ç»´åº¦: {x_shapes[0][1]}")
                print("ç‰¹å¾åˆ—è¡¨:")
                print("åŠ¨æ€ç‰¹å¾ï¼ˆæ¯ä¸ªæ—¶é—´æ­¥ï¼‰:")
                delta_features = [
                    self.column_mapping['delta_strain'],
                    self.column_mapping['delta_time'],
                    self.column_mapping['delta_temperature']
                ]
                for i, col in enumerate(delta_features):
                    print(f"  {i+1}. {col}")
                print("\né™æ€ç‰¹å¾ï¼ˆåºåˆ—åˆå§‹å€¼ï¼‰:")
                init_features = [
                    self.column_mapping['init_strain'],
                    self.column_mapping['init_time'],
                    self.column_mapping['init_temp']
                ]
                for i, col in enumerate(init_features):
                    print(f"  {i+4}. {col}")
        
        print("\nåºåˆ—é•¿åº¦ç»Ÿè®¡:")
        print(f"æœ€çŸ­: {min(self.sequence_lengths)}")
        print(f"æœ€é•¿: {max(self.sequence_lengths)}")
        print(f"å¹³å‡: {np.mean(self.sequence_lengths):.2f}")
        print(f"ä¸­ä½æ•°: {np.median(self.sequence_lengths)}")
        
        if self.temperature_stats:
            print("\næ¸©åº¦åˆ†å¸ƒ:")
            for temp, count in sorted(self.temperature_stats.items()):
                print(f"æ¸©åº¦ {temp}Â°C: {count} ä¸ªåºåˆ—")
        
        if self.strain_rate_stats:
            print("\nåº”å˜ç‡åˆ†å¸ƒ:")
            for rate, count in sorted(self.strain_rate_stats.items()):
                print(f"åº”å˜ç‡ {rate:.2e} sâ»Â¹: {count} ä¸ªåºåˆ—")
        
        print("="*50)
    
    def plot_dataset_statistics(self, dataset_name=None):
        """
        ç»˜åˆ¶å¹¶ä¿å­˜æ•°æ®é›†ç»Ÿè®¡å›¾è¡¨
        
        å‚æ•°:
        dataset_name: æ•°æ®é›†åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å½“å‰æ•°æ®é›†åç§°
        """
        try:
            if dataset_name is None:
                if self.dataset_name is None:
                    raise ValueError("æœªæŒ‡å®šæ•°æ®é›†åç§°")
                dataset_name = self.dataset_name
            
            # è·å–è·¯å¾„
            paths = self.get_dataset_paths(dataset_name)
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(paths['dataset_dir'], exist_ok=True)
            
            # è®¾ç½®ä¸­æ–‡å­—ä½“
            set_chinese_font()
            
            # åˆ›å»ºå›¾å½¢
            fig = plt.figure(figsize=(15, 10))
            
            # 1. åºåˆ—é•¿åº¦åˆ†å¸ƒ
            plt.subplot(2, 2, 1)
            plt.hist(self.sequence_lengths, bins=30, alpha=0.7)
            plt.title('åºåˆ—é•¿åº¦åˆ†å¸ƒ')
            plt.xlabel('åºåˆ—é•¿åº¦')
            plt.ylabel('é¢‘æ•°')
            plt.grid(True)
            
            # 2. æ¸©åº¦åˆ†å¸ƒ
            plt.subplot(2, 2, 2)
            temps = list(self.temperature_stats.keys())
            counts = list(self.temperature_stats.values())
            plt.bar(temps, counts, alpha=0.7)
            plt.title('æ¸©åº¦åˆ†å¸ƒ')
            plt.xlabel('æ¸©åº¦ (Â°C)')
            plt.ylabel('æ ·æœ¬æ•°')
            plt.grid(True)
            
            # 3. åº”å˜ç‡åˆ†å¸ƒ
            plt.subplot(2, 2, 3)
            strain_rates = list(self.strain_rate_stats.keys())
            counts = list(self.strain_rate_stats.values())
            plt.bar(strain_rates, counts, alpha=0.7)
            plt.title('åº”å˜ç‡åˆ†å¸ƒ')
            plt.xlabel('åº”å˜ç‡ (s^-1)')
            plt.ylabel('æ ·æœ¬æ•°')
            plt.xticks(rotation=45)
            plt.grid(True)
            
            # 4. åº”åŠ›èŒƒå›´åˆ†å¸ƒ - ç”±äºæ²¡æœ‰stress_statsï¼Œæš‚æ—¶ç§»é™¤è¿™ä¸ªå›¾
            plt.subplot(2, 2, 4)
            plt.text(0.5, 0.5, 'åº”åŠ›èŒƒå›´åˆ†å¸ƒ\n(æ•°æ®æœªæ”¶é›†)', 
                    horizontalalignment='center',
                    verticalalignment='center',
                    transform=plt.gca().transAxes)
            plt.title('åº”åŠ›èŒƒå›´åˆ†å¸ƒ')
            plt.axis('off')
            
            # è°ƒæ•´å¸ƒå±€
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            plt.savefig(paths['stats_plot'], dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"æ•°æ®é›†ç»Ÿè®¡å›¾è¡¨å·²ä¿å­˜è‡³: {paths['stats_plot']}")
            
        except Exception as e:
            print(f"ç»˜åˆ¶ç»Ÿè®¡å›¾è¡¨æ—¶å‡ºé”™: {e}")

    @staticmethod
    def print_usage_examples():
        """
        æ‰“å°ä½¿ç”¨ç¤ºä¾‹å’Œè¯´æ˜æ–‡æ¡£
        """
        print("\n" + "="*70)
        print("         EMSCDatasetGenerator ä½¿ç”¨è¯´æ˜")
        print("="*70)
        
        print("\n1. å½’ä¸€åŒ–é€‰é¡¹è¯´æ˜:")
        print("   normalize=True/False  : æ˜¯å¦å¯ç”¨æ•°æ®å½’ä¸€åŒ–")
        print("   scaler_typeé€‰é¡¹:")
        print("   - 'minmax'   : MinMaxScaler - å°†æ•°æ®ç¼©æ”¾åˆ°[0,1]èŒƒå›´")
        print("   - 'standard' : StandardScaler - Z-scoreæ ‡å‡†åŒ–ï¼Œå‡å€¼0ï¼Œæ ‡å‡†å·®1")
        print("   - 'robust'   : RobustScaler - åŸºäºä¸­ä½æ•°å’Œå››åˆ†ä½æ•°çš„æ ‡å‡†åŒ–")
        print("   - 'maxabs'   : MaxAbsScaler - åŸºäºæœ€å¤§ç»å¯¹å€¼çš„æ ‡å‡†åŒ–")
        
        print("\n2. è¾“å‡ºæ ¼å¼é€‰é¡¹è¯´æ˜:")
        print("   output_formaté€‰é¡¹:")
        print("   - 'npz'      : ä¿å­˜ä¸ºNPZæ ¼å¼ï¼ˆé»˜è®¤ï¼Œå…¼å®¹æ€§å¥½ï¼‰")
        print("   - 'tfrecord' : ä¿å­˜ä¸ºTFRecordæ ¼å¼ï¼ˆTensorFlowä¼˜åŒ–ï¼ŒåŠ è½½æ›´å¿«ï¼‰")
        print("   - 'both'     : åŒæ—¶ä¿å­˜NPZå’ŒTFRecordæ ¼å¼")
        
        print("\n3. ä½¿ç”¨ç¤ºä¾‹:")
        print("\n   # ç¤ºä¾‹1: ç”ŸæˆNPZæ ¼å¼æ•°æ®é›†ï¼ˆé»˜è®¤ï¼‰")
        print("   generator = EMSCDatasetGenerator(")
        print("       target_sequence_length=1000,")
        print("       normalize=True,")
        print("       scaler_type='minmax',")
        print("       output_format='npz'")
        print("   )")
        
        print("\n   # ç¤ºä¾‹2: ç”ŸæˆTFRecordæ ¼å¼æ•°æ®é›†ï¼ˆæ¨èç”¨äºå¤§æ•°æ®é›†ï¼‰")
        print("   generator = EMSCDatasetGenerator(")
        print("       target_sequence_length=1000,")
        print("       normalize=True,")
        print("       scaler_type='minmax',")
        print("       output_format='tfrecord'")
        print("   )")
        
        print("\n   # ç¤ºä¾‹3: åŒæ—¶ç”Ÿæˆä¸¤ç§æ ¼å¼")
        print("   generator = EMSCDatasetGenerator(")
        print("       target_sequence_length=1000,")
        print("       normalize=True,")
        print("       scaler_type='minmax',")
        print("       output_format='both'")
        print("   )")
        
        print("\n   # ç¤ºä¾‹4: ç¦ç”¨å½’ä¸€åŒ–ï¼Œç”ŸæˆTFRecord")
        print("   generator = EMSCDatasetGenerator(")
        print("       target_sequence_length=1000,")
        print("       normalize=False,")
        print("       output_format='tfrecord'")
        print("   )")
        
        print("\n4. æ ¼å¼é€‰æ‹©å»ºè®®:")
        print("   - å°æ•°æ®é›†(<100MB): æ¨èä½¿ç”¨ 'npz'")
        print("   - å¤§æ•°æ®é›†(>100MB): æ¨èä½¿ç”¨ 'tfrecord'")
        print("   - éœ€è¦å…¼å®¹æ€§: æ¨èä½¿ç”¨ 'both'")
        print("   - TensorFlowè®­ç»ƒ: æ¨èä½¿ç”¨ 'tfrecord'")
        
        print("\n5. å½’ä¸€åŒ–æ–¹æ³•é€‰æ‹©å»ºè®®:")
        print("   - ç¥ç»ç½‘ç»œæ¨¡å‹: æ¨èä½¿ç”¨ 'minmax'")
        print("   - æ•°æ®åŒ…å«å¼‚å¸¸å€¼: æ¨èä½¿ç”¨ 'robust'")
        print("   - ä¼ ç»Ÿæœºå™¨å­¦ä¹ : æ¨èä½¿ç”¨ 'standard'")
        print("   - æ•°æ®å·²é¢„å¤„ç†: å¯é€‰æ‹© normalize=False")
        
        print("\n6. æ³¨æ„äº‹é¡¹:")
        print("   - æ•°æ®é›†ä¼šä¿å­˜å½’ä¸€åŒ–å’Œæ ¼å¼è®¾ç½®")
        print("   - æ ‡å‡†åŒ–å™¨ä¼šè‡ªåŠ¨ä¿å­˜å’ŒåŠ è½½")
        print("   - TFRecordæ ¼å¼åŠ è½½é€Ÿåº¦æ›´å¿«ï¼Œä½†æ–‡ä»¶è¾ƒå¤§")
        print("   - å¯ä½¿ç”¨ force_normalize=True å¼ºåˆ¶é‡æ–°å½’ä¸€åŒ–")
        print("   - ä½¿ç”¨ get_scaler_info() æŸ¥çœ‹å½“å‰è®¾ç½®")
        
        print("="*70)

    def recover_physical_quantities(self, X_paths):
        """
        ä»å¢é‡å’Œåˆå€¼æ¢å¤å…¨ç‰©ç†é‡åºåˆ—
        
        Args:
            X_paths: è¾“å…¥åºåˆ—åˆ—è¡¨ï¼Œæ¯ä¸ªåºåˆ—åŒ…å« [delta_strain, delta_time, delta_temperature, 
                    init_strain, init_time, init_temp]
        
        Returns:
            physical_sequences: æ¢å¤åçš„ç‰©ç†é‡åºåˆ—åˆ—è¡¨ï¼Œæ¯ä¸ªåºåˆ—åŒ…å« [strain, time, temperature]
        """
        physical_sequences = []
        
        for x_seq in X_paths:
            # æå–å¢é‡å’Œåˆå€¼
            delta_strain = x_seq[:, 0]
            delta_time = x_seq[:, 1]
            delta_temperature = x_seq[:, 2]
            init_strain = x_seq[0, 3]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªç‚¹çš„åˆå€¼
            init_time = x_seq[0, 4]
            init_temp = x_seq[0, 5]
            
            # è®¡ç®—ç´¯ç§¯å€¼
            strain = np.cumsum(delta_strain) + init_strain
            time = np.cumsum(delta_time) + init_time
            temperature = np.cumsum(delta_temperature) + init_temp
            
            # ç»„åˆæˆç‰©ç†é‡åºåˆ—
            physical_seq = np.column_stack([strain, time, temperature])
            physical_sequences.append(physical_seq)
        
        return physical_sequences

    def normalize_window_features(self, window_X, window_Y, x_scaler, y_scaler):
        """
        ä½¿ç”¨å…¨å±€å½’ä¸€åŒ–å‚æ•°å¯¹çª—å£ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–
        
        Args:
            window_X: çª—å£è¾“å…¥æ•°æ® (window_size, 6)
            window_Y: çª—å£è¾“å‡ºæ•°æ® (window_size, 1)
            x_scaler: Xæ•°æ®çš„æ ‡å‡†åŒ–å™¨
            y_scaler: Yæ•°æ®çš„æ ‡å‡†åŒ–å™¨
        
        Returns:
            normalized_X: å½’ä¸€åŒ–åçš„çª—å£è¾“å…¥æ•°æ®
            normalized_Y: å½’ä¸€åŒ–åçš„çª—å£è¾“å‡ºæ•°æ®
        """
        # åˆ†ç¦»åŠ¨æ€ç‰¹å¾å’Œé™æ€ç‰¹å¾
        dynamic_features = window_X[:, :3]  # delta_strain, delta_time, delta_temperature
        static_features = window_X[:, 3:]   # init_strain, init_time, init_temp
        
        # å¯¹åŠ¨æ€ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–
        normalized_dynamic = x_scaler.transform(dynamic_features)
        
        # å¯¹é™æ€ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–ï¼ˆä½¿ç”¨ç›¸åŒçš„scalerï¼Œä½†åªå–ç¬¬ä¸€ä¸ªç‚¹ï¼‰
        normalized_static = x_scaler.transform(static_features[0:1])
        normalized_static = np.tile(normalized_static, (len(window_X), 1))
        
        # ç»„åˆå½’ä¸€åŒ–åçš„ç‰¹å¾
        normalized_X = np.column_stack([normalized_dynamic, normalized_static])
        
        # å¯¹è¾“å‡ºè¿›è¡Œå½’ä¸€åŒ–
        normalized_Y = y_scaler.transform(window_Y)
        
        return normalized_X, normalized_Y

    def prepare_window_data(self, physical_sequences, window_size, stride=None, 
                          x_scaler=None, y_scaler=None):
        """
        ä»ç‰©ç†é‡åºåˆ—ç”Ÿæˆè®­ç»ƒçª—å£æ•°æ®
        
        Args:
            physical_sequences: ç‰©ç†é‡åºåˆ—åˆ—è¡¨
            window_size: çª—å£å¤§å°
            stride: æ»‘åŠ¨æ­¥é•¿ï¼Œé»˜è®¤ä¸ºwindow_size//10
            x_scaler: Xæ•°æ®çš„æ ‡å‡†åŒ–å™¨
            y_scaler: Yæ•°æ®çš„æ ‡å‡†åŒ–å™¨
        
        Returns:
            windows_X: çª—å£è¾“å…¥æ•°æ®åˆ—è¡¨
            windows_Y: çª—å£è¾“å‡ºæ•°æ®åˆ—è¡¨
        """
        if stride is None:
            stride = window_size // 10
            
        windows_X = []
        windows_Y = []
        
        for seq_idx, physical_seq in enumerate(physical_sequences):
            seq_len = len(physical_seq)
            
            # å¯¹æ¯ä¸ªå¯èƒ½çš„èµ·å§‹ä½ç½®
            for start_idx in range(0, seq_len - window_size + 1, stride):
                end_idx = start_idx + window_size
                
                # æå–çª—å£çš„ç‰©ç†é‡æ•°æ®
                window_physical = physical_seq[start_idx:end_idx]
                
                # è®¡ç®—çª—å£å†…çš„å¢é‡
                delta_strain = np.zeros(window_size)
                delta_time = np.zeros(window_size)
                delta_temperature = np.zeros(window_size)
                
                for i in range(1, window_size):
                    delta_strain[i] = window_physical[i, 0] - window_physical[i-1, 0]
                    delta_time[i] = window_physical[i, 1] - window_physical[i-1, 1]
                    delta_temperature[i] = window_physical[i, 2] - window_physical[i-1, 2]
                
                # è·å–çª—å£èµ·å§‹ç‚¹çš„å€¼ä½œä¸ºåˆå§‹å€¼
                init_strain = window_physical[0, 0]
                init_time = window_physical[0, 1]
                init_temp = window_physical[0, 2]
                
                # æ„é€ çª—å£ç‰¹å¾
                window_X = np.column_stack([
                    delta_strain,
                    delta_time,
                    delta_temperature,
                    np.full(window_size, init_strain),
                    np.full(window_size, init_time),
                    np.full(window_size, init_temp)
                ])
                
                # å¦‚æœæœ‰æ ‡å‡†åŒ–å™¨ï¼Œè¿›è¡Œå½’ä¸€åŒ–
                if x_scaler is not None and y_scaler is not None:
                    window_X, window_Y = self.normalize_window_features(
                        window_X, window_Y, x_scaler, y_scaler)
                
                windows_X.append(window_X)
                windows_Y.append(window_Y)
        
        return windows_X, windows_Y

    def get_scalers(self, dataset_name):
        """
        åŠ è½½æ•°æ®é›†å¯¹åº”çš„æ ‡å‡†åŒ–å™¨
        
        Args:
            dataset_name: æ•°æ®é›†åç§°
        
        Returns:
            x_scaler: Xæ•°æ®çš„æ ‡å‡†åŒ–å™¨
            y_scaler: Yæ•°æ®çš„æ ‡å‡†åŒ–å™¨
        """
        paths = self.get_dataset_paths(dataset_name)
        
        if os.path.exists(paths['x_scaler_file']) and os.path.exists(paths['y_scaler_file']):
            x_scaler = joblib.load(paths['x_scaler_file'])
            y_scaler = joblib.load(paths['y_scaler_file'])
            return x_scaler, y_scaler
        else:
            print("è­¦å‘Š: æœªæ‰¾åˆ°æ ‡å‡†åŒ–å™¨æ–‡ä»¶")
            return None, None

    def _bytes_feature(self, value):
        """å°†numpyæ•°ç»„è½¬æ¢ä¸ºbytesç‰¹å¾"""
        if isinstance(value, type(tf.constant(0))):
            value = value.numpy()
        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value.tobytes()]))

    def _float_feature(self, value):
        """å°†floatå€¼è½¬æ¢ä¸ºfloatç‰¹å¾"""
        return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))

    def _int64_feature(self, value):
        """å°†intå€¼è½¬æ¢ä¸ºint64ç‰¹å¾"""
        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))

    def _save_tfrecord_dataset(self, x_scaled, y_scaled, paths):
        """
        ä¿å­˜TFRecordæ ¼å¼çš„æ•°æ®é›†
        
        å‚æ•°:
        x_scaled: æ ‡å‡†åŒ–åçš„Xæ•°æ®åˆ—è¡¨
        y_scaled: æ ‡å‡†åŒ–åçš„Yæ•°æ®åˆ—è¡¨
        paths: è·¯å¾„å­—å…¸
        """
        start_time = time.time()
        total_samples = len(x_scaled)
        
        print(f"å¼€å§‹è½¬æ¢ {total_samples} ä¸ªæ ·æœ¬åˆ°TFRecordæ ¼å¼...")
        
        # åˆ›å»ºTFRecordå†™å…¥å™¨
        with tf.io.TFRecordWriter(paths['tfrecord_file']) as writer:
            for i, (x_sample, y_sample) in enumerate(tqdm(zip(x_scaled, y_scaled), 
                                                          total=total_samples, 
                                                          desc="è½¬æ¢TFRecord")):
                # ç¡®ä¿æ•°æ®æ˜¯numpyæ•°ç»„å¹¶è½¬æ¢ä¸ºfloat32
                x_array = np.array(x_sample, dtype=np.float32)
                y_array = np.array(y_sample, dtype=np.float32)
                
                # åˆ›å»ºç‰¹å¾
                feature = {
                    'X_paths': self._bytes_feature(x_array),
                    'Y_paths': self._bytes_feature(y_array),
                    'sample_id': self._int64_feature(i)
                }
                
                # åˆ›å»ºExampleå¹¶å†™å…¥
                example = tf.train.Example(features=tf.train.Features(feature=feature))
                writer.write(example.SerializeToString())
        
        # åˆ›å»ºTFRecordä¿¡æ¯æ–‡ä»¶ - ä¸ç°æœ‰æ ¼å¼å…¼å®¹
        if x_scaled and y_scaled:
            sample_x = np.array(x_scaled[0], dtype=np.float32)
            sample_y = np.array(y_scaled[0], dtype=np.float32)
            
            # è®¡ç®—å±•å¹³åçš„å¤§å°
            x_flattened_size = sample_x.size
            y_flattened_size = sample_y.size
            
            dataset_info = {
                'keys': ['X_paths', 'Y_paths'],
                'total_samples': total_samples,
                'shapes': {
                    'X_paths': [x_flattened_size],  # å±•å¹³åçš„å¤§å°
                    'Y_paths': [y_flattened_size]   # å±•å¹³åçš„å¤§å°
                },
                'dtypes': {
                    'X_paths': 'float32',
                    'Y_paths': 'float32'
                },
                'created_at': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'source_file': paths['dataset_file'],
                'note': 'TFRecordæ ¼å¼æ•°æ®é›† - ç”±EMSCDatasetGeneratorç”Ÿæˆ',
                'normalize': self.normalize,
                'scaler_type': self.scaler_type if self.normalize else None,
                'output_format': self.output_format,
                'conversion_time': time.time() - start_time,
                'conversion_method': 'dataset_generator_tfrecord',
                'original_shapes': {
                    'X_paths': {
                        'outer_shape': [total_samples] + list(sample_x.shape),
                        'inner_shape': list(sample_x.shape),
                        'note': f'åµŒå¥—æ•°ç»„: å¤–å±‚({total_samples}, {sample_x.shape[0]}, {sample_x.shape[1]}), å†…å±‚{sample_x.shape}'
                    },
                    'Y_paths': {
                        'outer_shape': [total_samples] + list(sample_y.shape),
                        'inner_shape': list(sample_y.shape),
                        'note': f'åµŒå¥—æ•°ç»„: å¤–å±‚({total_samples}, {sample_y.shape[0]}, {sample_y.shape[1]}), å†…å±‚{sample_y.shape}'
                    }
                }
            }
        else:
            # å¤‡ç”¨ä¿¡æ¯
            dataset_info = {
                'keys': ['X_paths', 'Y_paths'],
                'total_samples': total_samples,
                'shapes': {'X_paths': 'unknown', 'Y_paths': 'unknown'},
                'dtypes': {'X_paths': 'float32', 'Y_paths': 'float32'},
                'created_at': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'note': 'TFRecordæ ¼å¼æ•°æ®é›† - ç”±EMSCDatasetGeneratorç”Ÿæˆ',
                'conversion_time': time.time() - start_time,
                'conversion_method': 'dataset_generator_tfrecord'
            }
        
        # ä¿å­˜ä¿¡æ¯æ–‡ä»¶
        with open(paths['tfrecord_info'], 'w') as f:
            json.dump(dataset_info, f, indent=2)
        
        elapsed_time = time.time() - start_time
        conversion_rate = total_samples / elapsed_time if elapsed_time > 0 else 0
        
        print(f"TFRecordè½¬æ¢å®Œæˆ:")
        print(f"  â±ï¸ è€—æ—¶: {elapsed_time:.2f}ç§’")
        print(f"  ğŸ“ˆ è½¬æ¢é€Ÿåº¦: {conversion_rate:.1f} æ ·æœ¬/ç§’")
        print(f"  ğŸ“‹ ä¿¡æ¯æ–‡ä»¶: {paths['tfrecord_info']}")

def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºæµ‹è¯•æ•°æ®é›†ç”Ÿæˆå™¨"""
    # æ‰“å°ä½¿ç”¨è¯´æ˜
    EMSCDatasetGenerator.print_usage_examples()
    
    # ç¤ºä¾‹ç”¨æ³•
    data_dir = "/Users/tianyunhu/Documents/temp/CTC/PPCC"  # æ›¿æ¢ä¸ºå®é™…çš„æ•°æ®ç›®å½•
    save_dir = "/Users/tianyunhu/Documents/temp/code/Test_app/EMSC_Model/msc_models"  # æ›¿æ¢ä¸ºå®é™…çš„ä¿å­˜ç›®å½•
    dataset_name = 'dataset_EMSC_big'

    # è·å–æ•°æ®æ–‡ä»¶åˆ—è¡¨
    file_list = glob.glob(os.path.join(data_dir, "*.xlsx"))
    
    if not file_list:
        print(f"åœ¨ {data_dir} ä¸­æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶")
        return
    
    # åˆ›å»ºæ•°æ®é›†ç”Ÿæˆå™¨ - ä¿®æ”¹ä¸ºåŒ¹é…è®­ç»ƒè„šæœ¬æœŸæœ›çš„æ ¼å¼
    # è·å–ç¬¬ä¸€ä¸ªExcelæ–‡ä»¶çš„é•¿åº¦ä½œä¸ºç›®æ ‡åºåˆ—é•¿åº¦
    first_file = file_list[0]
    df = pd.read_excel(first_file)
    target_sequence_length = len(df)
    window_size = target_sequence_length
    stride = 100  # å‡å°æ­¥é•¿ä»¥ç”Ÿæˆæ›´å¤šé‡å åºåˆ—
    max_subsequences = 50  # å‡å°‘æ¯ä¸ªæ–‡ä»¶çš„å­åºåˆ—æ•°é‡
    
    # ç¤ºä¾‹1: ç”ŸæˆTFRecordæ ¼å¼æ•°æ®é›†ï¼ˆæ¨èç”¨äºå¤§æ•°æ®é›†ï¼‰
    generator = EMSCDatasetGenerator(
        target_sequence_length=target_sequence_length,
        window_size=window_size,
        stride=stride,
        max_subsequences=max_subsequences,
        normalize=True,          # å¯ç”¨å½’ä¸€åŒ–
        scaler_type='minmax',     # ä½¿ç”¨MinMaxå½’ä¸€åŒ–
        output_format='both'      # åŒæ—¶ç”ŸæˆNPZå’ŒTFRecordä»¥ç¡®ä¿å…¼å®¹æ€§
    )
    
    # ç¤ºä¾‹2: ç”ŸæˆNPZæ ¼å¼æ•°æ®é›†ï¼ˆå…¼å®¹æ€§å¥½ï¼‰
    # generator = EMSCDatasetGenerator(
    #     target_sequence_length=target_sequence_length,
    #     window_size=window_size,
    #     stride=stride,
    #     max_subsequences=max_subsequences,
    #     normalize=True,
    #     scaler_type='minmax',
    #     output_format='npz'      # è¾“å‡ºæ ¼å¼ä¸ºNPZ
    # )
    
    # ç¤ºä¾‹3: åŒæ—¶ç”Ÿæˆä¸¤ç§æ ¼å¼
    # generator = EMSCDatasetGenerator(
    #     target_sequence_length=target_sequence_length,
    #     window_size=window_size,
    #     stride=stride,
    #     max_subsequences=max_subsequences,
    #     normalize=True,
    #     scaler_type='minmax',
    #     output_format='both'     # åŒæ—¶ç”ŸæˆNPZå’ŒTFRecord
    # )
    
    # ç¤ºä¾‹4: ç¦ç”¨å½’ä¸€åŒ–ï¼Œç”ŸæˆTFRecord
    # generator = EMSCDatasetGenerator(
    #     target_sequence_length=target_sequence_length,
    #     window_size=window_size,
    #     stride=stride,
    #     max_subsequences=max_subsequences,
    #     normalize=False,         # ç¦ç”¨å½’ä¸€åŒ–
    #     output_format='tfrecord' # è¾“å‡ºæ ¼å¼ä¸ºTFRecord
    # )
    
    # ç”Ÿæˆæ•°æ®é›†
    X_paths, Y_paths = generator.load_and_preprocess_data(file_list)
    
    # æ‰“å°å½“å‰å½’ä¸€åŒ–è®¾ç½®ä¿¡æ¯
    print("\nå½“å‰å½’ä¸€åŒ–è®¾ç½®:", generator.get_scaler_info())
    
    # å‡†å¤‡è®­ç»ƒåºåˆ—  
    dataset_path = os.path.join(save_dir, dataset_name + '.npz')
    X_seq, Y_seq, masks = generator.prepare_and_save_dataset(dataset_name)
    
    # ç»˜åˆ¶ç»Ÿè®¡å›¾è¡¨
    generator.plot_dataset_statistics(dataset_name)
    
    # æ¼”ç¤ºå¦‚ä½•æµ‹è¯•ä¸åŒçš„å½’ä¸€åŒ–æ–¹æ³•
    # print("\n" + "="*50)
    # print("æ¼”ç¤ºä¸åŒå½’ä¸€åŒ–æ–¹æ³•çš„æ•ˆæœ:")
    # print("="*50)
    
    # if X_paths and Y_paths:
    #     sample_x = X_paths[0][:100]  # å–ç¬¬ä¸€ä¸ªåºåˆ—çš„å‰100ä¸ªç‚¹ä½œä¸ºç¤ºä¾‹
    #     sample_y = Y_paths[0][:100]
        
    #     scaler_types = ['minmax', 'standard', 'robust', 'maxabs']
        
    #     for scaler_type in scaler_types:
    #         print(f"\n{scaler_type.upper()}æ ‡å‡†åŒ–æ•ˆæœ:")
    #         temp_generator = EMSCDatasetGenerator(normalize=True, scaler_type=scaler_type)
            
    #         # åˆ›å»ºä¸´æ—¶æ ‡å‡†åŒ–å™¨å¹¶æ‹Ÿåˆ
    #         temp_x_scaler = temp_generator._create_scaler()
    #         temp_y_scaler = temp_generator._create_scaler()
            
    #         temp_x_scaler.fit(sample_x)
    #         temp_y_scaler.fit(sample_y)
            
    #         # è½¬æ¢æ•°æ®
    #         scaled_x = temp_x_scaler.transform(sample_x)
    #         scaled_y = temp_y_scaler.transform(sample_y)
            
    #         print(f"  X - åŸå§‹èŒƒå›´: [{sample_x.min():.3f}, {sample_x.max():.3f}] -> æ ‡å‡†åŒ–å: [{scaled_x.min():.3f}, {scaled_x.max():.3f}]")
    #         print(f"  Y - åŸå§‹èŒƒå›´: [{sample_y.min():.3f}, {sample_y.max():.3f}] -> æ ‡å‡†åŒ–å: [{scaled_y.min():.3f}, {scaled_y.max():.3f}]")
            
    #         if scaler_type == 'standard':
    #             print(f"      Xå‡å€¼: {scaled_x.mean():.3f}, æ ‡å‡†å·®: {scaled_x.std():.3f}")
    #             print(f"      Yå‡å€¼: {scaled_y.mean():.3f}, æ ‡å‡†å·®: {scaled_y.std():.3f}")


if __name__ == "__main__":
    main()

