#!/usr/bin/env python3
"""
å°†npzæ•°æ®é›†è½¬æ¢ä¸ºTFRecordæ ¼å¼ï¼Œæä¾›é«˜æ•ˆçš„æ•°æ®åŠ è½½
"""

import tensorflow as tf
import numpy as np
import os
import json
from tqdm import tqdm
import multiprocessing as mp
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import threading
from functools import partial
import time
import shutil
import glob
try:
    import psutil
except ImportError:
    print("âš ï¸ è­¦å‘Š: psutilæœªå®‰è£…ï¼Œæ— æ³•ç›‘æ§å†…å­˜ä½¿ç”¨ç‡")
    psutil = None

def _bytes_feature(value):
    """å°†numpyæ•°ç»„è½¬æ¢ä¸ºbytesç‰¹å¾"""
    if isinstance(value, type(tf.constant(0))):
        value = value.numpy()
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value.tobytes()]))

def _float_feature(value):
    """å°†floatå€¼è½¬æ¢ä¸ºfloatç‰¹å¾"""
    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))

def check_dataset_exists(tfrecord_path):
    """
    æ£€æŸ¥TFRecordæ•°æ®é›†æ˜¯å¦å­˜åœ¨ä¸”å®Œæ•´
    
    Args:
        tfrecord_path: TFRecordæ–‡ä»¶è·¯å¾„
    
    Returns:
        bool: æ•°æ®é›†æ˜¯å¦å­˜åœ¨ä¸”å®Œæ•´
    """
    info_path = tfrecord_path + '.info.json'
    return os.path.exists(tfrecord_path) and os.path.exists(info_path)

def _process_data_chunk(data_chunk, keys, chunk_id):
    """
    å¤„ç†å•ä¸ªæ•°æ®å— - ç”¨äºå¤šè¿›ç¨‹
    
    Args:
        data_chunk: æ•°æ®å— {key: chunk_data}
        keys: å­—æ®µåˆ—è¡¨
        chunk_id: å—ID
    
    Returns:
        list: åºåˆ—åŒ–çš„Exampleåˆ—è¡¨
    """
    examples = []
    chunk_size = len(data_chunk[keys[0]])
    
    for i in range(chunk_size):
        feature = {}
        
        for key in keys:
            item_data = data_chunk[key][i]
            
            if isinstance(item_data, np.ndarray):
                if item_data.dtype == np.float32 or item_data.dtype == np.float64:
                    if item_data.size == 1:
                        feature[key] = _float_feature(float(item_data))
                    else:
                        feature[key] = _bytes_feature(item_data.astype(np.float32))
                elif item_data.dtype == object:
                    if isinstance(item_data, np.ndarray) and len(item_data) > 0:
                        combined_array = item_data.flatten().astype(np.float32)
                        feature[key] = _bytes_feature(combined_array)
                    else:
                        str_data = np.array([str(x) for x in item_data], dtype=np.string_)
                        feature[key] = _bytes_feature(str_data)
                else:
                    str_data = np.array([str(x) for x in item_data], dtype=np.string_)
                    feature[key] = _bytes_feature(str_data)
            else:
                str_data = np.array([str(item_data)], dtype=np.string_)
                feature[key] = _bytes_feature(str_data)
        
        example = tf.train.Example(features=tf.train.Features(feature=feature))
        examples.append(example.SerializeToString())
    
    return examples

def convert_npz_to_tfrecord(npz_path, tfrecord_path, batch_size=1000, force=False):
    """
    å°†npzæ•°æ®é›†è½¬æ¢ä¸ºTFRecordæ ¼å¼
    
    Args:
        npz_path: npzæ–‡ä»¶è·¯å¾„
        tfrecord_path: è¾“å‡ºTFRecordæ–‡ä»¶è·¯å¾„
        batch_size: æ¯æ‰¹å¤„ç†çš„æ•°æ®é‡
        force: æ˜¯å¦å¼ºåˆ¶é‡æ–°è½¬æ¢ï¼ˆå³ä½¿å·²å­˜åœ¨ï¼‰
    
    Returns:
        bool: è½¬æ¢æ˜¯å¦æˆåŠŸ
    """
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(npz_path):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {npz_path}")
        return False
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨è½¬æ¢åçš„æ–‡ä»¶
    if check_dataset_exists(tfrecord_path) and not force:
        print(f"âœ… TFRecordæ•°æ®é›†å·²å­˜åœ¨: {tfrecord_path}")
        print("ğŸ’¡ å¦‚éœ€é‡æ–°è½¬æ¢ï¼Œè¯·ä½¿ç”¨ --force å‚æ•°")
        return True
    
    try:
        print(f"ğŸ“¥ åŠ è½½npzæ•°æ®: {npz_path}")
        data = np.load(npz_path, mmap_mode='r', allow_pickle=True)  # å…è®¸åŠ è½½pickleæ•°æ®
        
        # è·å–æ‰€æœ‰é”®å
        keys = list(data.keys())
        print(f"ğŸ“‹ æ•°æ®é›†åŒ…å«ä»¥ä¸‹å­—æ®µ: {keys}")
        
        # åªä¿ç•™éœ€è¦è½¬æ¢çš„å­—æ®µ
        target_keys = ['X_paths', 'Y_paths']
        keys = [key for key in keys if key in target_keys]
        if not keys:
            print("âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°éœ€è¦è½¬æ¢çš„å­—æ®µ 'X_paths' æˆ– 'Y_paths'")
            return False
        print(f"ğŸ¯ å°†è½¬æ¢ä»¥ä¸‹å­—æ®µ: {keys}")
        
        # è·å–æ•°æ®ä¿¡æ¯
        total_samples = len(data[keys[0]])  # ä½¿ç”¨ç¬¬ä¸€ä¸ªé”®çš„é•¿åº¦ä½œä¸ºæ ·æœ¬æ•°
        print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {total_samples}")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(tfrecord_path), exist_ok=True)
        
        # åˆ›å»ºTFRecordå†™å…¥å™¨
        with tf.io.TFRecordWriter(tfrecord_path) as writer:
            # åˆ†æ‰¹å¤„ç†æ•°æ®
            for i in tqdm(range(0, total_samples, batch_size), desc="è½¬æ¢æ•°æ®"):
                # åˆ›å»ºç‰¹å¾å­—å…¸
                feature = {}
                
                # å¤„ç†æ¯ä¸ªå­—æ®µ
                for key in keys:
                    batch_data = data[key][i:i+batch_size]
                    if isinstance(batch_data, np.ndarray):
                        if batch_data.dtype == np.float32 or batch_data.dtype == np.float64:
                            if batch_data.size == 1:  # æ ‡é‡å€¼ï¼ˆå¦‚temperatureï¼‰
                                feature[key] = _float_feature(float(batch_data))
                            else:  # æ•°ç»„
                                feature[key] = _bytes_feature(batch_data)
                        elif batch_data.dtype == object:  # å¤„ç†å¯¹è±¡æ•°ç»„ï¼ˆåµŒå¥—numpyæ•°ç»„ï¼‰
                            # æ£€æŸ¥æ˜¯å¦æ˜¯åµŒå¥—çš„numpyæ•°ç»„
                            if len(batch_data) > 0 and isinstance(batch_data[0], np.ndarray):
                                # å°†åµŒå¥—çš„numpyæ•°ç»„å±•å¹³å¹¶åˆå¹¶
                                flattened_arrays = []
                                for item in batch_data:
                                    flattened_arrays.append(item.flatten())
                                # åˆå¹¶æ‰€æœ‰æ•°ç»„
                                combined_array = np.concatenate(flattened_arrays).astype(np.float32)
                                feature[key] = _bytes_feature(combined_array)
                            else:
                                # å°†å¯¹è±¡æ•°ç»„è½¬æ¢ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨
                                str_data = np.array([str(x) for x in batch_data], dtype=np.string_)
                                feature[key] = _bytes_feature(str_data)
                        else:
                            print(f"âš ï¸ è­¦å‘Š: å­—æ®µ {key} çš„æ•°æ®ç±»å‹ {batch_data.dtype} å°†è¢«è½¬æ¢ä¸ºå­—ç¬¦ä¸²")
                            str_data = np.array([str(x) for x in batch_data], dtype=np.string_)
                            feature[key] = _bytes_feature(str_data)
                    else:
                        # å¤„ç†éæ•°ç»„æ•°æ®
                        str_data = np.array([str(batch_data)], dtype=np.string_)
                        feature[key] = _bytes_feature(str_data)
                
                # åˆ›å»ºExample
                example = tf.train.Example(features=tf.train.Features(feature=feature))
                
                # å†™å…¥TFRecord
                writer.write(example.SerializeToString())
        
        # ä¿å­˜æ•°æ®é›†ä¿¡æ¯
        dataset_info = {
            'keys': keys,  # åªä¿å­˜è½¬æ¢çš„å­—æ®µ
            'total_samples': total_samples,
            'shapes': {},
            'dtypes': {},
            'created_at': str(np.datetime64('now')),
            'source_file': npz_path,
            'note': 'ä»…åŒ…å« X_paths å’Œ Y_paths å­—æ®µ'
        }
        
        # ä¸ºæ¯ä¸ªå­—æ®µè®¡ç®—æ­£ç¡®çš„å½¢çŠ¶å’Œç±»å‹ä¿¡æ¯
        for key in keys:
            original_data = data[key]
            if original_data.dtype == object and len(original_data) > 0 and isinstance(original_data[0], np.ndarray):
                # åµŒå¥—numpyæ•°ç»„ï¼šè®°å½•å±•å¹³åçš„æ€»å¤§å°
                sample_item = original_data[0]
                flattened_size = sample_item.size  # æ¯ä¸ªå­æ•°ç»„å±•å¹³åçš„å¤§å°
                dataset_info['shapes'][key] = (flattened_size * batch_size,)  # æ¯æ‰¹çš„å±•å¹³å¤§å°
                dataset_info['dtypes'][key] = 'float32'
                dataset_info['original_shapes'] = dataset_info.get('original_shapes', {})
                dataset_info['original_shapes'][key] = {
                    'outer_shape': original_data.shape,
                    'inner_shape': sample_item.shape,
                    'note': f'åµŒå¥—æ•°ç»„: å¤–å±‚{original_data.shape}, å†…å±‚{sample_item.shape}'
                }
            else:
                dataset_info['shapes'][key] = original_data.shape
                dataset_info['dtypes'][key] = str(original_data.dtype)
        
        info_path = tfrecord_path + '.info.json'
        with open(info_path, 'w') as f:
            json.dump(dataset_info, f, indent=2)
        
        print(f"âœ… è½¬æ¢å®Œæˆ: {tfrecord_path}")
        print(f"ğŸ“‹ æ•°æ®é›†ä¿¡æ¯å·²ä¿å­˜: {info_path}")
        
        # æ¸…ç†å†…å­˜
        del data
        import gc
        gc.collect()
        
        return True
        
    except Exception as e:
        print(f"âŒ è½¬æ¢å¤±è´¥: {e}")
        # æ¸…ç†å¯èƒ½å­˜åœ¨çš„éƒ¨åˆ†æ–‡ä»¶
        if os.path.exists(tfrecord_path):
            os.remove(tfrecord_path)
        if os.path.exists(tfrecord_path + '.info.json'):
            os.remove(tfrecord_path + '.info.json')
        return False

def convert_npz_to_tfrecord_fast(npz_path, tfrecord_path, batch_size=1000, force=False, 
                                num_workers=None, use_multiprocessing=True, memory_efficient=True):
    """
    ä¼˜åŒ–ç‰ˆæœ¬ï¼šä½¿ç”¨å¤šçº¿ç¨‹/å¤šè¿›ç¨‹åŠ é€Ÿè½¬æ¢
    
    Args:
        npz_path: npzæ–‡ä»¶è·¯å¾„
        tfrecord_path: è¾“å‡ºTFRecordæ–‡ä»¶è·¯å¾„
        batch_size: æ¯æ‰¹å¤„ç†çš„æ•°æ®é‡
        force: æ˜¯å¦å¼ºåˆ¶é‡æ–°è½¬æ¢
        num_workers: å·¥ä½œçº¿ç¨‹/è¿›ç¨‹æ•°ï¼ŒNoneä¸ºè‡ªåŠ¨æ£€æµ‹
        use_multiprocessing: æ˜¯å¦ä½¿ç”¨å¤šè¿›ç¨‹ï¼ˆå¦åˆ™ä½¿ç”¨å¤šçº¿ç¨‹ï¼‰
        memory_efficient: æ˜¯å¦å¯ç”¨å†…å­˜ä¼˜åŒ–æ¨¡å¼
    
    Returns:
        bool: è½¬æ¢æ˜¯å¦æˆåŠŸ
    """
    start_time = time.time()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(npz_path):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {npz_path}")
        return False
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    if check_dataset_exists(tfrecord_path) and not force:
        print(f"âœ… TFRecordæ•°æ®é›†å·²å­˜åœ¨: {tfrecord_path}")
        return True
    
    # è‡ªåŠ¨æ£€æµ‹å·¥ä½œè¿›ç¨‹æ•°
    if num_workers is None:
        num_workers = min(mp.cpu_count(), 8)  # é™åˆ¶æœ€å¤§è¿›ç¨‹æ•°é¿å…å†…å­˜çˆ†ç‚¸
    
    print(f"ğŸš€ å¿«é€Ÿè½¬æ¢æ¨¡å¼:")
    print(f"   - å·¥ä½œè¿›ç¨‹æ•°: {num_workers}")
    print(f"   - å¤šè¿›ç¨‹æ¨¡å¼: {'æ˜¯' if use_multiprocessing else 'å¦ï¼ˆå¤šçº¿ç¨‹ï¼‰'}")
    print(f"   - å†…å­˜ä¼˜åŒ–: {'æ˜¯' if memory_efficient else 'å¦'}")
    print(f"   - æ‰¹æ¬¡å¤§å°: {batch_size}")
    
    try:
        print(f"ğŸ“¥ åŠ è½½npzæ•°æ®: {npz_path}")
        
        if memory_efficient:
            # å†…å­˜ä¼˜åŒ–ï¼šä½¿ç”¨mmapæ¨¡å¼
            data = np.load(npz_path, mmap_mode='r', allow_pickle=True)
        else:
            data = np.load(npz_path, allow_pickle=True)
        
        # è·å–å­—æ®µä¿¡æ¯
        keys = list(data.keys())
        target_keys = ['X_paths', 'Y_paths']
        keys = [key for key in keys if key in target_keys]
        
        if not keys:
            print("âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°éœ€è¦è½¬æ¢çš„å­—æ®µ")
            return False
        
        print(f"ğŸ¯ è½¬æ¢å­—æ®µ: {keys}")
        
        total_samples = len(data[keys[0]])
        print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {total_samples}")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(tfrecord_path), exist_ok=True)
        
        # è®¡ç®—æ•°æ®å—å¤§å°
        if memory_efficient and total_samples > 10000:
            # å¤§æ•°æ®é›†ï¼šä½¿ç”¨å°å—å¤„ç†
            chunk_size = min(batch_size, 100)
        else:
            chunk_size = min(batch_size, total_samples // num_workers + 1)
        
        print(f"ğŸ”„ å¼€å§‹å¹¶è¡Œè½¬æ¢ (å—å¤§å°: {chunk_size})")
        
        # åˆ›å»ºTFRecordå†™å…¥å™¨
        with tf.io.TFRecordWriter(tfrecord_path) as writer:
            
            if use_multiprocessing and total_samples > 1000:
                # å¤šè¿›ç¨‹æ¨¡å¼ - é€‚åˆCPUå¯†é›†å‹ä»»åŠ¡
                with ProcessPoolExecutor(max_workers=num_workers) as executor:
                    futures = []
                    
                    # åˆ†å—æäº¤ä»»åŠ¡
                    for i in range(0, total_samples, chunk_size):
                        end_idx = min(i + chunk_size, total_samples)
                        
                        # å‡†å¤‡æ•°æ®å—
                        chunk_data = {}
                        for key in keys:
                            chunk_data[key] = data[key][i:end_idx]
                        
                        # æäº¤å¤„ç†ä»»åŠ¡
                        future = executor.submit(_process_data_chunk, chunk_data, keys, i // chunk_size)
                        futures.append(future)
                    
                    # æ”¶é›†ç»“æœå¹¶å†™å…¥
                    with tqdm(total=len(futures), desc="å¤„ç†æ•°æ®å—") as pbar:
                        for future in futures:
                            examples = future.result()
                            for example_bytes in examples:
                                writer.write(example_bytes)
                            pbar.update(1)
            
            else:
                # å¤šçº¿ç¨‹æ¨¡å¼ - é€‚åˆI/Oå¯†é›†å‹ä»»åŠ¡æˆ–å°æ•°æ®é›†
                with ThreadPoolExecutor(max_workers=num_workers) as executor:
                    
                    # åˆ›å»ºçº¿ç¨‹å®‰å…¨çš„å†™å…¥é”
                    write_lock = threading.Lock()
                    processed_count = [0]  # ä½¿ç”¨åˆ—è¡¨é¿å…é—­åŒ…é—®é¢˜
                    
                    def process_and_write(start_idx):
                        end_idx = min(start_idx + chunk_size, total_samples)
                        
                        # å¤„ç†æ•°æ®å—
                        chunk_data = {}
                        for key in keys:
                            chunk_data[key] = data[key][start_idx:end_idx]
                        
                        examples = _process_data_chunk(chunk_data, keys, start_idx // chunk_size)
                        
                        # çº¿ç¨‹å®‰å…¨å†™å…¥
                        with write_lock:
                            for example_bytes in examples:
                                writer.write(example_bytes)
                            processed_count[0] += len(examples)
                    
                    # æäº¤æ‰€æœ‰ä»»åŠ¡
                    futures = []
                    for i in range(0, total_samples, chunk_size):
                        future = executor.submit(process_and_write, i)
                        futures.append(future)
                    
                    # ç­‰å¾…å®Œæˆå¹¶æ˜¾ç¤ºè¿›åº¦
                    with tqdm(total=total_samples, desc="è½¬æ¢æ•°æ®") as pbar:
                        last_count = 0
                        for future in futures:
                            future.result()
                            current_count = processed_count[0]
                            pbar.update(current_count - last_count)
                            last_count = current_count
        
        # ä¿å­˜æ•°æ®é›†ä¿¡æ¯
        dataset_info = {
            'keys': keys,
            'total_samples': total_samples,
            'shapes': {},
            'dtypes': {},
            'created_at': str(np.datetime64('now')),
            'source_file': npz_path,
            'conversion_time': time.time() - start_time,
            'conversion_method': 'multiprocess' if use_multiprocessing else 'multithread',
            'num_workers': num_workers,
            'note': 'å¿«é€Ÿè½¬æ¢ç‰ˆæœ¬'
        }
        
        # è®¡ç®—å½¢çŠ¶ä¿¡æ¯
        for key in keys:
            original_data = data[key]
            if hasattr(original_data, 'shape'):
                if len(original_data.shape) > 1:
                    # å¤šç»´æ•°ç»„
                    sample_item = original_data[0]
                    if hasattr(sample_item, 'shape'):
                        flattened_size = sample_item.size
                        dataset_info['shapes'][key] = [flattened_size]
                        dataset_info['dtypes'][key] = 'float32'
                        dataset_info['original_shapes'] = dataset_info.get('original_shapes', {})
                        dataset_info['original_shapes'][key] = {
                            'outer_shape': list(original_data.shape),
                            'inner_shape': list(sample_item.shape),
                            'note': f'å¤šç»´æ•°ç»„: å¤–å±‚{original_data.shape}, å†…å±‚{sample_item.shape}'
                        }
                    else:
                        dataset_info['shapes'][key] = list(original_data.shape)
                        dataset_info['dtypes'][key] = str(original_data.dtype)
                else:
                    dataset_info['shapes'][key] = list(original_data.shape)
                    dataset_info['dtypes'][key] = str(original_data.dtype)
        
        # ä¿å­˜ä¿¡æ¯æ–‡ä»¶
        info_path = tfrecord_path + '.info.json'
        with open(info_path, 'w') as f:
            json.dump(dataset_info, f, indent=2)
        
        elapsed_time = time.time() - start_time
        print(f"âœ… å¿«é€Ÿè½¬æ¢å®Œæˆ: {tfrecord_path}")
        print(f"â±ï¸ è½¬æ¢è€—æ—¶: {elapsed_time:.2f}ç§’")
        print(f"ğŸ“ˆ è½¬æ¢é€Ÿåº¦: {total_samples/elapsed_time:.1f} æ ·æœ¬/ç§’")
        print(f"ğŸ“‹ æ•°æ®é›†ä¿¡æ¯: {info_path}")
        
        # æ¸…ç†å†…å­˜
        del data
        import gc
        gc.collect()
        
        return True
        
    except Exception as e:
        print(f"âŒ å¿«é€Ÿè½¬æ¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        
        # æ¸…ç†å¤±è´¥çš„æ–‡ä»¶
        for file_path in [tfrecord_path, tfrecord_path + '.info.json']:
            if os.path.exists(file_path):
                os.remove(file_path)
        return False

def convert_npz_to_tfrecord_streaming(npz_path, tfrecord_path, chunk_size=100, force=False, 
                                     num_workers=None, buffer_size=10, progress_callback=None):
    """
    æµå¼å¤„ç†ç‰ˆæœ¬ï¼šé€å—åŠ è½½å’Œè½¬æ¢ï¼Œé€‚åˆè¶…å¤§æ•°æ®é›†
    
    Args:
        npz_path: npzæ–‡ä»¶è·¯å¾„
        tfrecord_path: è¾“å‡ºTFRecordæ–‡ä»¶è·¯å¾„
        chunk_size: æ¯æ¬¡æµå¼å¤„ç†çš„å—å¤§å°
        force: æ˜¯å¦å¼ºåˆ¶é‡æ–°è½¬æ¢
        num_workers: å·¥ä½œçº¿ç¨‹æ•°
        buffer_size: å†…å­˜ç¼“å†²åŒºå¤§å°ï¼ˆå—æ•°ï¼‰
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
    
    Returns:
        bool: è½¬æ¢æ˜¯å¦æˆåŠŸ
    """
    start_time = time.time()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(npz_path):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {npz_path}")
        return False
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    if check_dataset_exists(tfrecord_path) and not force:
        print(f"âœ… TFRecordæ•°æ®é›†å·²å­˜åœ¨: {tfrecord_path}")
        return True
    
    if num_workers is None:
        num_workers = min(mp.cpu_count(), 4)  # æµå¼å¤„ç†ä½¿ç”¨è¾ƒå°‘çš„å·¥ä½œçº¿ç¨‹
    
    print(f"ğŸŒŠ æµå¼è½¬æ¢æ¨¡å¼:")
    print(f"   - æµå¼å—å¤§å°: {chunk_size}")
    print(f"   - å·¥ä½œçº¿ç¨‹æ•°: {num_workers}")
    print(f"   - å†…å­˜ç¼“å†²åŒº: {buffer_size} å—")
    
    try:
        # ç¬¬ä¸€æ¬¡æ‰“å¼€è·å–åŸºæœ¬ä¿¡æ¯
        print(f"ğŸ“Š åˆ†ææ•°æ®é›†ç»“æ„: {npz_path}")
        with np.load(npz_path, allow_pickle=True) as data:
            keys = list(data.keys())
            target_keys = ['X_paths', 'Y_paths']
            keys = [key for key in keys if key in target_keys]
            
            if not keys:
                print("âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°éœ€è¦è½¬æ¢çš„å­—æ®µ")
                return False
            
            total_samples = len(data[keys[0]])
            
            # è·å–æ•°æ®ç±»å‹å’Œå½¢çŠ¶ä¿¡æ¯
            sample_shapes = {}
            sample_dtypes = {}
            for key in keys:
                sample_data = data[key][0]
                if hasattr(sample_data, 'shape'):
                    sample_shapes[key] = sample_data.shape
                    sample_dtypes[key] = sample_data.dtype
                else:
                    sample_shapes[key] = ()
                    sample_dtypes[key] = type(sample_data)
        
        print(f"ğŸ¯ æµå¼è½¬æ¢å­—æ®µ: {keys}")
        print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"ğŸ’¾ é¢„è®¡å¤„ç†å—æ•°: {(total_samples + chunk_size - 1) // chunk_size}")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(tfrecord_path), exist_ok=True)
        
        # æµå¼å¤„ç†çŠ¶æ€
        processed_samples = 0
        memory_usage = []
        
        # åˆ›å»ºTFRecordå†™å…¥å™¨
        with tf.io.TFRecordWriter(tfrecord_path) as writer:
            
            # ä½¿ç”¨é˜Ÿåˆ—è¿›è¡Œç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¨¡å¼çš„æµå¼å¤„ç†
            from queue import Queue
            
            # æ•°æ®å—é˜Ÿåˆ—
            data_queue = Queue(maxsize=buffer_size)
            result_queue = Queue()
            
            def data_producer():
                """æ•°æ®ç”Ÿäº§è€…ï¼šæµå¼è¯»å–æ•°æ®å—"""
                try:
                    # ä½¿ç”¨memory mappingé¿å…åŠ è½½æ•´ä¸ªæ•°æ®é›†
                    data = np.load(npz_path, mmap_mode='r', allow_pickle=True)
                    
                    for i in range(0, total_samples, chunk_size):
                        end_idx = min(i + chunk_size, total_samples)
                        
                        # åªè¯»å–å½“å‰å—çš„æ•°æ®
                        chunk_data = {}
                        for key in keys:
                            chunk_data[key] = data[key][i:end_idx].copy()  # copyé¿å…mmapå¼•ç”¨
                        
                        data_queue.put((i // chunk_size, chunk_data))
                        
                        # ç›‘æ§å†…å­˜ä½¿ç”¨
                        if psutil and len(memory_usage) < 10:  # åªè®°å½•å‰10ä¸ªå—çš„å†…å­˜ä½¿ç”¨
                            memory_usage.append(psutil.virtual_memory().percent)
                    
                    # å‘é€ç»“æŸä¿¡å·
                    data_queue.put(None)
                    
                except Exception as e:
                    print(f"âŒ æ•°æ®ç”Ÿäº§è€…é”™è¯¯: {e}")
                    data_queue.put(None)
            
            def data_consumer():
                """æ•°æ®æ¶ˆè´¹è€…ï¼šå¤„ç†æ•°æ®å—å¹¶ç”ŸæˆTFRecordæ ·æœ¬"""
                try:
                    while True:
                        item = data_queue.get()
                        if item is None:  # ç»“æŸä¿¡å·
                            result_queue.put(None)
                            break
                        
                        chunk_id, chunk_data = item
                        
                        # å¤„ç†å½“å‰å—
                        examples = _process_data_chunk(chunk_data, keys, chunk_id)
                        result_queue.put((chunk_id, examples))
                        
                        # é‡Šæ”¾å†…å­˜
                        del chunk_data
                        
                        data_queue.task_done()
                        
                except Exception as e:
                    print(f"âŒ æ•°æ®æ¶ˆè´¹è€…é”™è¯¯: {e}")
                    result_queue.put(None)
            
            # å¯åŠ¨ç”Ÿäº§è€…å’Œæ¶ˆè´¹è€…çº¿ç¨‹
            from threading import Thread
            
            producer_thread = Thread(target=data_producer)
            consumer_thread = Thread(target=data_consumer)
            
            producer_thread.start()
            consumer_thread.start()
            
            # ä¸»çº¿ç¨‹ï¼šæ”¶é›†ç»“æœå¹¶å†™å…¥TFRecord
            last_report_time = time.time()
            chunk_times = []
            
            with tqdm(total=total_samples, desc="æµå¼è½¬æ¢", 
                     bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]') as pbar:
                while True:
                    result = result_queue.get()
                    if result is None:  # ç»“æŸä¿¡å·
                        break
                    
                    chunk_start_time = time.time()
                    chunk_id, examples = result
                    
                    # å†™å…¥TFRecord
                    for example_bytes in examples:
                        writer.write(example_bytes)
                    
                    processed_samples += len(examples)
                    pbar.update(len(examples))
                    
                    # æ€§èƒ½ç›‘æ§
                    chunk_time = time.time() - chunk_start_time
                    chunk_times.append(chunk_time)
                    
                    # æ¯10ä¸ªchunkæˆ–æ¯30ç§’æŠ¥å‘Šä¸€æ¬¡è¯¦ç»†è¿›åº¦
                    current_time = time.time()
                    if chunk_id % 10 == 0 or current_time - last_report_time > 30:
                        avg_chunk_time = np.mean(chunk_times[-10:]) if chunk_times else 0
                        samples_per_sec = len(examples) / avg_chunk_time if avg_chunk_time > 0 else 0
                        remaining_chunks = (total_samples - processed_samples) // chunk_size
                        eta_minutes = (remaining_chunks * avg_chunk_time) / 60 if avg_chunk_time > 0 else 0
                        
                        print(f"\nğŸ“Š è¿›åº¦æŠ¥å‘Š - å— {chunk_id+1}/{(total_samples-1)//chunk_size+1}:")
                        print(f"   âœ… å·²å¤„ç†: {processed_samples:,}/{total_samples:,} æ ·æœ¬ ({processed_samples/total_samples*100:.1f}%)")
                        print(f"   âš¡ å¤„ç†é€Ÿåº¦: {samples_per_sec:.1f} æ ·æœ¬/ç§’")
                        print(f"   â±ï¸ é¢„è®¡å‰©ä½™: {eta_minutes:.1f} åˆ†é’Ÿ")
                        if psutil:
                            memory_percent = psutil.virtual_memory().percent
                            print(f"   ğŸ§  å†…å­˜ä½¿ç”¨: {memory_percent:.1f}%")
                        last_report_time = current_time
                    
                    # è°ƒç”¨è¿›åº¦å›è°ƒ
                    if progress_callback:
                        progress_callback(processed_samples, total_samples, chunk_id)
                    
                    result_queue.task_done()
            
            # ç­‰å¾…çº¿ç¨‹ç»“æŸ
            producer_thread.join()
            consumer_thread.join()
        
        # ä¿å­˜æ•°æ®é›†ä¿¡æ¯
        dataset_info = {
            'keys': keys,
            'total_samples': total_samples,
            'shapes': {},
            'dtypes': {},
            'created_at': str(np.datetime64('now')),
            'source_file': npz_path,
            'conversion_time': time.time() - start_time,
            'conversion_method': 'streaming',
            'chunk_size': chunk_size,
            'num_workers': num_workers,
            'buffer_size': buffer_size,
            'memory_usage_samples': memory_usage,
            'note': 'æµå¼è½¬æ¢ç‰ˆæœ¬ - å†…å­˜å‹å¥½'
        }
        
        # è®¡ç®—å½¢çŠ¶ä¿¡æ¯
        for key in keys:
            if key in sample_shapes:
                if len(sample_shapes[key]) > 0:
                    flattened_size = np.prod(sample_shapes[key])
                    dataset_info['shapes'][key] = [flattened_size]
                    dataset_info['dtypes'][key] = 'float32'
                    dataset_info['original_shapes'] = dataset_info.get('original_shapes', {})
                    dataset_info['original_shapes'][key] = {
                        'outer_shape': [total_samples] + list(sample_shapes[key]),
                        'inner_shape': list(sample_shapes[key]),
                        'note': f'æµå¼å¤„ç†: å½¢çŠ¶{sample_shapes[key]}'
                    }
                else:
                    dataset_info['shapes'][key] = []
                    dataset_info['dtypes'][key] = str(sample_dtypes[key])
        
        # ä¿å­˜ä¿¡æ¯æ–‡ä»¶
        info_path = tfrecord_path + '.info.json'
        with open(info_path, 'w') as f:
            json.dump(dataset_info, f, indent=2)
        
        elapsed_time = time.time() - start_time
        avg_memory = np.mean(memory_usage) if memory_usage else 0
        
        print(f"âœ… æµå¼è½¬æ¢å®Œæˆ: {tfrecord_path}")
        print(f"â±ï¸ è½¬æ¢è€—æ—¶: {elapsed_time:.2f}ç§’")
        print(f"ğŸ“ˆ è½¬æ¢é€Ÿåº¦: {total_samples/elapsed_time:.1f} æ ·æœ¬/ç§’")
        print(f"ğŸ§  å¹³å‡å†…å­˜ä½¿ç”¨ç‡: {avg_memory:.1f}%")
        print(f"ğŸ“‹ æ•°æ®é›†ä¿¡æ¯: {info_path}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµå¼è½¬æ¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        
        # æ¸…ç†å¤±è´¥çš„æ–‡ä»¶
        for file_path in [tfrecord_path, tfrecord_path + '.info.json']:
            if os.path.exists(file_path):
                os.remove(file_path)
        return False

def convert_npz_to_tfrecord_adaptive(npz_path, tfrecord_path, force=False, 
                                    max_memory_gb=8, auto_optimize=True):
    """
    è‡ªé€‚åº”è½¬æ¢ï¼šæ ¹æ®æ•°æ®é›†å¤§å°å’Œç³»ç»Ÿèµ„æºè‡ªåŠ¨é€‰æ‹©æœ€ä½³è½¬æ¢æ–¹æ³•
    
    Args:
        npz_path: npzæ–‡ä»¶è·¯å¾„
        tfrecord_path: è¾“å‡ºTFRecordæ–‡ä»¶è·¯å¾„
        force: æ˜¯å¦å¼ºåˆ¶é‡æ–°è½¬æ¢
        max_memory_gb: æœ€å¤§å†…å­˜ä½¿ç”¨é™åˆ¶ï¼ˆGBï¼‰
        auto_optimize: æ˜¯å¦è‡ªåŠ¨ä¼˜åŒ–å‚æ•°
    
    Returns:
        bool: è½¬æ¢æ˜¯å¦æˆåŠŸ
    """
    if not psutil:
        print("âŒ é”™è¯¯: è‡ªé€‚åº”è½¬æ¢éœ€è¦psutilåº“ï¼Œè¯·å®‰è£…: pip install psutil")
        return False
    
    # è·å–ç³»ç»Ÿä¿¡æ¯
    total_memory_gb = psutil.virtual_memory().total / (1024**3)
    available_memory_gb = psutil.virtual_memory().available / (1024**3)
    cpu_count = mp.cpu_count()
    
    # è·å–æ•°æ®é›†å¤§å°
    file_size_gb = os.path.getsize(npz_path) / (1024**3)
    
    print(f"ğŸ¤– è‡ªé€‚åº”è½¬æ¢åˆ†æ:")
    print(f"   - æ•°æ®é›†å¤§å°: {file_size_gb:.2f} GB")
    print(f"   - ç³»ç»Ÿæ€»å†…å­˜: {total_memory_gb:.2f} GB")
    print(f"   - å¯ç”¨å†…å­˜: {available_memory_gb:.2f} GB")
    print(f"   - CPUæ ¸å¿ƒæ•°: {cpu_count}")
    print(f"   - å†…å­˜é™åˆ¶: {max_memory_gb:.2f} GB")
    
    # å…ˆå¿«é€Ÿæ£€æŸ¥æ ·æœ¬æ•°é‡ä»¥åšæ›´ç²¾ç¡®çš„å†³ç­–
    try:
        with np.load(npz_path, allow_pickle=True) as data:
            sample_count = len(data['X_paths']) if 'X_paths' in data else 0
        print(f"   - æ ·æœ¬æ•°é‡: {sample_count}")
    except:
        sample_count = 0
    
    # ä¼˜åŒ–çš„å†³ç­–é€»è¾‘ - é’ˆå¯¹å¤§æ–‡ä»¶ä¼˜åŒ–
    if sample_count < 1000:  # å°æ•°æ®é›†
        print("ğŸš€ é€‰æ‹©ï¼šåŸå§‹è½¬æ¢ï¼ˆå°æ•°æ®é›†ï¼‰")
        return convert_npz_to_tfrecord(npz_path, tfrecord_path, batch_size=1000, force=force)
    
    elif sample_count < 5000 and available_memory_gb > 4:  # ä¸­ç­‰æ•°æ®é›†ä¸”å†…å­˜å……è¶³
        print("ğŸš€ é€‰æ‹©ï¼šå¤šè¿›ç¨‹å¿«é€Ÿè½¬æ¢ï¼ˆä¼˜åŒ–å¤§æ–‡ä»¶ï¼‰")
        num_workers = min(cpu_count, 6)  # é€‚ä¸­çš„è¿›ç¨‹æ•°
        batch_size = max(50, min(500, sample_count // 20))  # åŠ¨æ€è°ƒæ•´batch_size
        print(f"   - ä¼˜åŒ–å‚æ•°: workers={num_workers}, batch_size={batch_size}")
        return convert_npz_to_tfrecord_fast(
            npz_path, tfrecord_path, 
            batch_size=batch_size, force=force,
            num_workers=num_workers, use_multiprocessing=True, 
            memory_efficient=True
        )
    
    elif sample_count < 20000 and available_memory_gb > 2:  # å¤§æ•°æ®é›†ä½†å†…å­˜å¯ç”¨
        print("ğŸš€ é€‰æ‹©ï¼šå¤šçº¿ç¨‹å¿«é€Ÿè½¬æ¢ï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰")
        num_workers = min(cpu_count // 2, 4)
        batch_size = max(100, min(1000, sample_count // 50))  # æ›´å¤§çš„batch_sizeæé«˜æ•ˆç‡
        print(f"   - ä¼˜åŒ–å‚æ•°: workers={num_workers}, batch_size={batch_size}")
        return convert_npz_to_tfrecord_fast(
            npz_path, tfrecord_path,
            batch_size=batch_size, force=force,
            num_workers=num_workers, use_multiprocessing=False,
            memory_efficient=True
        )
    
    else:  # è¶…å¤§æ•°æ®é›†æˆ–å†…å­˜ä¸¥é‡ä¸è¶³
        print("ğŸŒŠ é€‰æ‹©ï¼šæµå¼è½¬æ¢ï¼ˆè¶…å¤§æ•°æ®é›†/å†…å­˜å—é™ï¼‰")
        # é’ˆå¯¹å¤§æ–‡ä»¶ä¼˜åŒ–chunk_size
        if auto_optimize and sample_count > 0:
            # ç›®æ ‡ï¼šæ¯ä¸ªchunkå¤„ç†æ—¶é—´çº¦1-2ç§’ï¼Œå†…å­˜ä½¿ç”¨<500MB
            target_chunk_size = max(200, min(2000, sample_count // 100))
            chunk_size = target_chunk_size
        else:
            chunk_size = 500  # å¢å¤§é»˜è®¤chunk_sizeæé«˜æ•ˆç‡
        
        buffer_size = max(3, min(8, int(available_memory_gb)))  # æ ¹æ®å¯ç”¨å†…å­˜è°ƒæ•´
        num_workers = 2  # æµå¼å¤„ç†ä½¿ç”¨å°‘é‡çº¿ç¨‹é¿å…ç«äº‰
        
        print(f"   - ä¼˜åŒ–å‚æ•°: chunk_size={chunk_size}, buffer_size={buffer_size}")
        print(f"   - é¢„è®¡è½¬æ¢æ—¶é—´: {sample_count/chunk_size/10:.1f}-{sample_count/chunk_size/5:.1f} åˆ†é’Ÿ")
        
        return convert_npz_to_tfrecord_streaming(
            npz_path, tfrecord_path,
            chunk_size=chunk_size, force=force,
            num_workers=num_workers, buffer_size=buffer_size
        )

def load_tfrecord_dataset(tfrecord_path, batch_size=32, shuffle_buffer=1000, auto_convert=True, state_dim=None):
    """
    åŠ è½½TFRecordæ•°æ®é›†
    
    Args:
        tfrecord_path: TFRecordæ–‡ä»¶è·¯å¾„
        batch_size: æ‰¹æ¬¡å¤§å°
        shuffle_buffer: éšæœºæ‰“ä¹±ç¼“å†²åŒºå¤§å°
        auto_convert: å¦‚æœæ•°æ®é›†ä¸å­˜åœ¨ï¼Œæ˜¯å¦è‡ªåŠ¨ä»npzè½¬æ¢
        state_dim: çŠ¶æ€å‘é‡ç»´åº¦ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤å€¼8
    
    Returns:
        tf.data.Dataset: TensorFlowæ•°æ®é›†
    """
    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨
    if not check_dataset_exists(tfrecord_path):
        if auto_convert:
            # å°è¯•ä»npzè½¬æ¢
            npz_path = tfrecord_path.replace('.tfrecord', '.npz')
            if os.path.exists(npz_path):
                print(f"ğŸ”„ è‡ªåŠ¨è½¬æ¢æ•°æ®é›†: {npz_path}")
                if not convert_npz_to_tfrecord(npz_path, tfrecord_path):
                    raise FileNotFoundError(f"æ•°æ®é›†è½¬æ¢å¤±è´¥: {npz_path}")
            else:
                raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ•°æ®é›†æ–‡ä»¶: {tfrecord_path} æˆ– {npz_path}")
        else:
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ•°æ®é›†æ–‡ä»¶: {tfrecord_path}")
    
    try:
        # åŠ è½½æ•°æ®é›†ä¿¡æ¯
        info_path = tfrecord_path + '.info.json'
        with open(info_path, 'r') as f:
            dataset_info = json.load(f)
        
        # åŠ¨æ€åˆ›å»ºç‰¹å¾æè¿°
        feature_description = {}
        for key in dataset_info['keys']:
            if dataset_info['shapes'][key] == ():  # æ ‡é‡å€¼
                feature_description[key] = tf.io.FixedLenFeature([], tf.float32)
            else:  # æ•°ç»„
                feature_description[key] = tf.io.FixedLenFeature([], tf.string)
        
        def _parse_function(example_proto):
            # è§£æTFRecord
            parsed_features = tf.io.parse_single_example(example_proto, feature_description)
            
            # å°†bytesè½¬æ¢å›numpyæ•°ç»„
            decoded_data = {}
            for key in dataset_info['keys']:
                if dataset_info['dtypes'][key] == 'float32':  # åµŒå¥—numpyæ•°ç»„å·²è½¬æ¢ä¸ºfloat32
                    # è§£ç ä¸ºfloat32æ•°ç»„
                    decoded = tf.io.decode_raw(parsed_features[key], tf.float32)
                    decoded_data[key] = decoded
                elif dataset_info['shapes'][key] == ():  # æ ‡é‡å€¼
                    decoded_data[key] = parsed_features[key]
                else:  # å…¶ä»–æ•°ç»„ç±»å‹
                    decoded_data[key] = tf.io.decode_raw(parsed_features[key], tf.float32)
            
            # æ ¹æ®æ•°æ®é›†ä¿¡æ¯é‡æ„æ•°æ®
            X_data = decoded_data['X_paths']  # (6000,) -> (1000, 6)
            Y_data = decoded_data['Y_paths']  # (1000,) -> (1000, 1)
            
            # é‡æ„ä¸ºæ­£ç¡®çš„å½¢çŠ¶
            if 'original_shapes' in dataset_info:
                X_inner_shape = dataset_info['original_shapes']['X_paths']['inner_shape']  # [1000, 6]
                Y_inner_shape = dataset_info['original_shapes']['Y_paths']['inner_shape']  # [1000, 1]
                
                X_data_reshaped = tf.reshape(X_data, X_inner_shape)  # (1000, 6)
                Y_data_reshaped = tf.reshape(Y_data, Y_inner_shape)  # (1000, 1)
            else:
                # åå¤‡æ–¹æ¡ˆï¼šæ ¹æ®è®°å½•çš„å½¢çŠ¶æ¨æ–­
                # X_paths: 6000 -> (1000, 6), Y_paths: 1000 -> (1000, 1)
                X_data_reshaped = tf.reshape(X_data, [1000, 6])
                Y_data_reshaped = tf.reshape(Y_data, [1000, 1])
            
            # åˆ›å»ºåˆå§‹çŠ¶æ€ï¼ˆé›¶çŠ¶æ€ï¼‰
            # ä½¿ç”¨ä¼ å…¥çš„ state_dim å‚æ•°ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
            actual_state_dim = state_dim if state_dim is not None else 8
            init_state = tf.zeros((actual_state_dim,), dtype=tf.float32)
            
            # è¿”å›æ¨¡å‹æœŸæœ›çš„æ ¼å¼
            inputs = {
                'delta_input': X_data_reshaped,  # (1000, 6)
                'init_state': init_state         # (8,)
            }
            
            return inputs, Y_data_reshaped  # (1000, 1)
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = tf.data.TFRecordDataset(tfrecord_path)
        dataset = dataset.map(_parse_function)
        
        # è®¾ç½®æ•°æ®é›†å‚æ•°
        dataset = dataset.shuffle(shuffle_buffer)
        dataset = dataset.batch(batch_size)
        dataset = dataset.prefetch(tf.data.AUTOTUNE)
        
        return dataset
        
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
        raise

def split_and_convert_npz(npz_path, output_dir, samples_per_chunk=1000, 
                         target_keys=None, force=False, cleanup_splits=True):
    """
    å°†å¤§çš„NPZæ–‡ä»¶åˆ†å‰²æˆå°å—å¹¶åˆ†åˆ«è½¬æ¢ä¸ºTFRecord
    
    Args:
        npz_path: åŸå§‹NPZæ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        samples_per_chunk: æ¯ä¸ªåˆ†å—çš„æ ·æœ¬æ•°
        target_keys: è¦è½¬æ¢çš„å­—æ®µåˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºè½¬æ¢æ‰€æœ‰å­—æ®µ
        force: æ˜¯å¦å¼ºåˆ¶é‡æ–°è½¬æ¢
        cleanup_splits: è½¬æ¢å®Œæˆåæ˜¯å¦åˆ é™¤ä¸´æ—¶åˆ†å‰²æ–‡ä»¶
    
    Returns:
        bool: è½¬æ¢æ˜¯å¦æˆåŠŸ
    """
    start_time = time.time()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(npz_path):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {npz_path}")
        return False
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # æœ€ç»ˆåˆå¹¶çš„TFRecordæ–‡ä»¶è·¯å¾„
    final_tfrecord = os.path.join(output_dir, 'dataset.tfrecord')
    final_info = final_tfrecord + '.info.json'
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨æœ€ç»ˆæ–‡ä»¶
    if os.path.exists(final_tfrecord) and os.path.exists(final_info) and not force:
        print(f"âœ… è½¬æ¢åçš„æ•°æ®é›†å·²å­˜åœ¨: {final_tfrecord}")
        return True
    
    try:
        print(f"ğŸ“Š åˆ†æNPZæ–‡ä»¶: {npz_path}")
        # è·å–æ–‡ä»¶å¤§å°
        file_size_mb = os.path.getsize(npz_path) / (1024 * 1024)
        print(f"ğŸ“ æ–‡ä»¶å¤§å°: {file_size_mb:.1f} MB")
        
        # ç¬¬ä¸€æ­¥ï¼šåˆ†ææ•°æ®ç»“æ„
        with np.load(npz_path, allow_pickle=True) as data:
            all_keys = list(data.keys())
            
            # ç¡®å®šè¦è½¬æ¢çš„å­—æ®µ
            if target_keys is None:
                target_keys = ['X_paths', 'Y_paths']
            
            keys = [key for key in all_keys if key in target_keys]
            if not keys:
                print(f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°ç›®æ ‡å­—æ®µ {target_keys}")
                print(f"ğŸ“‹ å¯ç”¨å­—æ®µ: {all_keys}")
                return False
            
            print(f"ğŸ¯ å°†è½¬æ¢å­—æ®µ: {keys}")
            
            # è·å–æ€»æ ·æœ¬æ•°
            total_samples = len(data[keys[0]])
            print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {total_samples}")
            
            # è®¡ç®—åˆ†å—ä¿¡æ¯
            num_chunks = (total_samples + samples_per_chunk - 1) // samples_per_chunk
            print(f"ğŸ§© å°†åˆ†å‰²ä¸º {num_chunks} ä¸ªå—ï¼Œæ¯å—çº¦ {samples_per_chunk} æ ·æœ¬")
            
            # è·å–æ•°æ®ç±»å‹ä¿¡æ¯
            sample_info = {}
            for key in keys:
                sample_data = data[key][0]
                if hasattr(sample_data, 'shape') and hasattr(sample_data, 'dtype'):
                    sample_info[key] = {
                        'shape': sample_data.shape,
                        'dtype': sample_data.dtype,
                        'is_nested': sample_data.dtype == object
                    }
                    print(f"   {key}: shape={sample_data.shape}, dtype={sample_data.dtype}")
                else:
                    sample_info[key] = {
                        'shape': (),
                        'dtype': type(sample_data),
                        'is_nested': False
                    }
        
        # ç¬¬äºŒæ­¥ï¼šåˆ›å»ºåˆ†å‰²ç›®å½•
        splits_dir = os.path.join(output_dir, 'temp_splits')
        os.makedirs(splits_dir, exist_ok=True)
        
        print(f"ğŸ”ª å¼€å§‹åˆ†å‰²NPZæ–‡ä»¶...")
        
        # åˆ†å‰²æ–‡ä»¶åˆ—è¡¨
        chunk_files = []
        
        # ä½¿ç”¨å†…å­˜æ˜ å°„æ¨¡å¼åˆ†å‰²æ–‡ä»¶
        data = np.load(npz_path, mmap_mode='r', allow_pickle=True)
        
        for chunk_id in tqdm(range(num_chunks), desc="åˆ†å‰²æ–‡ä»¶"):
            start_idx = chunk_id * samples_per_chunk
            end_idx = min(start_idx + samples_per_chunk, total_samples)
            
            # åˆ›å»ºåˆ†å—æ–‡ä»¶è·¯å¾„
            chunk_file = os.path.join(splits_dir, f'chunk_{chunk_id:04d}.npz')
            chunk_files.append(chunk_file)
            
            # å¦‚æœåˆ†å—æ–‡ä»¶å·²å­˜åœ¨ä¸”ä¸å¼ºåˆ¶é‡å»ºï¼Œè·³è¿‡
            if os.path.exists(chunk_file) and not force:
                continue
            
            # æå–å½“å‰å—çš„æ•°æ®
            chunk_data = {}
            for key in keys:
                chunk_data[key] = data[key][start_idx:end_idx]
                
                # å¯¹äºåµŒå¥—æ•°ç»„ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†ä»¥é¿å…å¼•ç”¨é—®é¢˜
                if sample_info[key]['is_nested']:
                    # åˆ›å»ºç‹¬ç«‹çš„å‰¯æœ¬
                    chunk_data[key] = np.array([item.copy() if hasattr(item, 'copy') else item 
                                              for item in chunk_data[key]], dtype=object)
            
            # ä¿å­˜åˆ†å—
            np.savez_compressed(chunk_file, **chunk_data)
        
        # æ¸…ç†åŸå§‹æ•°æ®å¼•ç”¨
        del data
        import gc
        gc.collect()
        
        print(f"âœ… æ–‡ä»¶åˆ†å‰²å®Œæˆï¼Œå…± {len(chunk_files)} ä¸ªåˆ†å—")
        
        # ç¬¬ä¸‰æ­¥ï¼šå¹¶è¡Œè½¬æ¢åˆ†å—
        print(f"ğŸš€ å¼€å§‹å¹¶è¡Œè½¬æ¢åˆ†å—...")
        
        def convert_chunk(chunk_info):
            chunk_id, chunk_file = chunk_info
            tfrecord_file = chunk_file.replace('.npz', '.tfrecord')
            
            # ä½¿ç”¨ç®€å•çš„è½¬æ¢æ–¹æ³•
            success = convert_npz_to_tfrecord(
                chunk_file, tfrecord_file, 
                batch_size=min(100, samples_per_chunk), 
                force=force
            )
            
            if success:
                return tfrecord_file
            else:
                print(f"âŒ åˆ†å— {chunk_id} è½¬æ¢å¤±è´¥")
                return None
        
        # å¹¶è¡Œè½¬æ¢æ‰€æœ‰åˆ†å—
        max_workers = min(mp.cpu_count(), 4)  # é™åˆ¶å¹¶å‘æ•°é¿å…èµ„æºç«äº‰
        chunk_infos = [(i, chunk_file) for i, chunk_file in enumerate(chunk_files)]
        
        tfrecord_files = []
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            results = list(tqdm(
                executor.map(convert_chunk, chunk_infos),
                total=len(chunk_infos),
                desc="è½¬æ¢åˆ†å—"
            ))
            
            tfrecord_files = [f for f in results if f is not None]
        
        if len(tfrecord_files) != len(chunk_files):
            print(f"âŒ éƒ¨åˆ†åˆ†å—è½¬æ¢å¤±è´¥: {len(tfrecord_files)}/{len(chunk_files)}")
            return False
        
        print(f"âœ… æ‰€æœ‰åˆ†å—è½¬æ¢å®Œæˆ")
        
        # ç¬¬å››æ­¥ï¼šåˆå¹¶TFRecordæ–‡ä»¶
        print(f"ğŸ”— åˆå¹¶TFRecordæ–‡ä»¶...")
        
        with tf.io.TFRecordWriter(final_tfrecord) as final_writer:
            total_records = 0
            
            for tfrecord_file in tqdm(tfrecord_files, desc="åˆå¹¶æ–‡ä»¶"):
                # è¯»å–å¹¶å†™å…¥æ¯ä¸ªåˆ†å—çš„è®°å½•
                for record in tf.data.TFRecordDataset(tfrecord_file):
                    final_writer.write(record.numpy())
                    total_records += 1
        
        print(f"âœ… TFRecordåˆå¹¶å®Œæˆï¼Œå…± {total_records} æ¡è®°å½•")
        
        # ç¬¬äº”æ­¥ï¼šåˆ›å»ºåˆå¹¶åçš„ä¿¡æ¯æ–‡ä»¶
        final_dataset_info = {
            'keys': keys,
            'total_samples': total_samples,
            'total_records': total_records,
            'shapes': {},
            'dtypes': {},
            'created_at': str(np.datetime64('now')),
            'source_file': npz_path,
            'conversion_time': time.time() - start_time,
            'conversion_method': 'split_and_convert',
            'num_chunks': num_chunks,
            'samples_per_chunk': samples_per_chunk,
            'note': 'åˆ†å‰²è½¬æ¢ç‰ˆæœ¬ - å†…å­˜å‹å¥½ä¸”é«˜æ•ˆ'
        }
        
        # ä»ç¬¬ä¸€ä¸ªåˆ†å—çš„ä¿¡æ¯æ–‡ä»¶ä¸­è·å–è¯¦ç»†ä¿¡æ¯
        if tfrecord_files:
            first_info_file = tfrecord_files[0] + '.info.json'
            if os.path.exists(first_info_file):
                with open(first_info_file, 'r') as f:
                    first_info = json.load(f)
                    final_dataset_info['shapes'] = first_info.get('shapes', {})
                    final_dataset_info['dtypes'] = first_info.get('dtypes', {})
                    final_dataset_info['original_shapes'] = first_info.get('original_shapes', {})
        
        # ä¿å­˜æœ€ç»ˆä¿¡æ¯æ–‡ä»¶
        with open(final_info, 'w') as f:
            json.dump(final_dataset_info, f, indent=2)
        
        # ç¬¬å…­æ­¥ï¼šæ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if cleanup_splits:
            print(f"ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
            try:
                shutil.rmtree(splits_dir)
                print(f"âœ… ä¸´æ—¶æ–‡ä»¶æ¸…ç†å®Œæˆ")
            except Exception as e:
                print(f"âš ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        else:
            print(f"ğŸ’¾ ä¸´æ—¶æ–‡ä»¶ä¿ç•™åœ¨: {splits_dir}")
        
        # æ€§èƒ½ç»Ÿè®¡
        elapsed_time = time.time() - start_time
        conversion_rate = total_samples / elapsed_time
        
        print(f"\nğŸ‰ åˆ†å‰²è½¬æ¢å®Œæˆï¼")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {final_tfrecord}")
        print(f"ğŸ“Š è½¬æ¢ç»Ÿè®¡:")
        print(f"   â±ï¸ æ€»è€—æ—¶: {elapsed_time:.2f}ç§’")
        print(f"   ğŸ“ˆ è½¬æ¢é€Ÿåº¦: {conversion_rate:.1f} æ ·æœ¬/ç§’")
        print(f"   ğŸ§© åˆ†å—æ•°é‡: {num_chunks}")
        print(f"   ğŸ“¦ æœ€ç»ˆæ–‡ä»¶å¤§å°: {os.path.getsize(final_tfrecord)/(1024*1024):.1f} MB")
        print(f"   ğŸ’¾ å‹ç¼©æ¯”: {file_size_mb/(os.path.getsize(final_tfrecord)/(1024*1024)):.2f}:1")
        
        return True
        
    except Exception as e:
        print(f"âŒ åˆ†å‰²è½¬æ¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        
        # æ¸…ç†å¤±è´¥çš„æ–‡ä»¶
        if os.path.exists(final_tfrecord):
            os.remove(final_tfrecord)
        if os.path.exists(final_info):
            os.remove(final_info)
        
        return False

def smart_convert_npz_to_tfrecord(npz_path, output_path, method='auto', **kwargs):
    """
    æ™ºèƒ½è½¬æ¢ï¼šæ ¹æ®æ–‡ä»¶å¤§å°è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜è½¬æ¢æ–¹æ³•
    
    Args:
        npz_path: NPZæ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºè·¯å¾„ï¼ˆæ–‡ä»¶æˆ–ç›®å½•ï¼‰
        method: è½¬æ¢æ–¹æ³• ('auto', 'simple', 'fast', 'streaming', 'adaptive', 'split')
        **kwargs: å…¶ä»–å‚æ•°
    
    Returns:
        bool: è½¬æ¢æ˜¯å¦æˆåŠŸ
    """
    # è·å–æ–‡ä»¶å¤§å°
    file_size_mb = os.path.getsize(npz_path) / (1024 * 1024)
    
    # è·å–ç³»ç»Ÿå†…å­˜
    if psutil:
        available_memory_gb = psutil.virtual_memory().available / (1024**3)
    else:
        available_memory_gb = 4  # é»˜è®¤å‡è®¾4GBå¯ç”¨å†…å­˜
    
    print(f"ğŸ¤– æ™ºèƒ½è½¬æ¢åˆ†æ:")
    print(f"   ğŸ“ æ–‡ä»¶å¤§å°: {file_size_mb:.1f} MB")
    print(f"   ğŸ§  å¯ç”¨å†…å­˜: {available_memory_gb:.1f} GB")
    
    # è‡ªåŠ¨é€‰æ‹©è½¬æ¢æ–¹æ³•
    if method == 'auto':
        if file_size_mb < 50:  # å°æ–‡ä»¶ (<50MB)
            method = 'simple'
        elif file_size_mb < 500 and available_memory_gb > 2:  # ä¸­ç­‰æ–‡ä»¶ä¸”å†…å­˜å……è¶³
            method = 'fast'
        elif file_size_mb < 2000 and available_memory_gb > 1:  # å¤§æ–‡ä»¶ä½†å†…å­˜å¤Ÿç”¨
            method = 'adaptive'
        else:  # è¶…å¤§æ–‡ä»¶æˆ–å†…å­˜ä¸è¶³
            method = 'split'
    
    print(f"ğŸš€ é€‰æ‹©è½¬æ¢æ–¹æ³•: {method}")
    
    # æ ¹æ®æ–¹æ³•æ‰§è¡Œè½¬æ¢
    if method == 'simple':
        return convert_npz_to_tfrecord(npz_path, output_path, **kwargs)
    
    elif method == 'fast':
        return convert_npz_to_tfrecord_fast(npz_path, output_path, **kwargs)
    
    elif method == 'streaming':
        return convert_npz_to_tfrecord_streaming(npz_path, output_path, **kwargs)
    
    elif method == 'adaptive':
        return convert_npz_to_tfrecord_adaptive(npz_path, output_path, **kwargs)
    
    elif method == 'split':
        # å¯¹äºåˆ†å‰²æ–¹æ³•ï¼Œoutput_pathåº”è¯¥æ˜¯ç›®å½•
        if output_path.endswith('.tfrecord'):
            output_dir = os.path.dirname(output_path)
        else:
            output_dir = output_path
        
        return split_and_convert_npz(
            npz_path, output_dir, 
            samples_per_chunk=kwargs.get('samples_per_chunk', 1000),
            target_keys=kwargs.get('target_keys', ['X_paths', 'Y_paths']),
            force=kwargs.get('force', False),
            cleanup_splits=kwargs.get('cleanup_splits', True)
        )
    
    else:
        print(f"âŒ æœªçŸ¥çš„è½¬æ¢æ–¹æ³•: {method}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    parser = argparse.ArgumentParser(description='è½¬æ¢npzæ•°æ®é›†ä¸ºTFRecordæ ¼å¼')
    parser.add_argument('--npz_path', type=str, required=True,
                      help='è¾“å…¥npzæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--tfrecord_path', type=str, required=True,
                      help='è¾“å‡ºTFRecordæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--batch_size', type=int, default=1000,
                      help='è½¬æ¢æ—¶çš„æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--force', action='store_true',
                      help='å¼ºåˆ¶é‡æ–°è½¬æ¢ï¼ˆå³ä½¿å·²å­˜åœ¨ï¼‰')
    args = parser.parse_args()
    
    convert_npz_to_tfrecord(args.npz_path, args.tfrecord_path, args.batch_size, args.force)

if __name__ == '__main__':
    # main() 

    name = 'dataset_EMSC_tt'
    npz_path = f'/Users/tianyunhu/Documents/temp/code/Test_app/EMSC_Model/msc_models/{name}/{name}.npz'
    tfrecord_path = f'/Users/tianyunhu/Documents/temp/code/Test_app/EMSC_Model/msc_models/{name}/{name}.tfrecord'
    
    print("ğŸš€ æµ‹è¯•æ‰€æœ‰è½¬æ¢æ–¹æ³•...")
    
    def progress_callback(processed, total, chunk_id):
        """è¿›åº¦å›è°ƒå‡½æ•°"""
        if chunk_id % 5 == 0:  # æ¯5ä¸ªå—æŠ¥å‘Šä¸€æ¬¡
            print(f"   ğŸ“Š å·²å¤„ç†: {processed}/{total} æ ·æœ¬ ({processed/total*100:.1f}%)")
    
    # æµ‹è¯•ä¸åŒçš„è½¬æ¢æ–¹æ³•
    # print("\n=== 1. æµ‹è¯•æµå¼è½¬æ¢ ===")
    # convert_npz_to_tfrecord_streaming(
    #     npz_path, tfrecord_path + '_streaming', 
    #     chunk_size=20, force=True, 
    #     num_workers=2, buffer_size=5,
    #     progress_callback=progress_callback
    # )
    
    # print("\n=== 2. æµ‹è¯•è‡ªé€‚åº”è½¬æ¢ ===")
    convert_npz_to_tfrecord_adaptive(
        npz_path, tfrecord_path,
        force=True, max_memory_gb=4, auto_optimize=True
    )
    
    # print("\n=== æµ‹è¯•åˆ†å‰²è½¬æ¢ï¼ˆæ¨èç”¨äºå¤§æ–‡ä»¶ï¼‰===")
    # output_dir = os.path.dirname(tfrecord_path)
    # success = split_and_convert_npz(
    #     npz_path, output_dir,
    #     samples_per_chunk=20,  # æ¯å—20ä¸ªæ ·æœ¬ï¼Œé€‚åˆæµ‹è¯•
    #     target_keys=['X_paths', 'Y_paths'],
    #     force=True,
    #     cleanup_splits=True  # è½¬æ¢å®Œæˆåæ¸…ç†ä¸´æ—¶æ–‡ä»¶
    # )
    
    # if success:
    #     print("\n=== æµ‹è¯•æ™ºèƒ½è½¬æ¢ ===")
    #     # æµ‹è¯•æ™ºèƒ½è½¬æ¢ï¼ˆè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ–¹æ³•ï¼‰
    #     smart_convert_npz_to_tfrecord(
    #         npz_path, 
    #         os.path.join(output_dir, 'smart_dataset.tfrecord'),
    #         method='auto',  # è‡ªåŠ¨é€‰æ‹©æ–¹æ³•
    #         force=True
    #     )

